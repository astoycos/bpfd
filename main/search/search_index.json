{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Formerly know as bpfd bpfman: An eBPF Manager bpfman operates as an eBPF manager, focusing on simplifying the deployment and administration of eBPF programs. Its notable features encompass: System Overview : Provides insights into how eBPF is utilized in your system. eBPF Program Loader : Includes a built-in program loader that supports program cooperation for XDP and TC programs, as well as deployment of eBPF programs from OCI images. eBPF Filesystem Management : Manages the eBPF filesystem, facilitating the deployment of eBPF applications without requiring additional privileges. Our program loader and eBPF filesystem manager ensure the secure deployment of eBPF applications. Furthermore, bpfman includes a Kubernetes operator, extending these capabilities to Kubernetes. This allows users to confidently deploy eBPF through custom resource definitions across nodes in a cluster. Why eBPF? eBPF is a powerful general-purpose framework that allows running sandboxed programs in the kernel. It can be used for many purposes, including networking, monitoring, tracing and security. Why eBPF in Kubernetes? Demand is increasing from both Kubernetes developers and users. Examples of eBPF in Kubernetes include: Cilium and Calico CNIs Pixie : Open source observability KubeArmor : Container-aware runtime security enforcement system Blixt : Gateway API L4 conformance implementation NetObserv : Open source operator for network observability Challenges for eBPF in Kubernetes Requires privileged pods. eBPF-enabled apps require at least CAP_BPF permissions and potentially more depending on the type of program that is being attached. Since the Linux capabilities are very broad it is challenging to constrain a pod to the minimum set of privileges required. This can allow them to do damage (either unintentionally or intentionally). Handling multiple eBPF programs on the same eBPF hooks. Not all eBPF hooks are designed to support multiple programs. Some software using eBPF assumes exclusive use of an eBPF hook and can unintentionally eject existing programs when being attached. This can result in silent failures and non-deterministic failures. Debugging problems with deployments is hard. The cluster administrator may not be aware that eBPF programs are being used in a cluster. It is possible for some eBPF programs to interfere with others in unpredictable ways. SSH access or a privileged pod is necessary to determine the state of eBPF programs on each node in the cluster. Lifecycle management of eBPF programs. While there are libraries for the basic loading and unloading of eBPF programs, a lot of code is often needed around them for lifecycle management. Deployment on Kubernetes is not simple. It is an involved process that requires first writing a daemon that loads your eBPF bytecode and deploying it using a DaemonSet. This requires careful design and intricate knowledge of the eBPF program lifecycle to ensure your program stays loaded and that you can easily tolerate pod restarts and upgrades. In eBPF enabled K8s deployments today, the eBPF Program is often embedded into the userspace binary that loads and interacts with it. This means there's no easy way to have fine-grained versioning control of the bpfProgram in relation to it's accompanying userspace counterpart. What is bpfman? bpfman is a software stack that aims to make it easy to load, unload, modify and monitor eBPF programs whether on a single host, or in a Kubernetes cluster. bpfman includes the following core components: bpfman: A system daemon that supports loading, unloading, modifying and monitoring of eBPF programs exposed over a gRPC API. eBPF CRDS: bpfman provides a set of CRDs ( XdpProgram , TcProgram , etc.) that provide a way to express intent to load eBPF programs as well as a bpfman generated CRD ( BpfProgram ) used to represent the runtime state of loaded programs. bpfman-agent: The agent runs in a container in the bpfman daemonset and ensures that the requested eBPF programs for a given node are in the desired state. bpfman-operator: An operator, built using Operator SDK , that manages the installation and lifecycle of bpfman-agent and the CRDs in a Kubernetes cluster. bpfman is developed in Rust and built on top of Aya, a Rust eBPF library. The benefits of this solution include the following: Security Improved security because only the bpfman daemon, which can be tightly controlled, has the privileges needed to load eBPF programs, while access to the API can be controlled via standard RBAC methods. Within bpfman, only a single thread keeps these capabilities while the other threads (serving RPCs) do not. Gives the administrators control over who can load programs. Allows administrators to define rules for the ordering of networking eBPF programs. (ROADMAP) Visibility/Debuggability Improved visibility into what eBPF programs are running on a system, which enhances the debuggability for developers, administrators, and customer support. The greatest benefit is achieved when all apps use bpfman, but even if they don't, bpfman can provide visibility into all the eBPF programs loaded on the nodes in a cluster. Multi-program Support Support for the coexistence of multiple eBPF programs from multiple users. Uses the libxdp multiprog protocol to allow multiple XDP programs on single interface This same protocol is also supported for TC programs to provide a common multi-program user experience across both TC and XDP. Productivity Simplifies the deployment and lifecycle management of eBPF programs in a Kubernetes cluster. developers can stop worrying about program lifecycle (loading, attaching, pin management, etc.) and use existing eBPF libraries to interact with their program maps using well defined pin points which are managed by bpfman. Developers can still use Cilium/libbpf/Aya/etc libraries for eBPF development, and load/unload with bpfman. Provides eBPF Bytecode Image Specifications that allows fine-grained separate versioning control for userspace and kernelspace programs. This also allows for signing these container images to verify bytecode ownership. For more details, please see the following: Setup and Building bpfman for instructions on setting up your development environment and building bpfman. Tutorial for some examples of starting bpfman , managing logs, and using the CLI. Example eBPF Programs for some examples of eBPF programs written in Go, interacting with bpfman . Deploying the bpfman-operator for details on launching bpfman in a Kubernetes cluster. Meet the Community for details on community meeting details.","title":"Introduction"},{"location":"#bpfman-an-ebpf-manager","text":"bpfman operates as an eBPF manager, focusing on simplifying the deployment and administration of eBPF programs. Its notable features encompass: System Overview : Provides insights into how eBPF is utilized in your system. eBPF Program Loader : Includes a built-in program loader that supports program cooperation for XDP and TC programs, as well as deployment of eBPF programs from OCI images. eBPF Filesystem Management : Manages the eBPF filesystem, facilitating the deployment of eBPF applications without requiring additional privileges. Our program loader and eBPF filesystem manager ensure the secure deployment of eBPF applications. Furthermore, bpfman includes a Kubernetes operator, extending these capabilities to Kubernetes. This allows users to confidently deploy eBPF through custom resource definitions across nodes in a cluster.","title":"bpfman: An eBPF Manager"},{"location":"#why-ebpf","text":"eBPF is a powerful general-purpose framework that allows running sandboxed programs in the kernel. It can be used for many purposes, including networking, monitoring, tracing and security.","title":"Why eBPF?"},{"location":"#why-ebpf-in-kubernetes","text":"Demand is increasing from both Kubernetes developers and users. Examples of eBPF in Kubernetes include: Cilium and Calico CNIs Pixie : Open source observability KubeArmor : Container-aware runtime security enforcement system Blixt : Gateway API L4 conformance implementation NetObserv : Open source operator for network observability","title":"Why eBPF in Kubernetes?"},{"location":"#challenges-for-ebpf-in-kubernetes","text":"Requires privileged pods. eBPF-enabled apps require at least CAP_BPF permissions and potentially more depending on the type of program that is being attached. Since the Linux capabilities are very broad it is challenging to constrain a pod to the minimum set of privileges required. This can allow them to do damage (either unintentionally or intentionally). Handling multiple eBPF programs on the same eBPF hooks. Not all eBPF hooks are designed to support multiple programs. Some software using eBPF assumes exclusive use of an eBPF hook and can unintentionally eject existing programs when being attached. This can result in silent failures and non-deterministic failures. Debugging problems with deployments is hard. The cluster administrator may not be aware that eBPF programs are being used in a cluster. It is possible for some eBPF programs to interfere with others in unpredictable ways. SSH access or a privileged pod is necessary to determine the state of eBPF programs on each node in the cluster. Lifecycle management of eBPF programs. While there are libraries for the basic loading and unloading of eBPF programs, a lot of code is often needed around them for lifecycle management. Deployment on Kubernetes is not simple. It is an involved process that requires first writing a daemon that loads your eBPF bytecode and deploying it using a DaemonSet. This requires careful design and intricate knowledge of the eBPF program lifecycle to ensure your program stays loaded and that you can easily tolerate pod restarts and upgrades. In eBPF enabled K8s deployments today, the eBPF Program is often embedded into the userspace binary that loads and interacts with it. This means there's no easy way to have fine-grained versioning control of the bpfProgram in relation to it's accompanying userspace counterpart.","title":"Challenges for eBPF in Kubernetes"},{"location":"#what-is-bpfman","text":"bpfman is a software stack that aims to make it easy to load, unload, modify and monitor eBPF programs whether on a single host, or in a Kubernetes cluster. bpfman includes the following core components: bpfman: A system daemon that supports loading, unloading, modifying and monitoring of eBPF programs exposed over a gRPC API. eBPF CRDS: bpfman provides a set of CRDs ( XdpProgram , TcProgram , etc.) that provide a way to express intent to load eBPF programs as well as a bpfman generated CRD ( BpfProgram ) used to represent the runtime state of loaded programs. bpfman-agent: The agent runs in a container in the bpfman daemonset and ensures that the requested eBPF programs for a given node are in the desired state. bpfman-operator: An operator, built using Operator SDK , that manages the installation and lifecycle of bpfman-agent and the CRDs in a Kubernetes cluster. bpfman is developed in Rust and built on top of Aya, a Rust eBPF library. The benefits of this solution include the following: Security Improved security because only the bpfman daemon, which can be tightly controlled, has the privileges needed to load eBPF programs, while access to the API can be controlled via standard RBAC methods. Within bpfman, only a single thread keeps these capabilities while the other threads (serving RPCs) do not. Gives the administrators control over who can load programs. Allows administrators to define rules for the ordering of networking eBPF programs. (ROADMAP) Visibility/Debuggability Improved visibility into what eBPF programs are running on a system, which enhances the debuggability for developers, administrators, and customer support. The greatest benefit is achieved when all apps use bpfman, but even if they don't, bpfman can provide visibility into all the eBPF programs loaded on the nodes in a cluster. Multi-program Support Support for the coexistence of multiple eBPF programs from multiple users. Uses the libxdp multiprog protocol to allow multiple XDP programs on single interface This same protocol is also supported for TC programs to provide a common multi-program user experience across both TC and XDP. Productivity Simplifies the deployment and lifecycle management of eBPF programs in a Kubernetes cluster. developers can stop worrying about program lifecycle (loading, attaching, pin management, etc.) and use existing eBPF libraries to interact with their program maps using well defined pin points which are managed by bpfman. Developers can still use Cilium/libbpf/Aya/etc libraries for eBPF development, and load/unload with bpfman. Provides eBPF Bytecode Image Specifications that allows fine-grained separate versioning control for userspace and kernelspace programs. This also allows for signing these container images to verify bytecode ownership. For more details, please see the following: Setup and Building bpfman for instructions on setting up your development environment and building bpfman. Tutorial for some examples of starting bpfman , managing logs, and using the CLI. Example eBPF Programs for some examples of eBPF programs written in Go, interacting with bpfman . Deploying the bpfman-operator for details on launching bpfman in a Kubernetes cluster. Meet the Community for details on community meeting details.","title":"What is bpfman?"},{"location":"blog/","text":"Bpfman Blog","title":"Bpfman Blog"},{"location":"blog/#bpfman-blog","text":"","title":"Bpfman Blog"},{"location":"blog/posts/a-new-logo/","text":"A New Logo: Using Generative AI, of course Since we renamed the project to bpfman we are in need of a new logo. Given that the tech buzz around Generative AI is infectious, we decided to explore using generative AI to create our new logo. What we found was that it was a great way to generate ideas, but a human (me) was still needed to create the final design. The Brief I have a love of open source projects with animal mascots, so bpfman should be no different. The \"bee\" is used a lot for eBPF related projects. One such example is Crabby , the crab/bee hybrid, that I created for the Aya project. The logo should be cute and playful, but not too childish. As a nod to Podman , we'd like to use the same typeface and split color-scheme as they do, replacing purple with yellow. One bee is not enough! Since we're an eBPF manager, we need a more bees! via GIPHY And since those bees are bee-ing (sorry) managed, they should be organized. Maybe in a pyramid shape? The Process We used Bing Image Creator , which is backed by DALL-E 3 . Initially we tried to use the following prompt: Logo for open source software project called \"bpfman\". \"bpf\" should be yellow and \"man\" should be black or grey. an illustration of some organized bees above the text. cute. playful Our AI overlords came up with: Not bad, but not quite what we were looking for. It's clear that as smart as AI is, it struggles with text, so whatever we need will need some manual post-processing. There are bees, if you squint a bit, but they're not very organized. Let's refine our prompt a bit: Logo for open source software project called \"bpfman\" as one word. The \"bpf\" should be yellow and \"man\" should be black or grey. an illustration of some organized bees above the text. cute. playful. That... is worse. Let's try again: Logo for a project called \"bpfman\". In the text \"bpfman\", \"bpf\" should be yellow and \"man\" should be black or grey. add an illustration of some organized bees above the text. cute and playful style. The bottom left one is pretty good! So I shared it with the rest of the maintainers to see what they thought. At this point the feedback that I got was the bees were too cute! We're a manager, and managers are serious business, so we need serious bees. Prompting the AI for the whole logo was far too ambitious, so I decided I would just use the AI to generate the bees and then I would add the text myself. I tried a few different prompts, but the one that worked best was: 3 bees guarding a hive. stern expressions. simple vector style. The bottom right was exactly what I had in mind! With a little bit of post-processing, I ended up with this: Now it was time to solicit some feedback. Gathering Feedback After showing the logo to a few others, we decided that the bees were infact too stern. At this point we had a few options, like reverting back to our cute bees, however, this section in the [Bing Image Creator Terms of Service] was pointed out to me: Use of Creations. Subject to your compliance with this Agreement, the Microsoft Services Agreement, and our Content Policy, you may use Creations outside of the Online Services for any legal personal, non-commercial purpose. This means that we can't use the AI generated images for our logo. Was it all for nothing? Was it all for nothing? No! We learnt a lot from this process. Generative AI is great for generating ideas. Some of the logo compositions produced were great! It was also very useful to adjust the prompt based on feedback from team members so we could incorporate their ideas into the design. We also learnt that the AI is not great at text, so we should avoid using it for that. And finally, we learnt that we can't use the AI generated images for our logo. Well, not with the generator we used anyway. The (Semi) Final Design Process I started from scratch, taking inspiration from the AI generated images. The bees were drawn first and composed around a hive - as our AI overlords suggested. I then added the text, and colours, but it still felt like it was missing something. What if we added a force field around the hive? That might be cool! And so, I added a force field around the hive and played around with the colours until I was happy. Here's what we ended up with: We consulted a few more people and got some feedback. The general consensus was that the logo was too busy... However, the reception to the force field was that the favicon I'd mocked would work better as the logo. The Final Design Here's the final design: Pretty cool, right? Even if I do say so myself. Our mascot is a queen bee, because she's the manager of the hive. The force field, is now no longer a force field - It's a pheramone cloud that represents the Queen Mandibular Pheromone (QMP) that the queen bee produces to keep the hive organized. Conclusion I'm really happy with the result! I'm not a designer, so I'm sure there are things that could be improved, but I think it's a good start. What do you think? Join us on Slack and let us know!","title":"A New Logo: Using Generative AI, of course"},{"location":"blog/posts/a-new-logo/#a-new-logo-using-generative-ai-of-course","text":"Since we renamed the project to bpfman we are in need of a new logo. Given that the tech buzz around Generative AI is infectious, we decided to explore using generative AI to create our new logo. What we found was that it was a great way to generate ideas, but a human (me) was still needed to create the final design.","title":"A New Logo: Using Generative AI, of course"},{"location":"blog/posts/a-new-logo/#the-brief","text":"I have a love of open source projects with animal mascots, so bpfman should be no different. The \"bee\" is used a lot for eBPF related projects. One such example is Crabby , the crab/bee hybrid, that I created for the Aya project. The logo should be cute and playful, but not too childish. As a nod to Podman , we'd like to use the same typeface and split color-scheme as they do, replacing purple with yellow. One bee is not enough! Since we're an eBPF manager, we need a more bees! via GIPHY And since those bees are bee-ing (sorry) managed, they should be organized. Maybe in a pyramid shape?","title":"The Brief"},{"location":"blog/posts/a-new-logo/#the-process","text":"We used Bing Image Creator , which is backed by DALL-E 3 . Initially we tried to use the following prompt: Logo for open source software project called \"bpfman\". \"bpf\" should be yellow and \"man\" should be black or grey. an illustration of some organized bees above the text. cute. playful Our AI overlords came up with: Not bad, but not quite what we were looking for. It's clear that as smart as AI is, it struggles with text, so whatever we need will need some manual post-processing. There are bees, if you squint a bit, but they're not very organized. Let's refine our prompt a bit: Logo for open source software project called \"bpfman\" as one word. The \"bpf\" should be yellow and \"man\" should be black or grey. an illustration of some organized bees above the text. cute. playful. That... is worse. Let's try again: Logo for a project called \"bpfman\". In the text \"bpfman\", \"bpf\" should be yellow and \"man\" should be black or grey. add an illustration of some organized bees above the text. cute and playful style. The bottom left one is pretty good! So I shared it with the rest of the maintainers to see what they thought. At this point the feedback that I got was the bees were too cute! We're a manager, and managers are serious business, so we need serious bees. Prompting the AI for the whole logo was far too ambitious, so I decided I would just use the AI to generate the bees and then I would add the text myself. I tried a few different prompts, but the one that worked best was: 3 bees guarding a hive. stern expressions. simple vector style. The bottom right was exactly what I had in mind! With a little bit of post-processing, I ended up with this: Now it was time to solicit some feedback.","title":"The Process"},{"location":"blog/posts/a-new-logo/#gathering-feedback","text":"After showing the logo to a few others, we decided that the bees were infact too stern. At this point we had a few options, like reverting back to our cute bees, however, this section in the [Bing Image Creator Terms of Service] was pointed out to me: Use of Creations. Subject to your compliance with this Agreement, the Microsoft Services Agreement, and our Content Policy, you may use Creations outside of the Online Services for any legal personal, non-commercial purpose. This means that we can't use the AI generated images for our logo.","title":"Gathering Feedback"},{"location":"blog/posts/a-new-logo/#was-it-all-for-nothing","text":"Was it all for nothing? No! We learnt a lot from this process. Generative AI is great for generating ideas. Some of the logo compositions produced were great! It was also very useful to adjust the prompt based on feedback from team members so we could incorporate their ideas into the design. We also learnt that the AI is not great at text, so we should avoid using it for that. And finally, we learnt that we can't use the AI generated images for our logo. Well, not with the generator we used anyway.","title":"Was it all for nothing?"},{"location":"blog/posts/a-new-logo/#the-semi-final-design-process","text":"I started from scratch, taking inspiration from the AI generated images. The bees were drawn first and composed around a hive - as our AI overlords suggested. I then added the text, and colours, but it still felt like it was missing something. What if we added a force field around the hive? That might be cool! And so, I added a force field around the hive and played around with the colours until I was happy. Here's what we ended up with: We consulted a few more people and got some feedback. The general consensus was that the logo was too busy... However, the reception to the force field was that the favicon I'd mocked would work better as the logo.","title":"The (Semi) Final Design Process"},{"location":"blog/posts/a-new-logo/#the-final-design","text":"Here's the final design: Pretty cool, right? Even if I do say so myself. Our mascot is a queen bee, because she's the manager of the hive. The force field, is now no longer a force field - It's a pheramone cloud that represents the Queen Mandibular Pheromone (QMP) that the queen bee produces to keep the hive organized.","title":"The Final Design"},{"location":"blog/posts/a-new-logo/#conclusion","text":"I'm really happy with the result! I'm not a designer, so I'm sure there are things that could be improved, but I think it's a good start. What do you think? Join us on Slack and let us know!","title":"Conclusion"},{"location":"blog/posts/bpfd-becomes-bpfman/","text":"bpfd becomes bpfman Bpfd is now bpfman! We've renamed the project to better reflect the direction we're taking. We're still the same project, just with a new name. Why the name change? We've been using the name bpfd for a while now, but we were not the first to use it. There were projects before us that used the name bpfd , but since most were inactive, originally we didn't see this as an issue. More recently though the folks at Meta have started using the name systemd-bpfd for their proposed addition to systemd . In addition, we've been thinking about the future of the project, and particularly about security and whether it's wise to keep something with CAP_BPF capabilities running as a daemon - even if we've been very careful. This is similar to the issues faced by docker which eventually lead to the creation of podman. This issue led us down the path of redesigning the project to be daemonless. We'll be implementing these changes in the coming months and plan to perform our first release as bpfman in Q1 of 2024. The 'd' in bpfd stood for daemon, so with our new design and the confusion surrounding the name bpfd we though it was time for a change. Since we're a BPF manager, we're now bpfman! It's also a nice homage to podman , which we're big fans of. What does this mean for me? If you're a developer of bpfman you will need to update your Git remotes to point at our new organization and repository name. Github will redirect these for a while, but we recommend updating your remotes as soon as possible. If you're a user of bpfd or the bpfd-operator then version 0.3.1 will be the last release under the bpfd name. We will continue to support you as best we can, but we recommend upgrading to bpfman as soon as our first release is available. What's next? We've hinted at some of the changes we're planning, and of course, our roadmap is always available in Github . It's worth mentioning that we're also planning to expand our release packages to include RPMs and DEBs, making it even easier to install bpfman on your favorite Linux distribution. Thanks! We'd like to thank everyone who has contributed to bpfd over the years. We're excited about the future of bpfman and we hope you are too! Please bear with us as we make this transition, and if you have any questions or concerns, please reach out to us on Slack . We're in the '#bpfd' channel, but we'll be changing that to '#bpfman' soon.","title":"bpfd becomes bpfman"},{"location":"blog/posts/bpfd-becomes-bpfman/#bpfd-becomes-bpfman","text":"Bpfd is now bpfman! We've renamed the project to better reflect the direction we're taking. We're still the same project, just with a new name.","title":"bpfd becomes bpfman"},{"location":"blog/posts/bpfd-becomes-bpfman/#why-the-name-change","text":"We've been using the name bpfd for a while now, but we were not the first to use it. There were projects before us that used the name bpfd , but since most were inactive, originally we didn't see this as an issue. More recently though the folks at Meta have started using the name systemd-bpfd for their proposed addition to systemd . In addition, we've been thinking about the future of the project, and particularly about security and whether it's wise to keep something with CAP_BPF capabilities running as a daemon - even if we've been very careful. This is similar to the issues faced by docker which eventually lead to the creation of podman. This issue led us down the path of redesigning the project to be daemonless. We'll be implementing these changes in the coming months and plan to perform our first release as bpfman in Q1 of 2024. The 'd' in bpfd stood for daemon, so with our new design and the confusion surrounding the name bpfd we though it was time for a change. Since we're a BPF manager, we're now bpfman! It's also a nice homage to podman , which we're big fans of.","title":"Why the name change?"},{"location":"blog/posts/bpfd-becomes-bpfman/#what-does-this-mean-for-me","text":"If you're a developer of bpfman you will need to update your Git remotes to point at our new organization and repository name. Github will redirect these for a while, but we recommend updating your remotes as soon as possible. If you're a user of bpfd or the bpfd-operator then version 0.3.1 will be the last release under the bpfd name. We will continue to support you as best we can, but we recommend upgrading to bpfman as soon as our first release is available.","title":"What does this mean for me?"},{"location":"blog/posts/bpfd-becomes-bpfman/#whats-next","text":"We've hinted at some of the changes we're planning, and of course, our roadmap is always available in Github . It's worth mentioning that we're also planning to expand our release packages to include RPMs and DEBs, making it even easier to install bpfman on your favorite Linux distribution.","title":"What's next?"},{"location":"blog/posts/bpfd-becomes-bpfman/#thanks","text":"We'd like to thank everyone who has contributed to bpfd over the years. We're excited about the future of bpfman and we hope you are too! Please bear with us as we make this transition, and if you have any questions or concerns, please reach out to us on Slack . We're in the '#bpfd' channel, but we'll be changing that to '#bpfman' soon.","title":"Thanks!"},{"location":"blog/posts/introduction-to-bpfman/","text":"bpfman: A Novel Way to Manage eBPF In today's cloud ecosystem, there's a demand for low-level system access to enable high-performance observability, security, and networking functionality for applications. Historically these features have been implemented in user space , however, the ability to program such functionality into the kernel itself can provide many benefits including (but not limited to) performance. Regardless, many Linux users still opt away from in-tree or kernel module development due to the slow rate of iteration and ensuing large management burden. eBPF has emerged as a technology in the Linux Kernel looking to change all that. eBPF is a simple and efficient way to dynamically load programs into the kernel at runtime, with safety and performance provided by the kernel itself using a Just-In-Time (JIT) compiler and verification process. There are a wide variety of program types one can create with eBPF, which include everything from networking applications to security systems. However, eBPF is still a fairly nascent technology and it's not all kittens and rainbows. The process of developing, testing, deploying, and maintaining eBPF programs is not a road well traveled yet, and the story gets even more complicated when you want to deploy your programs in a multi-node system, such as a Kubernetes cluster. It was these kinds of problems that motivated the creation of bpfman , a system daemon for loading and managing eBPF programs in both traditional systems and Kubernetes clusters. In this blog post, we'll discuss the problems bpfman can help solve, and how to deploy and use it. Current Challenges with Developing and Deploying eBPF Programs While some organizations have had success developing, deploying, and maintaining production software which includes eBPF programs, the barrier to entry is still very high. Following the basic eBPF development workflow, which often involves many hours trying to interpret and fix mind-bending eBPF verifier errors, the process of deploying a program in testing and staging environments often results in a lot of custom program loading and management functionality specific to the application. When moving to production systems in environments like Kubernetes clusters the operational considerations continue to compound. Security is another significant challenge, which we will cover in more depth in a follow-on blog. However, at a high level, applications that use eBPF typically load their own eBPF programs, which requires at least CAP_BPF. Many BPF programs and attach points require additional capabilities from CAP_SYS_PTRACE, CAP_NET_ADMIN and even including CAP_SYS_ADMIN. These privileges include capabilities that aren\u2019t strictly necessary for eBPF and are too coarsely grained to be useful. Since the processes that load eBPF are usually long-lived and often don\u2019t drop privileges it leaves a wide attack surface. While it doesn't solve all the ergonomic and maintenance problems associated with adopting eBPF, bpfman does try to address several of these issues -- particularly as it pertains to security and the lifecycle management of eBPF programs. In the coming sections, we will go into more depth about what eBPF does, and how it can help reduce the costs associated with deploying and managing eBPF-powered workloads. bpfman Overview The bpfman project provides a software stack that makes it easy to manage the full lifecycle of eBPF programs. In particular, it can load, unload, modify, and monitor eBPF programs on a single host, or across a full Kubernetes cluster. The key components of bpfman include the bpfman daemon itself which can run independently on any Linux box, an accompanying Kubernetes Operator designed to bring first-class support to clusters via Custom Resource Definitions (CRDs), and eBPF program packaging. These components will be covered in more detail in the following sections. bpfman Daemon The bpfman daemon works directly with the operating system to manage eBPF programs. It loads, updates, and unloads eBPF programs, pins maps, and provides visibility into the eBPF programs loaded on a system. Currently, bpfman fully supports XDP, TC, Tracepoint, uProbe, and kProbe eBPF programs. In addition, bpfman can display information about all types of eBPF programs loaded on a system whether they were loaded by bpfman or some other mechanism. bpfman is developed in the Rust programming language and uses Aya , an eBPF library which is also developed in Rust. When used on an individual server, bpfman runs as a system daemon, and applications communicate with it using a gRPC API. bpfman can also be used via a command line which in turn uses the gRPC API. The following is an example of using bpfman to load and attach an xdp program. bpfman load-from-image -g GLOBAL_u8=01 -i quay.io/bpfman-bytecode/xdp_pass:latest xdp -i eth0 -p 100 This architecture is depicted in the following diagram. Using bpfman in this manner significantly improves security because the API is secured using mTLS, and only bpfman needs the privileges required to load and manage eBPF programs and maps. Writing eBPF code is tough enough as it is. Typically, an eBPF-based application would need to also implement support for the lifecycle management of the required eBPF programs. bpfman does that for you and allows you to focus on developing your application. Another key functional advantage that bpfman offers over libbpf or the Cilium ebpf-go library is support for multiple XDP programs. Standard XDP only allows a single XDP program on a given interface, while bpfman supports loading multiple XDP programs on each interface using the multi-prog protocol defined in libxdp. This allows the user to add, delete, update, prioritize, and re-prioritize the multiple programs on each interface. There is also support to configure whether the flow of execution should terminate and return or continue to the next program in the list based on the return value. While TC natively supports multiple programs on each attach point, it lacks the controls and flexibility enabled by the multi-prog protocol. bpfman therefore also supports the same XDP multi-prog solution for TC programs which has the added benefit of a consistent user experience for both XDP and TC programs. eBPF programs are also difficult to debug on a system. The visibility provided by bpfman can be a key tool in understanding what is deployed and how they may interact. bpfman Kubernetes Support The benefits of bpfman are brought to Kubernetes by the bpfman operator. The bpfman operator is developed in Go using the Operator SDK framework, so it should be familiar to most Kubernetes application developers. The bpfman operator deploys a daemonset, containing both bpfman and the bpfman agent processes on each node. Rather than making requests directly to bpfman with the gRPC API or CLI as described above, Kubernetes applications use bpfman custom resource definitions (CRDs) to make requests to bpfman to load and attach eBPF programs. bpfman uses two types of CRDs; Program CRDs for each eBPF program type (referred to as *Program CRDs, where * = Xdp, Tc, etc.) created by the application to express the desired state of an eBPF program on the cluster, and per node BpfProgram CRDs created by the bpfman agent to report the current state of the eBPF program on each node. Using XDP as an example, the application can request that an XDP program be loaded on multiple nodes using the XdpProgram CRD, which includes the necessary information such as the bytecode image to load, interface to attach it to, and priority. An XdpProgram CRD that would do the same thing as the CLI command shown above on every node in a cluster is shown below. apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes spec : name : pass # Select all nodes nodeselector : {} interfaceselector : primarynodeinterface : true priority : 0 bytecode : image : url : quay.io/bpfman-bytecode/xdp_pass:latest globaldata : GLOBAL_u8 : - 0x01 The bpfman agent on each node watches for the *Program CRDs, and makes calls to the local instance of bpfman as necessary to ensure that the state on the local node reflects the state requested in the *Program CRD. The bpfman agent on each node in turn creates and updates a BpfProgram object for the *Program CRD that reflects the state of the program on that node and reports the eBPF map information for the program. The following is the BpfProgram CRD on one node for the above XdpProgram CRD. kubectl get bpfprograms.bpfman.io xdp-pass-all-nodes-bpfman-deployment-control-plane-eth0 -o yaml apiVersion : bpfman.io/v1alpha1 kind : BpfProgram metadata : annotations : bpfman.io.xdpprogramcontroller/interface : eth0 creationTimestamp : \"2023-08-29T22:08:12Z\" finalizers : - bpfman.io.xdpprogramcontroller/finalizer generation : 1 labels : bpfman.io/ownedByProgram : xdp-pass-all-nodes kubernetes.io/hostname : bpfman-deployment-control-plane name : xdp-pass-all-nodes-bpfman-deployment-control-plane-eth0 ownerReferences : - apiVersion : bpfman.io/v1alpha1 blockOwnerDeletion : true controller : true kind : XdpProgram name : xdp-pass-all-nodes uid : 838dc2f8-a348-427e-9dc4-f6a6ea621930 resourceVersion : \"2690\" uid : 5a622961-e5b0-44fe-98af-30756b2d0b62 spec : type : xdp status : conditions : - lastTransitionTime : \"2023-08-29T22:08:14Z\" message : Successfully loaded bpfProgram reason : bpfmanLoaded status : \"True\" type : Loaded Finally, the bpfman operator watches for updates to the BpfProgram objects and reports the global state of each eBPF program. If the program was successfully loaded on every selected node, it will report success, otherwise, it will identify the node(s) that had a problem. The following is the XdpProgram CRD as updated by the operator. kubectl get xdpprograms.bpfman.io xdp-pass-all-nodes -o yaml apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"bpfman.io/v1alpha1\",\"kind\":\"XdpProgram\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/name\":\"xdpprogram\"},\"name\":\"xdp-pass-all-nodes\"},\"spec\":{\"bytecode\":{\"image\":{\"url\":\"quay.io/bpfman-bytecode/xdp_pass:latest\"}},\"globaldata\":{\"GLOBAL_u8\":[1]},\"interfaceselector\":{\"primarynodeinterface\":true},\"nodeselector\":{},\"priority\":0,\"bpffunctionname\":\"pass\"}} creationTimestamp : \"2023-08-29T22:08:12Z\" finalizers : - bpfman.io.operator/finalizer generation : 2 labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes resourceVersion : \"2685\" uid : 838dc2f8-a348-427e-9dc4-f6a6ea621930 spec : bytecode : image : imagepullpolicy : IfNotPresent url : quay.io/bpfman-bytecode/xdp_pass:latest globaldata : GLOBAL_u8 : 0x01 interfaceselector : primarynodeinterface : true mapownerselector : {} nodeselector : {} priority : 0 proceedon : - pass - dispatcher_return name : pass status : conditions : - lastTransitionTime : \"2023-08-29T22:08:12Z\" message : Waiting for Program Object to be reconciled to all nodes reason : ProgramsNotYetLoaded status : \"True\" type : NotYetLoaded - lastTransitionTime : \"2023-08-29T22:08:12Z\" message : bpfProgramReconciliation Succeeded on all nodes reason : ReconcileSuccess status : \"True\" type : ReconcileSuccess More details about this process can be seen here eBPF program packaging The eBPF Bytecode Image specification was created as part of the bpfman project to define a way to package eBPF bytecode as OCI container images. Its use was illustrated in the CLI and XdpProgram CRD examples above in which the XDP program was loaded from quay.io/bpfman-bytecode/xdp_pass:latest . The initial motivation for this image spec was to facilitate the deployment of eBPF programs in container orchestration systems such as Kubernetes, where it is necessary to provide a portable way to distribute bytecode to all nodes that need it. However, bytecode images have proven useful on standalone Linux systems as well. When coupled with BPF CO-RE (Compile Once \u2013 Run Everywhere), portability is further enhanced in that applications can use the same bytecode images across different kernel versions without the need to recompile them for each version. Another benefit of bytecode containers is image signing. There is currently no way to sign and validate raw eBPF bytecode. However, the bytecode containers can be signed and validated by bpfman using sigstore to improve supply chain security. Key benefits of bpfman This section reviews some of the key benefits of bpfman. These benefits mostly apply to both standalone and Kubernetes deployments, but we will focus on the benefits for Kubernetes here. Security Probably the most compelling benefit of using bpfman is enhanced security. When using bpfman, only the bpfman daemon, which can be tightly controlled, needs the privileges required to load eBPF programs, while access to the API can be controlled via standard RBAC methods on a per-application and per-CRD basis. Additionally, the signing and validating of bytecode images enables supply chain security. Visibility and Debuggability eBPF programs can interact with each other in unexpected ways. The multi-program support described above helps control these interactions by providing a common mechanism to prioritize and control the flow between the programs. However, there can still be problems, and there may be eBPF programs running on nodes that were loaded by other mechanisms that you don\u2019t even know about. bpfman helps here too by reporting all of the eBPF programs running on all of the nodes in a Kubernetes cluster. Productivity As described above, managing the lifecycle of eBPF programs is something that each application currently needs to do on its own. It is even more complicated to manage the lifecycle of eBPF programs across a Kubernetes cluster. bpfman does this for you so you don't have to. eBPF bytecode images help here as well by simplifying the distribution of eBPF bytecode to multiple nodes in a cluster, and also allowing separate fine-grained versioning control for user space and kernel space code. Demonstration This demonstration is adapted from the instructions documented by Andrew Stoycos here . These instructions use kind and bpfman release v0.2.1. It should also be possible to run this demo on other environments such as minikube or an actual cluster. Another option is to build the code yourself and use make run-on-kind to create the cluster as is described in the given links. Then, start with step 5. Run the demo 1. Create Kind Cluster kind create cluster --name = test-bpfman 2. Deploy Cert manager kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml 3. Deploy bpfman Crds kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/bpfman-crds-install-v0.2.1.yaml 4. Deploy bpfman-operator kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/bpfman-operator-install-v0.2.1.yaml 5. Verify the deployment kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE bpfman bpfman-daemon-nkzpf 2 /2 Running 0 28s bpfman bpfman-operator-77d697fdd4-clrf7 2 /2 Running 0 33s cert-manager cert-manager-99bb69456-x8n84 1 /1 Running 0 57s cert-manager cert-manager-cainjector-ffb4747bb-pt4hr 1 /1 Running 0 57s cert-manager cert-manager-webhook-545bd5d7d8-z5brw 1 /1 Running 0 57s kube-system coredns-565d847f94-gjjft 1 /1 Running 0 61s kube-system coredns-565d847f94-mf2cq 1 /1 Running 0 61s kube-system etcd-test-bpfman-control-plane 1 /1 Running 0 76s kube-system kindnet-lv6f9 1 /1 Running 0 61s kube-system kube-apiserver-test-bpfman-control-plane 1 /1 Running 0 76s kube-system kube-controller-manager-test-bpfman-control-plane 1 /1 Running 0 77s kube-system kube-proxy-dtmvb 1 /1 Running 0 61s kube-system kube-scheduler-test-bpfman-control-plane 1 /1 Running 0 78s local-path-storage local-path-provisioner-684f458cdd-8gxxv 1 /1 Running 0 61s Note that we have the bpfman-operator, bpf-daemon and cert-manager pods running. 6. Deploy the XDP counter program and user space application kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/go-xdp-counter-install-v0.2.1.yaml 7. Confirm that the programs are loaded Userspace program: kubectl get pods -n go-xdp-counter NAME READY STATUS RESTARTS AGE go-xdp-counter-ds-9lpgp 0 /1 ContainerCreating 0 5s XDP program: kubectl get xdpprograms.bpfman.io -o wide NAME BPFFUNCTIONNAME NODESELECTOR PRIORITY INTERFACESELECTOR PROCEEDON go-xdp-counter-example stats {} 55 { \"primarynodeinterface\" :true } [ \"pass\" , \"dispatcher_return\" ] 8. Confirm that the counter program is counting packets. Notes: The counters are updated every 5 seconds, and stats are being collected for the pod's primary node interface, which may not have a lot of traffic. However, running the kubectl command below generates traffic on that interface, so run the command a few times and give it a few seconds in between to confirm whether the counters are incrementing. Replace \"go-xdp-counter-ds-9lpgp\" with the go-xdp-counter pod name for your deployment. kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail 2023 /09/05 16 :58:21 1204 packets received 2023 /09/05 16 :58:21 13741238 bytes received 2023 /09/05 16 :58:24 1220 packets received 2023 /09/05 16 :58:24 13744258 bytes received 2023 /09/05 16 :58:27 1253 packets received 2023 /09/05 16 :58:27 13750364 bytes received 9. Deploy the xdp-pass-all-nodes program with priority set to 50 and proceedon set to drop as shown below kubectl apply -f - <<EOF apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes spec : name : pass nodeselector : {} interfaceselector : primarynodeinterface : true priority : 50 proceedon : - drop bytecode : image : url : quay.io/bpfman-bytecode/xdp_pass:latest EOF 10. Verify both XDP programs are loaded. kubectl get xdpprograms.bpfman.io -o wide NAME BPFFUNCTIONNAME NODESELECTOR PRIORITY INTERFACESELECTOR PROCEEDON go-xdp-counter-example stats {} 55 { \"primarynodeinterface\" :true } [ \"pass\" , \"dispatcher_return\" ] xdp-pass-all-nodes pass {} 50 { \"primarynodeinterface\" :true } [ \"drop\" ] The priority setting determines the order in which programs attached to the same interface are executed by the dispatcher with a lower number being a higher priority. The go-xdp-counter-example program was loaded at priority 55, so the xdp-pass-all-nodes program will execute before the go-xdp-counter-example program. The proceedon setting tells the dispatcher whether to \"proceed\" to execute the next lower priority program attached to the same interface depending on the program's return value. When we set proceedon to drop, execution will proceed only if the program returns XDP_DROP . However, the xdp-pass-all-nodes program only returns XDP_PASS , so execution will terminate after it runs. Therefore, by loading the xdp-pass-all-nodes program in this way, we should have effectively stopped the go-xdp-counter-example program from running. Let's confirm that. 11. Verify that packet counts are not being updated anymore Run the following command several times kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail 2023 /09/05 17 :10:27 1395 packets received 2023 /09/05 17 :10:27 13799730 bytes received 2023 /09/05 17 :10:30 1395 packets received 2023 /09/05 17 :10:30 13799730 bytes received 2023 /09/05 17 :10:33 1395 packets received 2023 /09/05 17 :10:33 13799730 bytes received 12. Now, change the priority of the xdp-pass program to 60 kubectl apply -f - <<EOF apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes spec : name : pass # Select all nodes nodeselector : {} interfaceselector : primarynodeinterface : true priority : 60 proceedon : - drop bytecode : image : url : quay.io/bpfman-bytecode/xdp_pass:latest EOF 13. Confirm that packets are being counted again Run the following command several times kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail 2023 /09/05 17 :12:21 1435 packets received 2023 /09/05 17 :12:21 13806214 bytes received 2023 /09/05 17 :12:24 1505 packets received 2023 /09/05 17 :12:24 13815359 bytes received 2023 /09/05 17 :12:27 1558 packets received 2023 /09/05 17 :12:27 13823065 bytes received We can see that the counters are incrementing again. 14. Clean everything up Delete the programs kubectl delete xdpprogram xdp-pass-all-nodes kubectl delete -f https://github.com/bpfman/bpfman/releases/download/v0.2.0/go-xdp-counter-install-v0.2.0.yaml And/or, delete the whole kind cluster kind delete clusters test-bpfman Joining the bpfman community If you're interested in bpfman and want to get involved, you can connect with the community in multiple ways. If you have some simple questions or need some help feel free to start a discussion . If you find an issue, or you want to request a new feature, please create an issue . If you want something a little more synchronous, the project maintains a #bpfman channel on Kubernetes Slack and we have a weekly community meeting where everyone can join and bring topics to discuss about the project. We hope to see you there!","title":"bpfman: A Novel Way to Manage eBPF"},{"location":"blog/posts/introduction-to-bpfman/#bpfman-a-novel-way-to-manage-ebpf","text":"In today's cloud ecosystem, there's a demand for low-level system access to enable high-performance observability, security, and networking functionality for applications. Historically these features have been implemented in user space , however, the ability to program such functionality into the kernel itself can provide many benefits including (but not limited to) performance. Regardless, many Linux users still opt away from in-tree or kernel module development due to the slow rate of iteration and ensuing large management burden. eBPF has emerged as a technology in the Linux Kernel looking to change all that. eBPF is a simple and efficient way to dynamically load programs into the kernel at runtime, with safety and performance provided by the kernel itself using a Just-In-Time (JIT) compiler and verification process. There are a wide variety of program types one can create with eBPF, which include everything from networking applications to security systems. However, eBPF is still a fairly nascent technology and it's not all kittens and rainbows. The process of developing, testing, deploying, and maintaining eBPF programs is not a road well traveled yet, and the story gets even more complicated when you want to deploy your programs in a multi-node system, such as a Kubernetes cluster. It was these kinds of problems that motivated the creation of bpfman , a system daemon for loading and managing eBPF programs in both traditional systems and Kubernetes clusters. In this blog post, we'll discuss the problems bpfman can help solve, and how to deploy and use it.","title":"bpfman: A Novel Way to Manage eBPF"},{"location":"blog/posts/introduction-to-bpfman/#current-challenges-with-developing-and-deploying-ebpf-programs","text":"While some organizations have had success developing, deploying, and maintaining production software which includes eBPF programs, the barrier to entry is still very high. Following the basic eBPF development workflow, which often involves many hours trying to interpret and fix mind-bending eBPF verifier errors, the process of deploying a program in testing and staging environments often results in a lot of custom program loading and management functionality specific to the application. When moving to production systems in environments like Kubernetes clusters the operational considerations continue to compound. Security is another significant challenge, which we will cover in more depth in a follow-on blog. However, at a high level, applications that use eBPF typically load their own eBPF programs, which requires at least CAP_BPF. Many BPF programs and attach points require additional capabilities from CAP_SYS_PTRACE, CAP_NET_ADMIN and even including CAP_SYS_ADMIN. These privileges include capabilities that aren\u2019t strictly necessary for eBPF and are too coarsely grained to be useful. Since the processes that load eBPF are usually long-lived and often don\u2019t drop privileges it leaves a wide attack surface. While it doesn't solve all the ergonomic and maintenance problems associated with adopting eBPF, bpfman does try to address several of these issues -- particularly as it pertains to security and the lifecycle management of eBPF programs. In the coming sections, we will go into more depth about what eBPF does, and how it can help reduce the costs associated with deploying and managing eBPF-powered workloads.","title":"Current Challenges with Developing and Deploying eBPF Programs"},{"location":"blog/posts/introduction-to-bpfman/#bpfman-overview","text":"The bpfman project provides a software stack that makes it easy to manage the full lifecycle of eBPF programs. In particular, it can load, unload, modify, and monitor eBPF programs on a single host, or across a full Kubernetes cluster. The key components of bpfman include the bpfman daemon itself which can run independently on any Linux box, an accompanying Kubernetes Operator designed to bring first-class support to clusters via Custom Resource Definitions (CRDs), and eBPF program packaging. These components will be covered in more detail in the following sections.","title":"bpfman Overview"},{"location":"blog/posts/introduction-to-bpfman/#bpfman-daemon","text":"The bpfman daemon works directly with the operating system to manage eBPF programs. It loads, updates, and unloads eBPF programs, pins maps, and provides visibility into the eBPF programs loaded on a system. Currently, bpfman fully supports XDP, TC, Tracepoint, uProbe, and kProbe eBPF programs. In addition, bpfman can display information about all types of eBPF programs loaded on a system whether they were loaded by bpfman or some other mechanism. bpfman is developed in the Rust programming language and uses Aya , an eBPF library which is also developed in Rust. When used on an individual server, bpfman runs as a system daemon, and applications communicate with it using a gRPC API. bpfman can also be used via a command line which in turn uses the gRPC API. The following is an example of using bpfman to load and attach an xdp program. bpfman load-from-image -g GLOBAL_u8=01 -i quay.io/bpfman-bytecode/xdp_pass:latest xdp -i eth0 -p 100 This architecture is depicted in the following diagram. Using bpfman in this manner significantly improves security because the API is secured using mTLS, and only bpfman needs the privileges required to load and manage eBPF programs and maps. Writing eBPF code is tough enough as it is. Typically, an eBPF-based application would need to also implement support for the lifecycle management of the required eBPF programs. bpfman does that for you and allows you to focus on developing your application. Another key functional advantage that bpfman offers over libbpf or the Cilium ebpf-go library is support for multiple XDP programs. Standard XDP only allows a single XDP program on a given interface, while bpfman supports loading multiple XDP programs on each interface using the multi-prog protocol defined in libxdp. This allows the user to add, delete, update, prioritize, and re-prioritize the multiple programs on each interface. There is also support to configure whether the flow of execution should terminate and return or continue to the next program in the list based on the return value. While TC natively supports multiple programs on each attach point, it lacks the controls and flexibility enabled by the multi-prog protocol. bpfman therefore also supports the same XDP multi-prog solution for TC programs which has the added benefit of a consistent user experience for both XDP and TC programs. eBPF programs are also difficult to debug on a system. The visibility provided by bpfman can be a key tool in understanding what is deployed and how they may interact.","title":"bpfman Daemon"},{"location":"blog/posts/introduction-to-bpfman/#bpfman-kubernetes-support","text":"The benefits of bpfman are brought to Kubernetes by the bpfman operator. The bpfman operator is developed in Go using the Operator SDK framework, so it should be familiar to most Kubernetes application developers. The bpfman operator deploys a daemonset, containing both bpfman and the bpfman agent processes on each node. Rather than making requests directly to bpfman with the gRPC API or CLI as described above, Kubernetes applications use bpfman custom resource definitions (CRDs) to make requests to bpfman to load and attach eBPF programs. bpfman uses two types of CRDs; Program CRDs for each eBPF program type (referred to as *Program CRDs, where * = Xdp, Tc, etc.) created by the application to express the desired state of an eBPF program on the cluster, and per node BpfProgram CRDs created by the bpfman agent to report the current state of the eBPF program on each node. Using XDP as an example, the application can request that an XDP program be loaded on multiple nodes using the XdpProgram CRD, which includes the necessary information such as the bytecode image to load, interface to attach it to, and priority. An XdpProgram CRD that would do the same thing as the CLI command shown above on every node in a cluster is shown below. apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes spec : name : pass # Select all nodes nodeselector : {} interfaceselector : primarynodeinterface : true priority : 0 bytecode : image : url : quay.io/bpfman-bytecode/xdp_pass:latest globaldata : GLOBAL_u8 : - 0x01 The bpfman agent on each node watches for the *Program CRDs, and makes calls to the local instance of bpfman as necessary to ensure that the state on the local node reflects the state requested in the *Program CRD. The bpfman agent on each node in turn creates and updates a BpfProgram object for the *Program CRD that reflects the state of the program on that node and reports the eBPF map information for the program. The following is the BpfProgram CRD on one node for the above XdpProgram CRD. kubectl get bpfprograms.bpfman.io xdp-pass-all-nodes-bpfman-deployment-control-plane-eth0 -o yaml apiVersion : bpfman.io/v1alpha1 kind : BpfProgram metadata : annotations : bpfman.io.xdpprogramcontroller/interface : eth0 creationTimestamp : \"2023-08-29T22:08:12Z\" finalizers : - bpfman.io.xdpprogramcontroller/finalizer generation : 1 labels : bpfman.io/ownedByProgram : xdp-pass-all-nodes kubernetes.io/hostname : bpfman-deployment-control-plane name : xdp-pass-all-nodes-bpfman-deployment-control-plane-eth0 ownerReferences : - apiVersion : bpfman.io/v1alpha1 blockOwnerDeletion : true controller : true kind : XdpProgram name : xdp-pass-all-nodes uid : 838dc2f8-a348-427e-9dc4-f6a6ea621930 resourceVersion : \"2690\" uid : 5a622961-e5b0-44fe-98af-30756b2d0b62 spec : type : xdp status : conditions : - lastTransitionTime : \"2023-08-29T22:08:14Z\" message : Successfully loaded bpfProgram reason : bpfmanLoaded status : \"True\" type : Loaded Finally, the bpfman operator watches for updates to the BpfProgram objects and reports the global state of each eBPF program. If the program was successfully loaded on every selected node, it will report success, otherwise, it will identify the node(s) that had a problem. The following is the XdpProgram CRD as updated by the operator. kubectl get xdpprograms.bpfman.io xdp-pass-all-nodes -o yaml apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"bpfman.io/v1alpha1\",\"kind\":\"XdpProgram\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/name\":\"xdpprogram\"},\"name\":\"xdp-pass-all-nodes\"},\"spec\":{\"bytecode\":{\"image\":{\"url\":\"quay.io/bpfman-bytecode/xdp_pass:latest\"}},\"globaldata\":{\"GLOBAL_u8\":[1]},\"interfaceselector\":{\"primarynodeinterface\":true},\"nodeselector\":{},\"priority\":0,\"bpffunctionname\":\"pass\"}} creationTimestamp : \"2023-08-29T22:08:12Z\" finalizers : - bpfman.io.operator/finalizer generation : 2 labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes resourceVersion : \"2685\" uid : 838dc2f8-a348-427e-9dc4-f6a6ea621930 spec : bytecode : image : imagepullpolicy : IfNotPresent url : quay.io/bpfman-bytecode/xdp_pass:latest globaldata : GLOBAL_u8 : 0x01 interfaceselector : primarynodeinterface : true mapownerselector : {} nodeselector : {} priority : 0 proceedon : - pass - dispatcher_return name : pass status : conditions : - lastTransitionTime : \"2023-08-29T22:08:12Z\" message : Waiting for Program Object to be reconciled to all nodes reason : ProgramsNotYetLoaded status : \"True\" type : NotYetLoaded - lastTransitionTime : \"2023-08-29T22:08:12Z\" message : bpfProgramReconciliation Succeeded on all nodes reason : ReconcileSuccess status : \"True\" type : ReconcileSuccess More details about this process can be seen here","title":"bpfman Kubernetes Support"},{"location":"blog/posts/introduction-to-bpfman/#ebpf-program-packaging","text":"The eBPF Bytecode Image specification was created as part of the bpfman project to define a way to package eBPF bytecode as OCI container images. Its use was illustrated in the CLI and XdpProgram CRD examples above in which the XDP program was loaded from quay.io/bpfman-bytecode/xdp_pass:latest . The initial motivation for this image spec was to facilitate the deployment of eBPF programs in container orchestration systems such as Kubernetes, where it is necessary to provide a portable way to distribute bytecode to all nodes that need it. However, bytecode images have proven useful on standalone Linux systems as well. When coupled with BPF CO-RE (Compile Once \u2013 Run Everywhere), portability is further enhanced in that applications can use the same bytecode images across different kernel versions without the need to recompile them for each version. Another benefit of bytecode containers is image signing. There is currently no way to sign and validate raw eBPF bytecode. However, the bytecode containers can be signed and validated by bpfman using sigstore to improve supply chain security.","title":"eBPF program packaging"},{"location":"blog/posts/introduction-to-bpfman/#key-benefits-of-bpfman","text":"This section reviews some of the key benefits of bpfman. These benefits mostly apply to both standalone and Kubernetes deployments, but we will focus on the benefits for Kubernetes here.","title":"Key benefits of bpfman"},{"location":"blog/posts/introduction-to-bpfman/#security","text":"Probably the most compelling benefit of using bpfman is enhanced security. When using bpfman, only the bpfman daemon, which can be tightly controlled, needs the privileges required to load eBPF programs, while access to the API can be controlled via standard RBAC methods on a per-application and per-CRD basis. Additionally, the signing and validating of bytecode images enables supply chain security.","title":"Security"},{"location":"blog/posts/introduction-to-bpfman/#visibility-and-debuggability","text":"eBPF programs can interact with each other in unexpected ways. The multi-program support described above helps control these interactions by providing a common mechanism to prioritize and control the flow between the programs. However, there can still be problems, and there may be eBPF programs running on nodes that were loaded by other mechanisms that you don\u2019t even know about. bpfman helps here too by reporting all of the eBPF programs running on all of the nodes in a Kubernetes cluster.","title":"Visibility and Debuggability"},{"location":"blog/posts/introduction-to-bpfman/#productivity","text":"As described above, managing the lifecycle of eBPF programs is something that each application currently needs to do on its own. It is even more complicated to manage the lifecycle of eBPF programs across a Kubernetes cluster. bpfman does this for you so you don't have to. eBPF bytecode images help here as well by simplifying the distribution of eBPF bytecode to multiple nodes in a cluster, and also allowing separate fine-grained versioning control for user space and kernel space code.","title":"Productivity"},{"location":"blog/posts/introduction-to-bpfman/#demonstration","text":"This demonstration is adapted from the instructions documented by Andrew Stoycos here . These instructions use kind and bpfman release v0.2.1. It should also be possible to run this demo on other environments such as minikube or an actual cluster. Another option is to build the code yourself and use make run-on-kind to create the cluster as is described in the given links. Then, start with step 5.","title":"Demonstration"},{"location":"blog/posts/introduction-to-bpfman/#run-the-demo","text":"1. Create Kind Cluster kind create cluster --name = test-bpfman 2. Deploy Cert manager kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml 3. Deploy bpfman Crds kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/bpfman-crds-install-v0.2.1.yaml 4. Deploy bpfman-operator kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/bpfman-operator-install-v0.2.1.yaml 5. Verify the deployment kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE bpfman bpfman-daemon-nkzpf 2 /2 Running 0 28s bpfman bpfman-operator-77d697fdd4-clrf7 2 /2 Running 0 33s cert-manager cert-manager-99bb69456-x8n84 1 /1 Running 0 57s cert-manager cert-manager-cainjector-ffb4747bb-pt4hr 1 /1 Running 0 57s cert-manager cert-manager-webhook-545bd5d7d8-z5brw 1 /1 Running 0 57s kube-system coredns-565d847f94-gjjft 1 /1 Running 0 61s kube-system coredns-565d847f94-mf2cq 1 /1 Running 0 61s kube-system etcd-test-bpfman-control-plane 1 /1 Running 0 76s kube-system kindnet-lv6f9 1 /1 Running 0 61s kube-system kube-apiserver-test-bpfman-control-plane 1 /1 Running 0 76s kube-system kube-controller-manager-test-bpfman-control-plane 1 /1 Running 0 77s kube-system kube-proxy-dtmvb 1 /1 Running 0 61s kube-system kube-scheduler-test-bpfman-control-plane 1 /1 Running 0 78s local-path-storage local-path-provisioner-684f458cdd-8gxxv 1 /1 Running 0 61s Note that we have the bpfman-operator, bpf-daemon and cert-manager pods running. 6. Deploy the XDP counter program and user space application kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v0.2.1/go-xdp-counter-install-v0.2.1.yaml 7. Confirm that the programs are loaded Userspace program: kubectl get pods -n go-xdp-counter NAME READY STATUS RESTARTS AGE go-xdp-counter-ds-9lpgp 0 /1 ContainerCreating 0 5s XDP program: kubectl get xdpprograms.bpfman.io -o wide NAME BPFFUNCTIONNAME NODESELECTOR PRIORITY INTERFACESELECTOR PROCEEDON go-xdp-counter-example stats {} 55 { \"primarynodeinterface\" :true } [ \"pass\" , \"dispatcher_return\" ] 8. Confirm that the counter program is counting packets. Notes: The counters are updated every 5 seconds, and stats are being collected for the pod's primary node interface, which may not have a lot of traffic. However, running the kubectl command below generates traffic on that interface, so run the command a few times and give it a few seconds in between to confirm whether the counters are incrementing. Replace \"go-xdp-counter-ds-9lpgp\" with the go-xdp-counter pod name for your deployment. kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail 2023 /09/05 16 :58:21 1204 packets received 2023 /09/05 16 :58:21 13741238 bytes received 2023 /09/05 16 :58:24 1220 packets received 2023 /09/05 16 :58:24 13744258 bytes received 2023 /09/05 16 :58:27 1253 packets received 2023 /09/05 16 :58:27 13750364 bytes received 9. Deploy the xdp-pass-all-nodes program with priority set to 50 and proceedon set to drop as shown below kubectl apply -f - <<EOF apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes spec : name : pass nodeselector : {} interfaceselector : primarynodeinterface : true priority : 50 proceedon : - drop bytecode : image : url : quay.io/bpfman-bytecode/xdp_pass:latest EOF 10. Verify both XDP programs are loaded. kubectl get xdpprograms.bpfman.io -o wide NAME BPFFUNCTIONNAME NODESELECTOR PRIORITY INTERFACESELECTOR PROCEEDON go-xdp-counter-example stats {} 55 { \"primarynodeinterface\" :true } [ \"pass\" , \"dispatcher_return\" ] xdp-pass-all-nodes pass {} 50 { \"primarynodeinterface\" :true } [ \"drop\" ] The priority setting determines the order in which programs attached to the same interface are executed by the dispatcher with a lower number being a higher priority. The go-xdp-counter-example program was loaded at priority 55, so the xdp-pass-all-nodes program will execute before the go-xdp-counter-example program. The proceedon setting tells the dispatcher whether to \"proceed\" to execute the next lower priority program attached to the same interface depending on the program's return value. When we set proceedon to drop, execution will proceed only if the program returns XDP_DROP . However, the xdp-pass-all-nodes program only returns XDP_PASS , so execution will terminate after it runs. Therefore, by loading the xdp-pass-all-nodes program in this way, we should have effectively stopped the go-xdp-counter-example program from running. Let's confirm that. 11. Verify that packet counts are not being updated anymore Run the following command several times kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail 2023 /09/05 17 :10:27 1395 packets received 2023 /09/05 17 :10:27 13799730 bytes received 2023 /09/05 17 :10:30 1395 packets received 2023 /09/05 17 :10:30 13799730 bytes received 2023 /09/05 17 :10:33 1395 packets received 2023 /09/05 17 :10:33 13799730 bytes received 12. Now, change the priority of the xdp-pass program to 60 kubectl apply -f - <<EOF apiVersion : bpfman.io/v1alpha1 kind : XdpProgram metadata : labels : app.kubernetes.io/name : xdpprogram name : xdp-pass-all-nodes spec : name : pass # Select all nodes nodeselector : {} interfaceselector : primarynodeinterface : true priority : 60 proceedon : - drop bytecode : image : url : quay.io/bpfman-bytecode/xdp_pass:latest EOF 13. Confirm that packets are being counted again Run the following command several times kubectl logs go-xdp-counter-ds-9lpgp -n go-xdp-counter | tail 2023 /09/05 17 :12:21 1435 packets received 2023 /09/05 17 :12:21 13806214 bytes received 2023 /09/05 17 :12:24 1505 packets received 2023 /09/05 17 :12:24 13815359 bytes received 2023 /09/05 17 :12:27 1558 packets received 2023 /09/05 17 :12:27 13823065 bytes received We can see that the counters are incrementing again. 14. Clean everything up Delete the programs kubectl delete xdpprogram xdp-pass-all-nodes kubectl delete -f https://github.com/bpfman/bpfman/releases/download/v0.2.0/go-xdp-counter-install-v0.2.0.yaml And/or, delete the whole kind cluster kind delete clusters test-bpfman","title":"Run the demo"},{"location":"blog/posts/introduction-to-bpfman/#joining-the-bpfman-community","text":"If you're interested in bpfman and want to get involved, you can connect with the community in multiple ways. If you have some simple questions or need some help feel free to start a discussion . If you find an issue, or you want to request a new feature, please create an issue . If you want something a little more synchronous, the project maintains a #bpfman channel on Kubernetes Slack and we have a weekly community meeting where everyone can join and bring topics to discuss about the project. We hope to see you there!","title":"Joining the bpfman community"},{"location":"blog/posts/sled-integration/","text":"bpfman's Shift Towards a Daemonless Design and Using Sled: a High Performance Embedded Database As part of issue #860 the community has steadily been converting all of the internal state management to go through a sled database instance which is part of the larger effort to make bpfman completely damonless . This article will go over the reasons behind the change and dive into some of the details of the actual implementation. Why? State management in bpfman has always been a headache, not because there's a huge amount of disparate data but there's multiple representations of the same data. Additionally the delicate filesystem interactions and layout previously used to ensure persistence across restarts often led to issues. Understanding the existing flow of data in bpfman can help make this a bit clearer: With this design there was a lot of data wrangling required to convert the tonic generated rust bindings for the protocol buffer API into data structures that were useful for bpfman. Specifically, data would arrive via GRPC server as specified in bpfman.v1.rs where rust types are inferred from the protobuf definition. In rpc.rs data was then converted to an internal set of structures defined in command.rs . Prior to pull request #683 there was an explosion of types, with each bpfman command having it's own set of internal structures and enums. Now, most of the data for a program that bpfman needs internally for all commands to manage an eBPF program is stored in the ProgramData structure, which we'll take a deeper look at a bit later. Additionally, there is extra complexity for XDP and TC program types which rely on an eBPF dispatcher program to provide multi-program support on a single network interface, however this article will try to instead focus on the simpler examples. The tree of data stored by bpfman is quite complex and this is made even more complicated since bpfman has to be persistent across restarts. To support this, raw data was often flushed to disk in the form of JSON files (all types in command.rs needed to implement serde's Serialize and Deserialize ). Specific significance would also be encoded to bpfman's directory structure, i.e all program related information was encoded in /run/bpfd/programs/<ID> . The extra infrastructure and failure modes introduced by this process was a constant headache, pushing the community to find a better solution. Why Sled? Sled is an open source project described in github as \"the champagne of beta embedded databases\". The \"reasons\" for choosing an embedded database from the project website are pretty much spot on: Embedded databases are useful in several cases : - you want to store data on disk, without facing the complexity of files - you want to be simple, without operating an external database - you want to be fast, without paying network costs - using disk storage as a building block in your system As discussed in the previous section, persistence across restarts, is one of bpfman's core design constraints, and with sled we almost get it for free! Additionally due to the pervasive nature of data management to bpfman's core workflow the data-store needed to be kept as simple and light weight as possible, ruling out heavier production-ready external database systems such as MySQL or Redis. Now this mostly focused on why embedded dbs in general, but why did we choose sled...well because it's written in :crab: Rust :crab: of course! Apart from the obvious we took a small dive into the project before rewriting everything by transitioning the OCI bytecode image library to use the db rather than the filesystem. Overall the experience was extremely positive due to the following: No more dealing directly with the filesystem, the sled instance is flushed to the fs automatically every 500 ms by default and for good measure we manually flush it before shutting down. The API is extremely simple, traditional get and insert operations function as expected. Error handling with sled:Error is relatively simple and easy to map explicitly to a bpfmanError The db \"tree\" concept makes it easy to have separate key-spaces within the same instance. Transitioning to Sled Using the new embedded database started with the creation of a sled instance which could be easily shared across all of the modules in bpfman. To do this we utilized a globally available [ lazy_static ] variable called ROOT_DB in main.rs : #[cfg(not(test))] lazy_static ! { pub static ref ROOT_DB : Db = Config :: default () . path ( STDIR_DB ) . open () . expect ( \"Unable to open root database\" ); } #[cfg(test)] lazy_static ! { pub static ref ROOT_DB : Db = Config :: default () . temporary ( true ) . open () . expect ( \"Unable to open temporary root database\" ); } This block creates OR opens the filesystem backed database at /var/lib/bpfman/db database only when the ROOT_DB variable is first accessed, and also allows for the creation of a temporary db instance if running in unit tests. With this setup all of the modules within bpfman can now easily access the database instance by simply using it i.e use crate::ROOT_DB . Next the existing bpfman structures needed to be flattened in order to work with the db, the central ProgramData can be used to demonstrate how this was completed. Prior to the recent sled conversion that structure looked like: /// ProgramInfo stores information about bpf programs that are loaded and managed /// by bpfd. #[derive(Debug, Serialize, Deserialize, Clone, Default)] pub ( crate ) struct ProgramData { // known at load time, set by user name : String , location : Location , metadata : HashMap < String , String > , global_data : HashMap < String , Vec < u8 >> , map_owner_id : Option < u32 > , // populated after load kernel_info : Option < KernelProgramInfo > , map_pin_path : Option < PathBuf > , maps_used_by : Option < Vec < u32 >> , // program_bytes is used to temporarily cache the raw program data during // the loading process. It MUST be cleared following a load so that there // is not a long lived copy of the program data living on the heap. #[serde(skip_serializing, skip_deserializing)] program_bytes : Vec < u8 > , } This worked well enough, but as mentioned before the process of flushing the data to disk involved manual serialization to JSON, which needed to occur at a specific point in time (following program load) which made disaster recovery almost impossible and could sometimes result in lost or partially reconstructed state. With sled the first idea was to completely flatten ALL of bpfman's data into a single key-space, so that program.name now simply turns into a db.get(\"program_<ID>_name\") , however removing all of the core structures would have resulted in a complex diff which would have been hard to review and merge. Therefore a more staged approach was taken, the ProgramData structure was kept around, and now looks like: /// ProgramInfo stores information about bpf programs that are loaded and managed /// by bpfman. #[derive(Debug, Clone)] pub ( crate ) struct ProgramData { // Prior to load this will be a temporary Tree with a random ID, following // load it will be replaced with the main program database tree. db_tree : sled :: Tree , // populated after load, randomly generated prior to load. id : u32 , // program_bytes is used to temporarily cache the raw program data during // the loading process. It MUST be cleared following a load so that there // is not a long lived copy of the program data living on the heap. program_bytes : Vec < u8 > , } All of the fields are now removed in favor of a private reference to the unique [ sled::Tree ] instance for this ProgramData which is named using the unique kernel id for the program. Each sled::Tree represents a single logical key-space / namespace / bucket which allows key generation to be kept simple, i.e db.get(\"program_<ID>_name\") now can be db_tree_prog_0000.get(\"program_name) . Additionally getters and setters are now built for each existing field so that access to the db can be controlled and the serialization/deserialization process can be hidden from the caller: ... pub ( crate ) fn set_name ( & mut self , name : & str ) -> Result < (), BpfmanError > { self . insert ( \"name\" , name . as_bytes ()) } pub ( crate ) fn get_name ( & self ) -> Result < String , BpfmanError > { self . get ( \"name\" ). map ( | v | bytes_to_string ( & v )) } ... Therefore, ProgramData is now less of a container for program data and more of a wrapper for accessing program data. The getters/setters act as a bridge between standard Rust types and the raw bytes stored in the database, i.e the [ sled::IVec type]. Once this was completed for all the relevant fields on all the relevant types, see pull request #874 , the data bpfman needed for it's managed eBPF programs was now automatically synced to disk :partying_face: Tradeoffs All design changes come with some tradeoffs: for bpfman's conversion to using sled the main negative ended up being with the complexity introduced with the [ sled::IVec type]. It is basically just a thread-safe reference-counting pointer to a raw byte slice, and the only type raw database operations can be performed with. Previously when using serde_json all serialization/deserialization was automatically handled, however with sled the conversion is manual handled internally. Therefore, instead of a library handling the conversion of a rust string ( std::string::String ) to raw bytes &[u8] bpfman has to handle it internally, using [ std::string::String::as_bytes ] and bpfman::utils::bytes_to_string : pub ( crate ) fn bytes_to_string ( bytes : & [ u8 ]) -> String { String :: from_utf8 ( bytes . to_vec ()). expect ( \"failed to convert &[u8] to string\" ) } For strings, conversion was simple enough, but when working with more complex rust data types like HashMaps and Vectors this became a bit more of an issue. For Vectors , we simply flatten the structure into a group of key/values with indexes encoded into the key: pub ( crate ) fn set_kernel_map_ids ( & mut self , map_ids : Vec < u32 > ) -> Result < (), BpfmanError > { let map_ids = map_ids . iter (). map ( | i | i . to_ne_bytes ()). collect :: < Vec < _ >> (); map_ids . iter (). enumerate (). try_for_each ( | ( i , v ) | { sled_insert ( & self . db_tree , format ! ( \"kernel_map_ids_{i}\" ). as_str (), v ) }) } The sled scan_prefix(<K>) api then allows for easy fetching and rebuilding of the vector: pub ( crate ) fn get_kernel_map_ids ( & self ) -> Result < Vec < u32 > , BpfmanError > { self . db_tree . scan_prefix ( \"kernel_map_ids_\" . as_bytes ()) . map ( | n | n . map ( | ( _ , v ) | bytes_to_u32 ( v . to_vec ()))) . map ( | n | { n . map_err ( | e | { BpfmanError :: DatabaseError ( \"Failed to get map ids\" . to_string (), e . to_string ()) }) }) . collect () } For HashMaps , we follow a similar paradigm, except the map key is encoded in the database key: pub ( crate ) fn set_metadata ( & mut self , data : HashMap < String , String > , ) -> Result < (), BpfmanError > { data . iter (). try_for_each ( | ( k , v ) | { sled_insert ( & self . db_tree , format ! ( \"metadata_{k}\" ). as_str (), v . as_bytes (), ) }) } pub ( crate ) fn get_metadata ( & self ) -> Result < HashMap < String , String > , BpfmanError > { self . db_tree . scan_prefix ( \"metadata_\" ) . map ( | n | { n . map ( | ( k , v ) | { ( bytes_to_string ( & k ) . strip_prefix ( \"metadata_\" ) . unwrap () . to_string (), bytes_to_string ( & v ). to_string (), ) }) }) . map ( | n | { n . map_err ( | e | { BpfmanError :: DatabaseError ( \"Failed to get metadata\" . to_string (), e . to_string ()) }) }) . collect () } The same result could be achieved by creating individual database trees for each Vector / HashMap instance, however our goal was to keep the layout as flat as possible. Although this resulted in some extra complexity within the data layer, the overall benefits still outweighed the extra code once the conversion was complete. Moving forward and Getting Involved Once the conversion to sled is fully complete, see issue #860 , the project will be able to completely transition to becoming a library without having to worry about data and state management. If you are interested in in memory databases, eBPF, Rust, or any of the technologies discussed today please don't hesitate to reach out on [kubernetes slack at #bpfman ] or join one of the community meetings to get involved.","title":"bpfman's Shift Towards a Daemonless Design and Using Sled: a High Performance Embedded Database"},{"location":"blog/posts/sled-integration/#bpfmans-shift-towards-a-daemonless-design-and-using-sled-a-high-performance-embedded-database","text":"As part of issue #860 the community has steadily been converting all of the internal state management to go through a sled database instance which is part of the larger effort to make bpfman completely damonless . This article will go over the reasons behind the change and dive into some of the details of the actual implementation.","title":"bpfman's Shift Towards a Daemonless Design and Using Sled: a High Performance Embedded Database"},{"location":"blog/posts/sled-integration/#why","text":"State management in bpfman has always been a headache, not because there's a huge amount of disparate data but there's multiple representations of the same data. Additionally the delicate filesystem interactions and layout previously used to ensure persistence across restarts often led to issues. Understanding the existing flow of data in bpfman can help make this a bit clearer: With this design there was a lot of data wrangling required to convert the tonic generated rust bindings for the protocol buffer API into data structures that were useful for bpfman. Specifically, data would arrive via GRPC server as specified in bpfman.v1.rs where rust types are inferred from the protobuf definition. In rpc.rs data was then converted to an internal set of structures defined in command.rs . Prior to pull request #683 there was an explosion of types, with each bpfman command having it's own set of internal structures and enums. Now, most of the data for a program that bpfman needs internally for all commands to manage an eBPF program is stored in the ProgramData structure, which we'll take a deeper look at a bit later. Additionally, there is extra complexity for XDP and TC program types which rely on an eBPF dispatcher program to provide multi-program support on a single network interface, however this article will try to instead focus on the simpler examples. The tree of data stored by bpfman is quite complex and this is made even more complicated since bpfman has to be persistent across restarts. To support this, raw data was often flushed to disk in the form of JSON files (all types in command.rs needed to implement serde's Serialize and Deserialize ). Specific significance would also be encoded to bpfman's directory structure, i.e all program related information was encoded in /run/bpfd/programs/<ID> . The extra infrastructure and failure modes introduced by this process was a constant headache, pushing the community to find a better solution.","title":"Why?"},{"location":"blog/posts/sled-integration/#why-sled","text":"Sled is an open source project described in github as \"the champagne of beta embedded databases\". The \"reasons\" for choosing an embedded database from the project website are pretty much spot on: Embedded databases are useful in several cases : - you want to store data on disk, without facing the complexity of files - you want to be simple, without operating an external database - you want to be fast, without paying network costs - using disk storage as a building block in your system As discussed in the previous section, persistence across restarts, is one of bpfman's core design constraints, and with sled we almost get it for free! Additionally due to the pervasive nature of data management to bpfman's core workflow the data-store needed to be kept as simple and light weight as possible, ruling out heavier production-ready external database systems such as MySQL or Redis. Now this mostly focused on why embedded dbs in general, but why did we choose sled...well because it's written in :crab: Rust :crab: of course! Apart from the obvious we took a small dive into the project before rewriting everything by transitioning the OCI bytecode image library to use the db rather than the filesystem. Overall the experience was extremely positive due to the following: No more dealing directly with the filesystem, the sled instance is flushed to the fs automatically every 500 ms by default and for good measure we manually flush it before shutting down. The API is extremely simple, traditional get and insert operations function as expected. Error handling with sled:Error is relatively simple and easy to map explicitly to a bpfmanError The db \"tree\" concept makes it easy to have separate key-spaces within the same instance.","title":"Why Sled?"},{"location":"blog/posts/sled-integration/#transitioning-to-sled","text":"Using the new embedded database started with the creation of a sled instance which could be easily shared across all of the modules in bpfman. To do this we utilized a globally available [ lazy_static ] variable called ROOT_DB in main.rs : #[cfg(not(test))] lazy_static ! { pub static ref ROOT_DB : Db = Config :: default () . path ( STDIR_DB ) . open () . expect ( \"Unable to open root database\" ); } #[cfg(test)] lazy_static ! { pub static ref ROOT_DB : Db = Config :: default () . temporary ( true ) . open () . expect ( \"Unable to open temporary root database\" ); } This block creates OR opens the filesystem backed database at /var/lib/bpfman/db database only when the ROOT_DB variable is first accessed, and also allows for the creation of a temporary db instance if running in unit tests. With this setup all of the modules within bpfman can now easily access the database instance by simply using it i.e use crate::ROOT_DB . Next the existing bpfman structures needed to be flattened in order to work with the db, the central ProgramData can be used to demonstrate how this was completed. Prior to the recent sled conversion that structure looked like: /// ProgramInfo stores information about bpf programs that are loaded and managed /// by bpfd. #[derive(Debug, Serialize, Deserialize, Clone, Default)] pub ( crate ) struct ProgramData { // known at load time, set by user name : String , location : Location , metadata : HashMap < String , String > , global_data : HashMap < String , Vec < u8 >> , map_owner_id : Option < u32 > , // populated after load kernel_info : Option < KernelProgramInfo > , map_pin_path : Option < PathBuf > , maps_used_by : Option < Vec < u32 >> , // program_bytes is used to temporarily cache the raw program data during // the loading process. It MUST be cleared following a load so that there // is not a long lived copy of the program data living on the heap. #[serde(skip_serializing, skip_deserializing)] program_bytes : Vec < u8 > , } This worked well enough, but as mentioned before the process of flushing the data to disk involved manual serialization to JSON, which needed to occur at a specific point in time (following program load) which made disaster recovery almost impossible and could sometimes result in lost or partially reconstructed state. With sled the first idea was to completely flatten ALL of bpfman's data into a single key-space, so that program.name now simply turns into a db.get(\"program_<ID>_name\") , however removing all of the core structures would have resulted in a complex diff which would have been hard to review and merge. Therefore a more staged approach was taken, the ProgramData structure was kept around, and now looks like: /// ProgramInfo stores information about bpf programs that are loaded and managed /// by bpfman. #[derive(Debug, Clone)] pub ( crate ) struct ProgramData { // Prior to load this will be a temporary Tree with a random ID, following // load it will be replaced with the main program database tree. db_tree : sled :: Tree , // populated after load, randomly generated prior to load. id : u32 , // program_bytes is used to temporarily cache the raw program data during // the loading process. It MUST be cleared following a load so that there // is not a long lived copy of the program data living on the heap. program_bytes : Vec < u8 > , } All of the fields are now removed in favor of a private reference to the unique [ sled::Tree ] instance for this ProgramData which is named using the unique kernel id for the program. Each sled::Tree represents a single logical key-space / namespace / bucket which allows key generation to be kept simple, i.e db.get(\"program_<ID>_name\") now can be db_tree_prog_0000.get(\"program_name) . Additionally getters and setters are now built for each existing field so that access to the db can be controlled and the serialization/deserialization process can be hidden from the caller: ... pub ( crate ) fn set_name ( & mut self , name : & str ) -> Result < (), BpfmanError > { self . insert ( \"name\" , name . as_bytes ()) } pub ( crate ) fn get_name ( & self ) -> Result < String , BpfmanError > { self . get ( \"name\" ). map ( | v | bytes_to_string ( & v )) } ... Therefore, ProgramData is now less of a container for program data and more of a wrapper for accessing program data. The getters/setters act as a bridge between standard Rust types and the raw bytes stored in the database, i.e the [ sled::IVec type]. Once this was completed for all the relevant fields on all the relevant types, see pull request #874 , the data bpfman needed for it's managed eBPF programs was now automatically synced to disk :partying_face:","title":"Transitioning to Sled"},{"location":"blog/posts/sled-integration/#tradeoffs","text":"All design changes come with some tradeoffs: for bpfman's conversion to using sled the main negative ended up being with the complexity introduced with the [ sled::IVec type]. It is basically just a thread-safe reference-counting pointer to a raw byte slice, and the only type raw database operations can be performed with. Previously when using serde_json all serialization/deserialization was automatically handled, however with sled the conversion is manual handled internally. Therefore, instead of a library handling the conversion of a rust string ( std::string::String ) to raw bytes &[u8] bpfman has to handle it internally, using [ std::string::String::as_bytes ] and bpfman::utils::bytes_to_string : pub ( crate ) fn bytes_to_string ( bytes : & [ u8 ]) -> String { String :: from_utf8 ( bytes . to_vec ()). expect ( \"failed to convert &[u8] to string\" ) } For strings, conversion was simple enough, but when working with more complex rust data types like HashMaps and Vectors this became a bit more of an issue. For Vectors , we simply flatten the structure into a group of key/values with indexes encoded into the key: pub ( crate ) fn set_kernel_map_ids ( & mut self , map_ids : Vec < u32 > ) -> Result < (), BpfmanError > { let map_ids = map_ids . iter (). map ( | i | i . to_ne_bytes ()). collect :: < Vec < _ >> (); map_ids . iter (). enumerate (). try_for_each ( | ( i , v ) | { sled_insert ( & self . db_tree , format ! ( \"kernel_map_ids_{i}\" ). as_str (), v ) }) } The sled scan_prefix(<K>) api then allows for easy fetching and rebuilding of the vector: pub ( crate ) fn get_kernel_map_ids ( & self ) -> Result < Vec < u32 > , BpfmanError > { self . db_tree . scan_prefix ( \"kernel_map_ids_\" . as_bytes ()) . map ( | n | n . map ( | ( _ , v ) | bytes_to_u32 ( v . to_vec ()))) . map ( | n | { n . map_err ( | e | { BpfmanError :: DatabaseError ( \"Failed to get map ids\" . to_string (), e . to_string ()) }) }) . collect () } For HashMaps , we follow a similar paradigm, except the map key is encoded in the database key: pub ( crate ) fn set_metadata ( & mut self , data : HashMap < String , String > , ) -> Result < (), BpfmanError > { data . iter (). try_for_each ( | ( k , v ) | { sled_insert ( & self . db_tree , format ! ( \"metadata_{k}\" ). as_str (), v . as_bytes (), ) }) } pub ( crate ) fn get_metadata ( & self ) -> Result < HashMap < String , String > , BpfmanError > { self . db_tree . scan_prefix ( \"metadata_\" ) . map ( | n | { n . map ( | ( k , v ) | { ( bytes_to_string ( & k ) . strip_prefix ( \"metadata_\" ) . unwrap () . to_string (), bytes_to_string ( & v ). to_string (), ) }) }) . map ( | n | { n . map_err ( | e | { BpfmanError :: DatabaseError ( \"Failed to get metadata\" . to_string (), e . to_string ()) }) }) . collect () } The same result could be achieved by creating individual database trees for each Vector / HashMap instance, however our goal was to keep the layout as flat as possible. Although this resulted in some extra complexity within the data layer, the overall benefits still outweighed the extra code once the conversion was complete.","title":"Tradeoffs"},{"location":"blog/posts/sled-integration/#moving-forward-and-getting-involved","text":"Once the conversion to sled is fully complete, see issue #860 , the project will be able to completely transition to becoming a library without having to worry about data and state management. If you are interested in in memory databases, eBPF, Rust, or any of the technologies discussed today please don't hesitate to reach out on [kubernetes slack at #bpfman ] or join one of the community meetings to get involved.","title":"Moving forward and Getting Involved"},{"location":"blog/posts/community/2024/2024-01-04-meeting/","text":"Community Meeting: January 4, 2024 Welcome to 2024! Welcome to the first bpfman Community Meeting of 2024. We are happy to start off a new year and excited for all the changes in store for bpfman in 2024! Below were some of the discussion points from this weeks Community Meeting. bpfman-csi Needs To Become Its Own Binary Kubernetes Support For Attaching uprobes In Containers Building The Community bpfman-csi Needs To Become Its Own Binary Some of the next work items for bpfman revolve around removing the async code from the code base, make bpfman-core a rust library, and removing all the gRPC logic. Dave ( @dave-tucker ) is currently investigating this. One area to help out is to take the bpfman-csi thread and making it it's own binary. This may require making bpfman a bin and lib crate (which is fine, just needs a lib.rs and to be very careful about what we\u2019re exporting). Andrew ( @astoycos ) is starting to take a look at this. Kubernetes Support For Attaching uprobes In Containers Base support for attaching uprobes in containers is currently merged. Andre ( @anfredette ) pushed PR#875 for the integration with Kubernetes. The hard problems are solved, like getting the Container PID, but the current PR has some shortcuts to get the functionality working before the holiday break. So the PR#875 is not ready for review, but Dave ( @dave-tucker ) and Andre ( @anfredette ) may have a quick review to verify the design principles. Building The Community Short discussion on building the Community. In a previous meeting, Dave ( @dave-tucker ) suggested capturing the meeting minutes in blogs. By placing in a blog, they become searchable from search engines. Billy ( @billy99 ) re-raised this topic and volunteered to start capturing the content. In future meetings, we may use the transcript feature from Google Meet to capture the content and try generating the blog via ChatGTP. Light-hearted Moments and Casual Conversations Amidst the technical discussions, the community members took a moment to share some light-hearted moments and casual conversations. Topics ranged from the challenges of post-holiday credit card bills to the complexities of managing family schedules during exam week. The discussion touched on the quirks of public school rules and the unique challenges of parenting during exam periods. The meeting ended on a friendly note, with plans for further collaboration and individual tasks assigned for the upcoming days. Participants expressed their commitment to pushing updates and improvements, with a promise to reconvene in the near future. Attendees Andre Fredette (Red Hat) Andrew Stoycos (Red Hat) Billy McFall (Red Hat) Dave Tucker (Red Hat) bpfman Community Info A friendly reminder that the Community Meetings are every Thursday 10am-11am Eastern US Time and all are welcome! Google Meet joining info: Google Meet Or dial: (US) +1 984-221-0859 PIN: 613 588 790# Agenda Document","title":"Community Meeting: January 4, 2024"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#community-meeting-january-4-2024","text":"","title":"Community Meeting: January 4, 2024"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#welcome-to-2024","text":"Welcome to the first bpfman Community Meeting of 2024. We are happy to start off a new year and excited for all the changes in store for bpfman in 2024! Below were some of the discussion points from this weeks Community Meeting. bpfman-csi Needs To Become Its Own Binary Kubernetes Support For Attaching uprobes In Containers Building The Community","title":"Welcome to 2024!"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#bpfman-csi-needs-to-become-its-own-binary","text":"Some of the next work items for bpfman revolve around removing the async code from the code base, make bpfman-core a rust library, and removing all the gRPC logic. Dave ( @dave-tucker ) is currently investigating this. One area to help out is to take the bpfman-csi thread and making it it's own binary. This may require making bpfman a bin and lib crate (which is fine, just needs a lib.rs and to be very careful about what we\u2019re exporting). Andrew ( @astoycos ) is starting to take a look at this.","title":"bpfman-csi Needs To Become Its Own Binary"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#kubernetes-support-for-attaching-uprobes-in-containers","text":"Base support for attaching uprobes in containers is currently merged. Andre ( @anfredette ) pushed PR#875 for the integration with Kubernetes. The hard problems are solved, like getting the Container PID, but the current PR has some shortcuts to get the functionality working before the holiday break. So the PR#875 is not ready for review, but Dave ( @dave-tucker ) and Andre ( @anfredette ) may have a quick review to verify the design principles.","title":"Kubernetes Support For Attaching uprobes In Containers"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#building-the-community","text":"Short discussion on building the Community. In a previous meeting, Dave ( @dave-tucker ) suggested capturing the meeting minutes in blogs. By placing in a blog, they become searchable from search engines. Billy ( @billy99 ) re-raised this topic and volunteered to start capturing the content. In future meetings, we may use the transcript feature from Google Meet to capture the content and try generating the blog via ChatGTP.","title":"Building The Community"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#light-hearted-moments-and-casual-conversations","text":"Amidst the technical discussions, the community members took a moment to share some light-hearted moments and casual conversations. Topics ranged from the challenges of post-holiday credit card bills to the complexities of managing family schedules during exam week. The discussion touched on the quirks of public school rules and the unique challenges of parenting during exam periods. The meeting ended on a friendly note, with plans for further collaboration and individual tasks assigned for the upcoming days. Participants expressed their commitment to pushing updates and improvements, with a promise to reconvene in the near future.","title":"Light-hearted Moments and Casual Conversations"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#attendees","text":"Andre Fredette (Red Hat) Andrew Stoycos (Red Hat) Billy McFall (Red Hat) Dave Tucker (Red Hat)","title":"Attendees"},{"location":"blog/posts/community/2024/2024-01-04-meeting/#bpfman-community-info","text":"A friendly reminder that the Community Meetings are every Thursday 10am-11am Eastern US Time and all are welcome! Google Meet joining info: Google Meet Or dial: (US) +1 984-221-0859 PIN: 613 588 790# Agenda Document","title":"bpfman Community Info"},{"location":"design/daemonless/","text":"Daemonless bpfd Introduction The bpfd daemon is a userspace daemon that runs on the host and responds to gRPC API requests over a unix socket, to load, unload and list the eBPF programs on a host. The rationale behind running as a daemon was because something needs to be listening on the unix socket for API requests, and that we also maintain some state in-memory about the programs that have been loaded. However, since this daemon requires root privileges to load and unload eBPF programs it is a security risk for this to be a long-running - even with the mitigations we have in place to drop privileges and run as a non-root user. This risk is equivalent to that of something like Docker . This document describes the design of a daemonless bpfd, which is a bpfd that runs only runs when required, for example, to load or unload an eBPF program. Design The daemonless bpfd is a single binary that collects some of the functionality from both bpfd and bpfctl. :note: Daemonless, not rootless. Since CAP_BPF (and more) is required to load and unload eBPF programs, we will still need to run as root. But at least we can run as root for a shorter period of time. Command: bpfd system service This command will run the bpfd gRPC API server - for one or more of the gRPC API services we support. It will listen on a unix socket (or tcp socket) for API requests - provided via a positional argument, defaulting to unix:///var/run/bpfd.sock . It will shutdown after a timeout of inactivity - provided by a --timeout flag defaulting to 5 seconds. It will support being run as a systemd service, via socket activation, which will allow it to be started on demand when a request is made to the unix socket. When in this mode it will not create the unix socket itself, but will instead use LISTEN_FDS to determine the file descriptor of the unix socket to use. Usage in local development (or packaged in a container) is still possible by running as follows: sudo bpfd --timeout=0 unix:///var/run/bpfd.sock :note: The bpfd user and group will be deprecated. We will also remove some of the unit-file complexity (i.e directories) and handle this in bpfd itself. Command: bpfd load file As the name suggests, this command will load an eBPF program from a file. This was formerly bpfctl load-from-file . Command: bpfd load image As the name suggests, this command will load an eBPF program from a container image. This was formerly bpfctl load-from-image . Command: bpfd unload This command will unload an eBPF program. This was formerly bpfctl unload . Command: bpfd list This command will list the eBPF programs that are currently loaded. This was formerly bpfctl list . Command: bpfd pull This command will pull the bpfd container image from a registry. This was formerly bpfctl pull . Command: bpfd images This command will list the bpfd container images that are available. This command didn't exist, but makes sense to add. Command: bpfd version This command will print the version of bpfd. This command didn't exist, but makes sense to add. State Management This is perhaps the most significant change from how bpfd currently works. Currently bpfd maintains state in-memory about the programs that have been loaded (by bpfd, and the kernel). Some of this state is flushed to disk, so if bpfd is restarted it can reconstruct it. Flushing to disk and state reconstruction is cumbersome at present and having to move all state management out of in-memory stores is a forcing function to improve this. We will replace the existing state management with sled , which gives us a familiar API to work with while also being fast, reliable and persistent. Metrics and Monitoring While adding metrics and monitoring is not a goal of this design, it should nevertheless be a consideration. In order to provide metrics to Prometheus or OpenTelemetry we will require an additional exporter process. We can either: Use the bpfd socket and retrieve metrics via the gRPC API Place state access + metrics gathering functions in a library, such that they could be used directly by the exporter process without requiring the bpfd socket. The latter would be more inline with how podman-prometheus-exporter works. The benefit here is that, the metrics exporter process can be long running with less privileges - whereas if it were to hit the API over the socket it would effectively negate the point of being daemonless in the first place since collection will likley occur more frequently than the timeout on the socket. Benefits The benefits of this design are: No long-running daemon with root privileges No need to run as a non-root user, this is important since the number of capabilities required is only getting larger. We only need to ship a single binary. We can use systemd socket activation to start bpfd on demand + timeout after a period of inactivity. Forcs us to fix state management, since we can never rely on in-memory state. Bpfd becomes more modular - if we wish to add programs for runtime enforcement, metrics, or any other purpose then it's design is decoupled from that of bpfd. It could be another binary, or a subcommand on the CLI etc... Drawbacks None yet. Backwards Compatibility The bpfctl command will be removed and all functionality folded into bpfd The bpfd command will be renamed to bpfd system service","title":"Daemonless"},{"location":"design/daemonless/#daemonless-bpfd","text":"","title":"Daemonless bpfd"},{"location":"design/daemonless/#introduction","text":"The bpfd daemon is a userspace daemon that runs on the host and responds to gRPC API requests over a unix socket, to load, unload and list the eBPF programs on a host. The rationale behind running as a daemon was because something needs to be listening on the unix socket for API requests, and that we also maintain some state in-memory about the programs that have been loaded. However, since this daemon requires root privileges to load and unload eBPF programs it is a security risk for this to be a long-running - even with the mitigations we have in place to drop privileges and run as a non-root user. This risk is equivalent to that of something like Docker . This document describes the design of a daemonless bpfd, which is a bpfd that runs only runs when required, for example, to load or unload an eBPF program.","title":"Introduction"},{"location":"design/daemonless/#design","text":"The daemonless bpfd is a single binary that collects some of the functionality from both bpfd and bpfctl. :note: Daemonless, not rootless. Since CAP_BPF (and more) is required to load and unload eBPF programs, we will still need to run as root. But at least we can run as root for a shorter period of time.","title":"Design"},{"location":"design/daemonless/#command-bpfd-system-service","text":"This command will run the bpfd gRPC API server - for one or more of the gRPC API services we support. It will listen on a unix socket (or tcp socket) for API requests - provided via a positional argument, defaulting to unix:///var/run/bpfd.sock . It will shutdown after a timeout of inactivity - provided by a --timeout flag defaulting to 5 seconds. It will support being run as a systemd service, via socket activation, which will allow it to be started on demand when a request is made to the unix socket. When in this mode it will not create the unix socket itself, but will instead use LISTEN_FDS to determine the file descriptor of the unix socket to use. Usage in local development (or packaged in a container) is still possible by running as follows: sudo bpfd --timeout=0 unix:///var/run/bpfd.sock :note: The bpfd user and group will be deprecated. We will also remove some of the unit-file complexity (i.e directories) and handle this in bpfd itself.","title":"Command: bpfd system service"},{"location":"design/daemonless/#command-bpfd-load-file","text":"As the name suggests, this command will load an eBPF program from a file. This was formerly bpfctl load-from-file .","title":"Command: bpfd load file"},{"location":"design/daemonless/#command-bpfd-load-image","text":"As the name suggests, this command will load an eBPF program from a container image. This was formerly bpfctl load-from-image .","title":"Command: bpfd load image"},{"location":"design/daemonless/#command-bpfd-unload","text":"This command will unload an eBPF program. This was formerly bpfctl unload .","title":"Command: bpfd unload"},{"location":"design/daemonless/#command-bpfd-list","text":"This command will list the eBPF programs that are currently loaded. This was formerly bpfctl list .","title":"Command: bpfd list"},{"location":"design/daemonless/#command-bpfd-pull","text":"This command will pull the bpfd container image from a registry. This was formerly bpfctl pull .","title":"Command: bpfd pull"},{"location":"design/daemonless/#command-bpfd-images","text":"This command will list the bpfd container images that are available. This command didn't exist, but makes sense to add.","title":"Command: bpfd images"},{"location":"design/daemonless/#command-bpfd-version","text":"This command will print the version of bpfd. This command didn't exist, but makes sense to add.","title":"Command: bpfd version"},{"location":"design/daemonless/#state-management","text":"This is perhaps the most significant change from how bpfd currently works. Currently bpfd maintains state in-memory about the programs that have been loaded (by bpfd, and the kernel). Some of this state is flushed to disk, so if bpfd is restarted it can reconstruct it. Flushing to disk and state reconstruction is cumbersome at present and having to move all state management out of in-memory stores is a forcing function to improve this. We will replace the existing state management with sled , which gives us a familiar API to work with while also being fast, reliable and persistent.","title":"State Management"},{"location":"design/daemonless/#metrics-and-monitoring","text":"While adding metrics and monitoring is not a goal of this design, it should nevertheless be a consideration. In order to provide metrics to Prometheus or OpenTelemetry we will require an additional exporter process. We can either: Use the bpfd socket and retrieve metrics via the gRPC API Place state access + metrics gathering functions in a library, such that they could be used directly by the exporter process without requiring the bpfd socket. The latter would be more inline with how podman-prometheus-exporter works. The benefit here is that, the metrics exporter process can be long running with less privileges - whereas if it were to hit the API over the socket it would effectively negate the point of being daemonless in the first place since collection will likley occur more frequently than the timeout on the socket.","title":"Metrics and Monitoring"},{"location":"design/daemonless/#benefits","text":"The benefits of this design are: No long-running daemon with root privileges No need to run as a non-root user, this is important since the number of capabilities required is only getting larger. We only need to ship a single binary. We can use systemd socket activation to start bpfd on demand + timeout after a period of inactivity. Forcs us to fix state management, since we can never rely on in-memory state. Bpfd becomes more modular - if we wish to add programs for runtime enforcement, metrics, or any other purpose then it's design is decoupled from that of bpfd. It could be another binary, or a subcommand on the CLI etc...","title":"Benefits"},{"location":"design/daemonless/#drawbacks","text":"None yet.","title":"Drawbacks"},{"location":"design/daemonless/#backwards-compatibility","text":"The bpfctl command will be removed and all functionality folded into bpfd The bpfd command will be renamed to bpfd system service","title":"Backwards Compatibility"},{"location":"developer-guide/api-spec/","text":"API Specification","title":"Kubernetes CRD API-Reference"},{"location":"developer-guide/api-spec/#api-specification","text":"","title":"API Specification"},{"location":"developer-guide/configuration/","text":"Configuration bpfman Configuration File bpfman looks for a configuration file to be present at /etc/bpfman/bpfman.toml . If no file is found, defaults are assumed. There is an example at scripts/bpfman.toml , similar to: [interfaces] [interface.eth0] xdp_mode = \"hw\" # Valid xdp modes are \"hw\", \"skb\" and \"drv\". Default: \"skb\". Config Section: [interfaces] This section of the configuration file allows the XDP Mode for a given interface to be set. If not set, the default value of skb will be used. Multiple interfaces can be configured. [interfaces] [interfaces.eth0] xdp_mode = \"drv\" [interfaces.eth1] xdp_mode = \"hw\" [interfaces.eth2] xdp_mode = \"skb\" Valid fields: xdp_mode : XDP Mode for a given interface. Valid values: [\"drv\"|\"hw\"|\"skb\"]","title":"Configuration"},{"location":"developer-guide/configuration/#configuration","text":"","title":"Configuration"},{"location":"developer-guide/configuration/#bpfman-configuration-file","text":"bpfman looks for a configuration file to be present at /etc/bpfman/bpfman.toml . If no file is found, defaults are assumed. There is an example at scripts/bpfman.toml , similar to: [interfaces] [interface.eth0] xdp_mode = \"hw\" # Valid xdp modes are \"hw\", \"skb\" and \"drv\". Default: \"skb\".","title":"bpfman Configuration File"},{"location":"developer-guide/configuration/#config-section-interfaces","text":"This section of the configuration file allows the XDP Mode for a given interface to be set. If not set, the default value of skb will be used. Multiple interfaces can be configured. [interfaces] [interfaces.eth0] xdp_mode = \"drv\" [interfaces.eth1] xdp_mode = \"hw\" [interfaces.eth2] xdp_mode = \"skb\" Valid fields: xdp_mode : XDP Mode for a given interface. Valid values: [\"drv\"|\"hw\"|\"skb\"]","title":"Config Section: [interfaces]"},{"location":"developer-guide/debugging/","text":"Debugging using VSCode and lldb on a remote machine or VM Install code-lldb vscode extension Add a configuration to .vscode/launch.json like the following (customizing for a given system using the comment in the configuration file): { \"name\" : \"Remote debug bpfman\" , \"type\" : \"lldb\" , \"request\" : \"launch\" , \"program\" : \"<ABSOLUTE_PATH>/github.com/bpfman/bpfman/target/debug/bpfman\" , // Local path to latest debug binary. \"initCommands\" : [ \"platform select remote-linux\" , // Execute `platform list` for a list of available remote platform plugins. \"platform connect connect://<IP_ADDRESS_OF_VM>:8081\" , // replace <IP_ADDRESS_OF_VM> \"settings set target.inherit-env false\" , ], \"env\" : { \"RUST_LOG\" : \"debug\" }, \"cargo\" : { \"args\" : [ \"build\" , \"--bin=bpfman\" , \"--package=bpfman\" ], \"filter\" : { \"name\" : \"bpfman\" , \"kind\" : \"bin\" } }, \"cwd\" : \"${workspaceFolder}\" , } , On the VM or Server install lldb-server : dnf based OS: sudo dnf install lldb apt based OS: sudo apt install lldb Start lldb-server on the VM or Server (make sure to do this in the ~/home directory) cd ~ sudo lldb-server platform --server --listen 0 .0.0.0:8081 Add breakpoints as needed via the vscode GUI and then hit F5 to start debugging!","title":"Debugging"},{"location":"developer-guide/debugging/#debugging-using-vscode-and-lldb-on-a-remote-machine-or-vm","text":"Install code-lldb vscode extension Add a configuration to .vscode/launch.json like the following (customizing for a given system using the comment in the configuration file): { \"name\" : \"Remote debug bpfman\" , \"type\" : \"lldb\" , \"request\" : \"launch\" , \"program\" : \"<ABSOLUTE_PATH>/github.com/bpfman/bpfman/target/debug/bpfman\" , // Local path to latest debug binary. \"initCommands\" : [ \"platform select remote-linux\" , // Execute `platform list` for a list of available remote platform plugins. \"platform connect connect://<IP_ADDRESS_OF_VM>:8081\" , // replace <IP_ADDRESS_OF_VM> \"settings set target.inherit-env false\" , ], \"env\" : { \"RUST_LOG\" : \"debug\" }, \"cargo\" : { \"args\" : [ \"build\" , \"--bin=bpfman\" , \"--package=bpfman\" ], \"filter\" : { \"name\" : \"bpfman\" , \"kind\" : \"bin\" } }, \"cwd\" : \"${workspaceFolder}\" , } , On the VM or Server install lldb-server : dnf based OS: sudo dnf install lldb apt based OS: sudo apt install lldb Start lldb-server on the VM or Server (make sure to do this in the ~/home directory) cd ~ sudo lldb-server platform --server --listen 0 .0.0.0:8081 Add breakpoints as needed via the vscode GUI and then hit F5 to start debugging!","title":"Debugging using VSCode and lldb on a remote machine or VM"},{"location":"developer-guide/develop-operator/","text":"Developing the bpfman-operator This section is intended to give developer level details regarding the layout and design of the bpfman-operator. At its core the operator was implemented using the operator-sdk framework which make those docs another good resource if anything is missed here. High level design overview This repository houses two main processes, the bpfman-agent and the bpfman-operator along with CRD api definitions for BpfProgram and *Program Objects. The following diagram depicts how all these components work together to create a functioning operator. Building and deploying For building and deploying the bpfman-operator simply see the attached Make help output. make help Usage: make <target> General help Display this help. Local Dependencies kustomize Download kustomize locally if necessary. controller-gen Download controller-gen locally if necessary. envtest Download envtest-setup locally if necessary. opm Download opm locally if necessary. Development manifests Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects. generate Generate ALL auto-generated code. generate-register Generate register code see all ` zz_generated.register.go ` files. generate-deepcopy Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations see all ` zz_generated.register.go ` files. generate-typed-clients Generate typed client code generate-typed-listers Generate typed listers code generate-typed-informers Generate typed informers code fmt Run go fmt against code. verify Verify all the autogenerated code test Run Unit tests. test-integration Run Integration tests. bundle Generate bundle manifests and metadata, then validate generated files. build-release-yamls Generate the crd install bundle for a specific release version. Build build Build bpfman-operator and bpfman-agent binaries. build-images Build bpfman, bpfman-agent, and bpfman-operator images. push-images Push bpfman, bpfman-agent, bpfman-operator images. load-images-kind Load bpfman, bpfman-agent, and bpfman-operator images into the running local kind devel cluster. bundle-build Build the bundle image. bundle-push Push the bundle image. catalog-build Build a catalog image. catalog-push Push a catalog image. CRD Deployment install Install CRDs into the K8s cluster specified in ~/.kube/config. uninstall Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found = true to ignore resource not found errors during deletion. Vanilla K8s Deployment setup-kind Setup Kind cluster deploy Deploy bpfman-operator to the K8s cluster specified in ~/.kube/config with the csi driver initialized. undeploy Undeploy bpfman-operator from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found = true to ignore resource not found errors during deletion. kind-reload-images Reload locally build images into a kind cluster and restart the ds and deployment so they ' re picked up. run-on-kind Kind Deploy runs the bpfman-operator on a local kind cluster using local builds of bpfman, bpfman-agent, and bpfman-operator Openshift Deployment deploy-openshift Deploy bpfman-operator to the Openshift cluster specified in ~/.kube/config. undeploy-openshift Undeploy bpfman-operator from the Openshift cluster specified in ~/.kube/config. Call with ignore-not-found = true to ignore resource not found errors during deletion. Running Locally in KIND To run locally in a kind cluster with an up to date build simply run: make run-on-kind The container images used for bpfman , bpfman-agent , and bpfman-operator can also be manually configured, by default local image builds will be used for the kind deployment. BPFMAN_IMG = <your/image/url> BPFMAN_AGENT_IMG = <your/image/url> BPFMAN_OPERATOR_IMG = <your/image/url> make run-on-kind Then rebuild and load a fresh build run: make kind-reload-images Which will rebuild the bpfman-operator, bpfman-agent, and bpfman images and load them into the kind cluster. Testing Locally To run all of the Unit Tests defined in the bpfman-operator controller code simply run make test . To run Integration Tests locally: Build the images locally with the int-test tag. BPFMAN_AGENT_IMG = quay.io/bpfman/bpfman-agent:int-test BPFMAN_IMG = quay.io/bpfman/bpfman:int-test BPFMAN_OPERATOR_IMG = quay.io/bpfman/bpfman-operator:int-test make build-images Run the integration test suite. BPFMAN_AGENT_IMG = quay.io/bpfman/bpfman-agent:int-test BPFMAN_IMG = quay.io/bpfman/bpfman:int-test BPFMAN_OPERATOR_IMG = quay.io/bpfman/bpfman-operator:int-test make test-integration Additionally the integration test can be configured with the following environment variables: KEEP_TEST_CLUSTER : If set to true the test cluster will not be torn down after the integration test suite completes. USE_EXISTING_KIND_CLUSTER : If this is set to the name of the existing kind cluster the integration test suite will use that cluster instead of creating a new one. Project Layout The bpfman-operator project layout is guided by the recommendations from both the operator-sdk framework and the standard golang project-layout . The following is a brief description of the main directories and their contents. NOTE: Bolded directories contain auto-generated code /apis : Contains the K8s CRD api definitions( *_types.go ) for each version along with the auto-generated register and deepcopy methods( zz_generated.deepcopy.go and zz_generate_register.go ). /bundle : Contains the OLM bundle manifests and metadata for the operator. More details can be found in the operator-sdk documentation. /cmd : Contains the main entry-points for the bpfman-operator and bpfman-agent processes. /config : Contains the configuration files for launching the bpfman-operator on a cluster. /bpfman-deployment : Contains static deployment yamls for the bpfman-daemon, this includes two containers, one for bpfman and the other for the bpfman-agent . This DaemonSet yaml is NOT deployed statically by kustomize, instead it's statically copied into the operator image which is then responsible for deploying and configuring the bpfman-daemon DaemonSet. Lastly, this directory also contains the default config used to configure the bpfman-daemon, along with the cert-manager certificates used to encrypt communication between the bpfman-agent and bpfman. /bpfman-operator-deployment: Contains the static deployment yaml for the bpfman-operator. This is deployed statically by kustomize. /crd : Contains the CRD manifests for all of the bpfman-operator APIs. /bases : Is where the actual CRD definitions are stored. These definitions are auto-generated by controller-gen . /default : Contains the default deployment configuration for the bpfman-operator. /manifests : Contains the bases for generating OLM manifests. /openshift : Contains the Openshift specific deployment configuration for the bpfman-operator. /prometheus : Contains the prometheus manifests used to deploy Prometheus to a cluster. At the time of writing this the bpfman-operator is NOT exposing any metrics to prometheus, but this is a future goal. /rbac : Contains rbac yamls for getting bpfman and the bpfman-operator up and running on Kubernetes. /bpfman-agent : Contains the rbac yamls for the bpfman-agent. They are automatically generated by kubebuilder via build tags in the bpfman-agent controller code. /bpfman-operator : Contains the rbac yamls for the bpfman-operator. They are automatically generated by kubebuilder via build tags in the bpfman-operator controller code. /samples : Contains sample CR definitions that can be deployed by users for each of our supported APIs. /scorecard : Contains the scorecard manifests used to deploy scorecard to a cluster. At the time of writing this the bpfman-operator is NOT running any scorecard tests. /test : Contains the test manifests used to deploy the bpfman-operator to a kind cluster for integration testing. /controllers : Contains the controller implementations for all of the bpfman-operator APIs. Each controller is responsible for reconciling the state of the cluster with the desired state defined by the user. This is where the source of truth for the auto-generated RBAC can be found, keep an eye out for //+kubebuilder:rbac:groups=bpfman.io comment tags. /bpfmanagent : Contains the controller implementations which reconcile user created *Program types to multiple BpfProgram objects. /bpfmanoperator : Contains the controller implementations which reconcile global BpfProgram object state back to the user by ensuring the user created *Program objects are reporting the correct status. /hack : Contains any scripts+static files used by the bpfman-operator to facilitate development. /internal : Contains all private library code and is used by the bpfman-operator and bpfman-agent controllers. /pkg : Contains all public library code this is consumed externally and internally. /client : Contains the autogenerated clientset, informers and listers for all of the bpfman-operator APIs. These are autogenerated by the k8s.io/code-generator project , and can be consumed by users wishing to programmatically interact with bpfman specific APIs. /helpers : Contains helper functions which can be consumed by users wishing to programmatically interact with bpfman specific APIs. /test/integration : Contains integration tests for the bpfman-operator. These tests are run against a kind cluster and are responsible for testing the bpfman-operator in a real cluster environment. It uses the kubernetes-testing-framework project to programmatically spin-up all of the required infrastructure for our unit tests. Makefile : Contains all of the make targets used to build, test, and generate code used by the bpfman-operator.","title":"Developing the bpfman-operator"},{"location":"developer-guide/develop-operator/#developing-the-bpfman-operator","text":"This section is intended to give developer level details regarding the layout and design of the bpfman-operator. At its core the operator was implemented using the operator-sdk framework which make those docs another good resource if anything is missed here.","title":"Developing the bpfman-operator"},{"location":"developer-guide/develop-operator/#high-level-design-overview","text":"This repository houses two main processes, the bpfman-agent and the bpfman-operator along with CRD api definitions for BpfProgram and *Program Objects. The following diagram depicts how all these components work together to create a functioning operator.","title":"High level design overview"},{"location":"developer-guide/develop-operator/#building-and-deploying","text":"For building and deploying the bpfman-operator simply see the attached Make help output. make help Usage: make <target> General help Display this help. Local Dependencies kustomize Download kustomize locally if necessary. controller-gen Download controller-gen locally if necessary. envtest Download envtest-setup locally if necessary. opm Download opm locally if necessary. Development manifests Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects. generate Generate ALL auto-generated code. generate-register Generate register code see all ` zz_generated.register.go ` files. generate-deepcopy Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations see all ` zz_generated.register.go ` files. generate-typed-clients Generate typed client code generate-typed-listers Generate typed listers code generate-typed-informers Generate typed informers code fmt Run go fmt against code. verify Verify all the autogenerated code test Run Unit tests. test-integration Run Integration tests. bundle Generate bundle manifests and metadata, then validate generated files. build-release-yamls Generate the crd install bundle for a specific release version. Build build Build bpfman-operator and bpfman-agent binaries. build-images Build bpfman, bpfman-agent, and bpfman-operator images. push-images Push bpfman, bpfman-agent, bpfman-operator images. load-images-kind Load bpfman, bpfman-agent, and bpfman-operator images into the running local kind devel cluster. bundle-build Build the bundle image. bundle-push Push the bundle image. catalog-build Build a catalog image. catalog-push Push a catalog image. CRD Deployment install Install CRDs into the K8s cluster specified in ~/.kube/config. uninstall Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found = true to ignore resource not found errors during deletion. Vanilla K8s Deployment setup-kind Setup Kind cluster deploy Deploy bpfman-operator to the K8s cluster specified in ~/.kube/config with the csi driver initialized. undeploy Undeploy bpfman-operator from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found = true to ignore resource not found errors during deletion. kind-reload-images Reload locally build images into a kind cluster and restart the ds and deployment so they ' re picked up. run-on-kind Kind Deploy runs the bpfman-operator on a local kind cluster using local builds of bpfman, bpfman-agent, and bpfman-operator Openshift Deployment deploy-openshift Deploy bpfman-operator to the Openshift cluster specified in ~/.kube/config. undeploy-openshift Undeploy bpfman-operator from the Openshift cluster specified in ~/.kube/config. Call with ignore-not-found = true to ignore resource not found errors during deletion.","title":"Building and deploying"},{"location":"developer-guide/develop-operator/#running-locally-in-kind","text":"To run locally in a kind cluster with an up to date build simply run: make run-on-kind The container images used for bpfman , bpfman-agent , and bpfman-operator can also be manually configured, by default local image builds will be used for the kind deployment. BPFMAN_IMG = <your/image/url> BPFMAN_AGENT_IMG = <your/image/url> BPFMAN_OPERATOR_IMG = <your/image/url> make run-on-kind Then rebuild and load a fresh build run: make kind-reload-images Which will rebuild the bpfman-operator, bpfman-agent, and bpfman images and load them into the kind cluster.","title":"Running Locally in KIND"},{"location":"developer-guide/develop-operator/#testing-locally","text":"To run all of the Unit Tests defined in the bpfman-operator controller code simply run make test . To run Integration Tests locally: Build the images locally with the int-test tag. BPFMAN_AGENT_IMG = quay.io/bpfman/bpfman-agent:int-test BPFMAN_IMG = quay.io/bpfman/bpfman:int-test BPFMAN_OPERATOR_IMG = quay.io/bpfman/bpfman-operator:int-test make build-images Run the integration test suite. BPFMAN_AGENT_IMG = quay.io/bpfman/bpfman-agent:int-test BPFMAN_IMG = quay.io/bpfman/bpfman:int-test BPFMAN_OPERATOR_IMG = quay.io/bpfman/bpfman-operator:int-test make test-integration Additionally the integration test can be configured with the following environment variables: KEEP_TEST_CLUSTER : If set to true the test cluster will not be torn down after the integration test suite completes. USE_EXISTING_KIND_CLUSTER : If this is set to the name of the existing kind cluster the integration test suite will use that cluster instead of creating a new one.","title":"Testing Locally"},{"location":"developer-guide/develop-operator/#project-layout","text":"The bpfman-operator project layout is guided by the recommendations from both the operator-sdk framework and the standard golang project-layout . The following is a brief description of the main directories and their contents. NOTE: Bolded directories contain auto-generated code /apis : Contains the K8s CRD api definitions( *_types.go ) for each version along with the auto-generated register and deepcopy methods( zz_generated.deepcopy.go and zz_generate_register.go ). /bundle : Contains the OLM bundle manifests and metadata for the operator. More details can be found in the operator-sdk documentation. /cmd : Contains the main entry-points for the bpfman-operator and bpfman-agent processes. /config : Contains the configuration files for launching the bpfman-operator on a cluster. /bpfman-deployment : Contains static deployment yamls for the bpfman-daemon, this includes two containers, one for bpfman and the other for the bpfman-agent . This DaemonSet yaml is NOT deployed statically by kustomize, instead it's statically copied into the operator image which is then responsible for deploying and configuring the bpfman-daemon DaemonSet. Lastly, this directory also contains the default config used to configure the bpfman-daemon, along with the cert-manager certificates used to encrypt communication between the bpfman-agent and bpfman. /bpfman-operator-deployment: Contains the static deployment yaml for the bpfman-operator. This is deployed statically by kustomize. /crd : Contains the CRD manifests for all of the bpfman-operator APIs. /bases : Is where the actual CRD definitions are stored. These definitions are auto-generated by controller-gen . /default : Contains the default deployment configuration for the bpfman-operator. /manifests : Contains the bases for generating OLM manifests. /openshift : Contains the Openshift specific deployment configuration for the bpfman-operator. /prometheus : Contains the prometheus manifests used to deploy Prometheus to a cluster. At the time of writing this the bpfman-operator is NOT exposing any metrics to prometheus, but this is a future goal. /rbac : Contains rbac yamls for getting bpfman and the bpfman-operator up and running on Kubernetes. /bpfman-agent : Contains the rbac yamls for the bpfman-agent. They are automatically generated by kubebuilder via build tags in the bpfman-agent controller code. /bpfman-operator : Contains the rbac yamls for the bpfman-operator. They are automatically generated by kubebuilder via build tags in the bpfman-operator controller code. /samples : Contains sample CR definitions that can be deployed by users for each of our supported APIs. /scorecard : Contains the scorecard manifests used to deploy scorecard to a cluster. At the time of writing this the bpfman-operator is NOT running any scorecard tests. /test : Contains the test manifests used to deploy the bpfman-operator to a kind cluster for integration testing. /controllers : Contains the controller implementations for all of the bpfman-operator APIs. Each controller is responsible for reconciling the state of the cluster with the desired state defined by the user. This is where the source of truth for the auto-generated RBAC can be found, keep an eye out for //+kubebuilder:rbac:groups=bpfman.io comment tags. /bpfmanagent : Contains the controller implementations which reconcile user created *Program types to multiple BpfProgram objects. /bpfmanoperator : Contains the controller implementations which reconcile global BpfProgram object state back to the user by ensuring the user created *Program objects are reporting the correct status. /hack : Contains any scripts+static files used by the bpfman-operator to facilitate development. /internal : Contains all private library code and is used by the bpfman-operator and bpfman-agent controllers. /pkg : Contains all public library code this is consumed externally and internally. /client : Contains the autogenerated clientset, informers and listers for all of the bpfman-operator APIs. These are autogenerated by the k8s.io/code-generator project , and can be consumed by users wishing to programmatically interact with bpfman specific APIs. /helpers : Contains helper functions which can be consumed by users wishing to programmatically interact with bpfman specific APIs. /test/integration : Contains integration tests for the bpfman-operator. These tests are run against a kind cluster and are responsible for testing the bpfman-operator in a real cluster environment. It uses the kubernetes-testing-framework project to programmatically spin-up all of the required infrastructure for our unit tests. Makefile : Contains all of the make targets used to build, test, and generate code used by the bpfman-operator.","title":"Project Layout"},{"location":"developer-guide/documentation/","text":"Documentation This section describes how to modify the related documentation around bpfman. All bpfman's documentation is written in Markdown, and leverages mkdocs to generate a static site, which is hosted on netlify . If this is the first time building using mkdocs , jump to the Development Environment Setup section for help installing the tooling. Documentation Notes This section describes some notes on the dos and don'ts when writing documentation. Website Management The headings and layout of the website, as well as other configuration settings, are managed from the mkdocs.yml file in the project root directory. Markdown Style When writing documentation via a Markdown file, the following format has been followed: Text on a given line should not exceed 100 characters, unless it's example syntax or a link that should be broken up. Each new sentence should start on a new line. That way, if text needs to be inserted, whole paragraphs don't need to be adjusted. Links to other markdown files are relative to the file the link is placed in. Governance Files There are a set of well known governance files that are typically placed in the root directory of most projects, like README.md, MAINTAINERS.md, CONTRIBUTING.md, etc. mkdocs expects all files used in the static website to be located under a common directory, docs/ for bpfman. To reference the governance files from the static website, a directory ( docs/governance/ ) was created with a file for each governance file, the only contains --8<-- and the file name. This indicates to mkdocs to pull the additional file from the project root directory. For example: docs/governance/MEETINGS.md NOTE: This works for the website generation, but if a Markdown file is viewed through Github (not the website), the link is broken. So these files should only be linked from docs/index.md and mkdocs.yml . docs/developer-guide/api-spec.md The file docs/developer-guide/api-spec.md documents the CRDs used in a Kubernetes deployment. The contents are auto-generated when PRs are pushed to Github. The script scripts/make-docs.sh manages the generation of this file. Generate Documentation On each PR pushed to https://github.com/bpfman/bpfman the documentation is generated. To preview of the generated site, click on the Details link of the netlify/bpfman/deploy-preview Check from the Github GUI. If you would like to test locally, build and preview the generated documentation, from the bpfman root directory, use mkdocs to build: cd bpfman/ mkdocs build To preview from a build on a local machine, start the mkdocs dev-server with the command below, then open up http://127.0.0.1:8000/ in your browser, and you'll see the default home page being displayed: mkdocs serve To preview from a build on a remote machine, start the mkdocs dev-server with the command below, then open up http://<ServerIP>:8000/ in your browser, and you'll see the default home page being displayed: mkdocs serve -a 0.0.0.0:8000 Development Environment Setup The recommended installation method is using pip . pip install mkdocs pip install pymdown-extensions pip install mkdocs-material Once installed, ensure the mkdocs is in your PATH: mkdocs -V mkdocs, version 1.4.3 from /home/$USER/.local/lib/python3.11/site-packages/mkdocs (Python 3.11)","title":"Documentation"},{"location":"developer-guide/documentation/#documentation","text":"This section describes how to modify the related documentation around bpfman. All bpfman's documentation is written in Markdown, and leverages mkdocs to generate a static site, which is hosted on netlify . If this is the first time building using mkdocs , jump to the Development Environment Setup section for help installing the tooling.","title":"Documentation"},{"location":"developer-guide/documentation/#documentation-notes","text":"This section describes some notes on the dos and don'ts when writing documentation.","title":"Documentation Notes"},{"location":"developer-guide/documentation/#website-management","text":"The headings and layout of the website, as well as other configuration settings, are managed from the mkdocs.yml file in the project root directory.","title":"Website Management"},{"location":"developer-guide/documentation/#markdown-style","text":"When writing documentation via a Markdown file, the following format has been followed: Text on a given line should not exceed 100 characters, unless it's example syntax or a link that should be broken up. Each new sentence should start on a new line. That way, if text needs to be inserted, whole paragraphs don't need to be adjusted. Links to other markdown files are relative to the file the link is placed in.","title":"Markdown Style"},{"location":"developer-guide/documentation/#governance-files","text":"There are a set of well known governance files that are typically placed in the root directory of most projects, like README.md, MAINTAINERS.md, CONTRIBUTING.md, etc. mkdocs expects all files used in the static website to be located under a common directory, docs/ for bpfman. To reference the governance files from the static website, a directory ( docs/governance/ ) was created with a file for each governance file, the only contains --8<-- and the file name. This indicates to mkdocs to pull the additional file from the project root directory. For example: docs/governance/MEETINGS.md NOTE: This works for the website generation, but if a Markdown file is viewed through Github (not the website), the link is broken. So these files should only be linked from docs/index.md and mkdocs.yml .","title":"Governance Files"},{"location":"developer-guide/documentation/#docsdeveloper-guideapi-specmd","text":"The file docs/developer-guide/api-spec.md documents the CRDs used in a Kubernetes deployment. The contents are auto-generated when PRs are pushed to Github. The script scripts/make-docs.sh manages the generation of this file.","title":"docs/developer-guide/api-spec.md"},{"location":"developer-guide/documentation/#generate-documentation","text":"On each PR pushed to https://github.com/bpfman/bpfman the documentation is generated. To preview of the generated site, click on the Details link of the netlify/bpfman/deploy-preview Check from the Github GUI. If you would like to test locally, build and preview the generated documentation, from the bpfman root directory, use mkdocs to build: cd bpfman/ mkdocs build To preview from a build on a local machine, start the mkdocs dev-server with the command below, then open up http://127.0.0.1:8000/ in your browser, and you'll see the default home page being displayed: mkdocs serve To preview from a build on a remote machine, start the mkdocs dev-server with the command below, then open up http://<ServerIP>:8000/ in your browser, and you'll see the default home page being displayed: mkdocs serve -a 0.0.0.0:8000","title":"Generate Documentation"},{"location":"developer-guide/documentation/#development-environment-setup","text":"The recommended installation method is using pip . pip install mkdocs pip install pymdown-extensions pip install mkdocs-material Once installed, ensure the mkdocs is in your PATH: mkdocs -V mkdocs, version 1.4.3 from /home/$USER/.local/lib/python3.11/site-packages/mkdocs (Python 3.11)","title":"Development Environment Setup"},{"location":"developer-guide/image-build/","text":"bpfman Container Images Container images for the bpfman binaries are automatically built and pushed to quay.io/bpfman whenever code is merged into the main branch of the github.com/bpfman/bpfman repository under the :latest tag. Building the images locally bpfman docker build -f /Containerfile.bpfman . -t bpfman:local Running locally in container bpfman sudo docker run --init --privileged --net = host -v /etc/bpfman/certs/:/etc/bpfman/certs/ -v /sys/fs/bpf:/sys/fs/bpf quay.io/bpfman/bpfman:latest","title":"bpfman Container Images"},{"location":"developer-guide/image-build/#bpfman-container-images","text":"Container images for the bpfman binaries are automatically built and pushed to quay.io/bpfman whenever code is merged into the main branch of the github.com/bpfman/bpfman repository under the :latest tag.","title":"bpfman Container Images"},{"location":"developer-guide/image-build/#building-the-images-locally","text":"","title":"Building the images locally"},{"location":"developer-guide/image-build/#bpfman","text":"docker build -f /Containerfile.bpfman . -t bpfman:local","title":"bpfman"},{"location":"developer-guide/image-build/#running-locally-in-container","text":"","title":"Running locally in container"},{"location":"developer-guide/image-build/#bpfman_1","text":"sudo docker run --init --privileged --net = host -v /etc/bpfman/certs/:/etc/bpfman/certs/ -v /sys/fs/bpf:/sys/fs/bpf quay.io/bpfman/bpfman:latest","title":"bpfman"},{"location":"developer-guide/linux-capabilities/","text":"Linux Capabilities Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled. Capabilities are a per-thread attribute. See capabilities man-page . When bpfman is run as a systemd service, the set of linux capabilities are restricted to only the required set of capabilities via the bpfman.service file using the AmbientCapabilities and CapabilityBoundingSet fields (see bpfman.service ). All spawned threads are stripped of all capabilities, removing all sudo privileges (see drop_linux_capabilities() usage), leaving only the main thread with only the needed set of capabilities. Current bpfman Linux Capabilities Below are the current set of Linux capabilities required by bpfman to operate: CAP_BPF: Required to load BPF programs and create BPF maps. CAP_DAC_READ_SEARCH: Required by Tracepoint programs, needed by aya to check the tracefs mount point. For example, trying to read \"/sys/kernel/tracing\" and \"/sys/kernel/debug/tracing\". CAP_NET_ADMIN: Required for TC programs to attach/detach to/from a qdisc. CAP_SETPCAP: Required to allow bpfman to drop Linux Capabilities on spawned threads. CAP_SYS_ADMIN: Kprobe (Kprobe and Uprobe) and Tracepoint programs are considered perfmon programs and require CAP_PERFMON and CAP_SYS_ADMIN to load. TC and XDP programs are considered admin programs and require CAP_NET_ADMIN and CAP_SYS_ADMIN to load. CAP_SYS_RESOURCE: Required by bpfman to call setrlimit() on RLIMIT_MEMLOCK . Debugging Linux Capabilities As new features are added, the set of Linux capabilities required by bpfman may change over time. The following describes the steps to determine the set of capabilities required by bpfman. If there are any Permission denied (os error 13) type errors when starting or running bpfman as a systemd service, adjusting the linux capabilities is a good place to start. Determine Required Capabilities The first step is to turn all capabilities on and see if that fixes the problem. This can be done without recompiling the code by editing bpfman.service . Comment out the finite list of granted capabilities and set to ~ , which indicates all capabilities. sudo vi /usr/lib/systemd/system/bpfman.service : [ Service ] : AmbientCapabilities = ~ CapabilityBoundingSet = ~ #AmbientCapabilities=CAP_BPF CAP_DAC_OVERRIDE CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SETPCAP CAP_SYS_ADMIN CAP_SYS_RESOURCE #CapabilityBoundingSet=CAP_BPF CAP_DAC_OVERRIDE CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SETPCAP CAP_SYS_ADMIN CAP_SYS_RESOURCE Reload the service file and start/restart bpfman and watch the bpfman logs and see if the problem is resolved: sudo systemctl daemon-reload sudo systemctl start bpfman If so, then the next step is to watch the set of capabilities being requested by bpfman. Run the bcc capable tool to watch capabilities being requested real-time and restart bpfman: $ sudo /usr/share/bcc/tools/capable TIME UID PID COMM CAP NAME AUDIT : 16 :36:00 979 75553 tokio-runtime-w 8 CAP_SETPCAP 1 16 :36:00 979 75553 tokio-runtime-w 8 CAP_SETPCAP 1 16 :36:00 979 75553 tokio-runtime-w 8 CAP_SETPCAP 1 16 :36:00 0 616 systemd-journal 19 CAP_SYS_PTRACE 1 16 :36:00 0 616 systemd-journal 19 CAP_SYS_PTRACE 1 16 :36:00 979 75550 bpfman 24 CAP_SYS_RESOURCE 1 16 :36:00 979 75550 bpfman 1 CAP_DAC_OVERRIDE 1 16 :36:00 979 75550 bpfman 21 CAP_SYS_ADMIN 1 16 :36:00 979 75550 bpfman 21 CAP_SYS_ADMIN 1 16 :36:00 0 75555 modprobe 16 CAP_SYS_MODULE 1 16 :36:00 0 628 systemd-udevd 2 CAP_DAC_READ_SEARCH 1 16 :36:00 0 75556 bpf_preload 24 CAP_SYS_RESOURCE 1 16 :36:00 0 75556 bpf_preload 39 CAP_BPF 1 16 :36:00 0 75556 bpf_preload 39 CAP_BPF 1 16 :36:00 0 75556 bpf_preload 39 CAP_BPF 1 16 :36:00 0 75556 bpf_preload 38 CAP_PERFMON 1 16 :36:00 0 75556 bpf_preload 38 CAP_PERFMON 1 16 :36:00 0 75556 bpf_preload 38 CAP_PERFMON 1 : Compare the output to list in bpfman.service and determine the delta. Determine Capabilities Per Thread For additional debugging, it may be helpful to know the granted capabilities on a per thread basis. As mentioned above, all spawned threads are stripped of all Linux capabilities, so if a thread is requesting a capability, that functionality should be moved off the spawned thread and onto the main thread. First, determine the bpfman process id, then determine the set of threads: $ ps -ef | grep bpfman : bpfman 75550 1 0 16 :36 ? 00 :00:00 /usr/sbin/bpfman : $ ps -T -p 75550 PID SPID TTY TIME CMD 75550 75550 ? 00 :00:00 bpfman 75550 75551 ? 00 :00:00 tokio-runtime-w 75550 75552 ? 00 :00:00 tokio-runtime-w 75550 75553 ? 00 :00:00 tokio-runtime-w 75550 75554 ? 00 :00:00 tokio-runtime-w Then dump the capabilities of each thread: $ grep Cap /proc/75550/status CapInh: 000000c001201106 CapPrm: 000000c001201106 CapEff: 000000c001201106 CapBnd: 000000c001201106 CapAmb: 000000c001201106 $ grep Cap /proc/75551/status CapInh: 0000000000000000 CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 0000000000000000 CapAmb: 0000000000000000 $ grep Cap /proc/75552/status CapInh: 0000000000000000 CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 0000000000000000 CapAmb: 0000000000000000 : $ capsh --decode=000000c001201106 0x000000c001201106=cap_dac_override,cap_dac_read_search,cap_setpcap,cap_net_admin,cap_sys_admin,cap_sys_resource,cap_perfmon,cap_bpf Removing CAP_BPF from bpfman Clients One of the advantages of using bpfman is that it is doing all the loading and unloading of eBPF programs, so it requires CAP_BPF, but clients of bpfman are just making gRPC calls to bpfman, so they do not need to be privileged or require CAP_BPF. It must be noted that this is only true for kernels 5.19 or higher. Prior to kernel 5.19 , all eBPF sys calls required CAP_BPF, which are used to access maps shared between the BFP program and the userspace program. In kernel 5.19, a change went in that only requires CAP_BPF for map creation (BPF_MAP_CREATE) and loading programs (BPF_PROG_LOAD). See bpf: refine kernel.unprivileged_bpf_disabled behaviour .","title":"Linux Capabilities"},{"location":"developer-guide/linux-capabilities/#linux-capabilities","text":"Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled. Capabilities are a per-thread attribute. See capabilities man-page . When bpfman is run as a systemd service, the set of linux capabilities are restricted to only the required set of capabilities via the bpfman.service file using the AmbientCapabilities and CapabilityBoundingSet fields (see bpfman.service ). All spawned threads are stripped of all capabilities, removing all sudo privileges (see drop_linux_capabilities() usage), leaving only the main thread with only the needed set of capabilities.","title":"Linux Capabilities"},{"location":"developer-guide/linux-capabilities/#current-bpfman-linux-capabilities","text":"Below are the current set of Linux capabilities required by bpfman to operate: CAP_BPF: Required to load BPF programs and create BPF maps. CAP_DAC_READ_SEARCH: Required by Tracepoint programs, needed by aya to check the tracefs mount point. For example, trying to read \"/sys/kernel/tracing\" and \"/sys/kernel/debug/tracing\". CAP_NET_ADMIN: Required for TC programs to attach/detach to/from a qdisc. CAP_SETPCAP: Required to allow bpfman to drop Linux Capabilities on spawned threads. CAP_SYS_ADMIN: Kprobe (Kprobe and Uprobe) and Tracepoint programs are considered perfmon programs and require CAP_PERFMON and CAP_SYS_ADMIN to load. TC and XDP programs are considered admin programs and require CAP_NET_ADMIN and CAP_SYS_ADMIN to load. CAP_SYS_RESOURCE: Required by bpfman to call setrlimit() on RLIMIT_MEMLOCK .","title":"Current bpfman Linux Capabilities"},{"location":"developer-guide/linux-capabilities/#debugging-linux-capabilities","text":"As new features are added, the set of Linux capabilities required by bpfman may change over time. The following describes the steps to determine the set of capabilities required by bpfman. If there are any Permission denied (os error 13) type errors when starting or running bpfman as a systemd service, adjusting the linux capabilities is a good place to start.","title":"Debugging Linux Capabilities"},{"location":"developer-guide/linux-capabilities/#determine-required-capabilities","text":"The first step is to turn all capabilities on and see if that fixes the problem. This can be done without recompiling the code by editing bpfman.service . Comment out the finite list of granted capabilities and set to ~ , which indicates all capabilities. sudo vi /usr/lib/systemd/system/bpfman.service : [ Service ] : AmbientCapabilities = ~ CapabilityBoundingSet = ~ #AmbientCapabilities=CAP_BPF CAP_DAC_OVERRIDE CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SETPCAP CAP_SYS_ADMIN CAP_SYS_RESOURCE #CapabilityBoundingSet=CAP_BPF CAP_DAC_OVERRIDE CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SETPCAP CAP_SYS_ADMIN CAP_SYS_RESOURCE Reload the service file and start/restart bpfman and watch the bpfman logs and see if the problem is resolved: sudo systemctl daemon-reload sudo systemctl start bpfman If so, then the next step is to watch the set of capabilities being requested by bpfman. Run the bcc capable tool to watch capabilities being requested real-time and restart bpfman: $ sudo /usr/share/bcc/tools/capable TIME UID PID COMM CAP NAME AUDIT : 16 :36:00 979 75553 tokio-runtime-w 8 CAP_SETPCAP 1 16 :36:00 979 75553 tokio-runtime-w 8 CAP_SETPCAP 1 16 :36:00 979 75553 tokio-runtime-w 8 CAP_SETPCAP 1 16 :36:00 0 616 systemd-journal 19 CAP_SYS_PTRACE 1 16 :36:00 0 616 systemd-journal 19 CAP_SYS_PTRACE 1 16 :36:00 979 75550 bpfman 24 CAP_SYS_RESOURCE 1 16 :36:00 979 75550 bpfman 1 CAP_DAC_OVERRIDE 1 16 :36:00 979 75550 bpfman 21 CAP_SYS_ADMIN 1 16 :36:00 979 75550 bpfman 21 CAP_SYS_ADMIN 1 16 :36:00 0 75555 modprobe 16 CAP_SYS_MODULE 1 16 :36:00 0 628 systemd-udevd 2 CAP_DAC_READ_SEARCH 1 16 :36:00 0 75556 bpf_preload 24 CAP_SYS_RESOURCE 1 16 :36:00 0 75556 bpf_preload 39 CAP_BPF 1 16 :36:00 0 75556 bpf_preload 39 CAP_BPF 1 16 :36:00 0 75556 bpf_preload 39 CAP_BPF 1 16 :36:00 0 75556 bpf_preload 38 CAP_PERFMON 1 16 :36:00 0 75556 bpf_preload 38 CAP_PERFMON 1 16 :36:00 0 75556 bpf_preload 38 CAP_PERFMON 1 : Compare the output to list in bpfman.service and determine the delta.","title":"Determine Required Capabilities"},{"location":"developer-guide/linux-capabilities/#determine-capabilities-per-thread","text":"For additional debugging, it may be helpful to know the granted capabilities on a per thread basis. As mentioned above, all spawned threads are stripped of all Linux capabilities, so if a thread is requesting a capability, that functionality should be moved off the spawned thread and onto the main thread. First, determine the bpfman process id, then determine the set of threads: $ ps -ef | grep bpfman : bpfman 75550 1 0 16 :36 ? 00 :00:00 /usr/sbin/bpfman : $ ps -T -p 75550 PID SPID TTY TIME CMD 75550 75550 ? 00 :00:00 bpfman 75550 75551 ? 00 :00:00 tokio-runtime-w 75550 75552 ? 00 :00:00 tokio-runtime-w 75550 75553 ? 00 :00:00 tokio-runtime-w 75550 75554 ? 00 :00:00 tokio-runtime-w Then dump the capabilities of each thread: $ grep Cap /proc/75550/status CapInh: 000000c001201106 CapPrm: 000000c001201106 CapEff: 000000c001201106 CapBnd: 000000c001201106 CapAmb: 000000c001201106 $ grep Cap /proc/75551/status CapInh: 0000000000000000 CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 0000000000000000 CapAmb: 0000000000000000 $ grep Cap /proc/75552/status CapInh: 0000000000000000 CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 0000000000000000 CapAmb: 0000000000000000 : $ capsh --decode=000000c001201106 0x000000c001201106=cap_dac_override,cap_dac_read_search,cap_setpcap,cap_net_admin,cap_sys_admin,cap_sys_resource,cap_perfmon,cap_bpf","title":"Determine Capabilities Per Thread"},{"location":"developer-guide/linux-capabilities/#removing-cap_bpf-from-bpfman-clients","text":"One of the advantages of using bpfman is that it is doing all the loading and unloading of eBPF programs, so it requires CAP_BPF, but clients of bpfman are just making gRPC calls to bpfman, so they do not need to be privileged or require CAP_BPF. It must be noted that this is only true for kernels 5.19 or higher. Prior to kernel 5.19 , all eBPF sys calls required CAP_BPF, which are used to access maps shared between the BFP program and the userspace program. In kernel 5.19, a change went in that only requires CAP_BPF for map creation (BPF_MAP_CREATE) and loading programs (BPF_PROG_LOAD). See bpf: refine kernel.unprivileged_bpf_disabled behaviour .","title":"Removing CAP_BPF from bpfman Clients"},{"location":"developer-guide/logging/","text":"Logging This section describes how to enable logging in different bpfman deployments. Local Privileged Bpfman Process bpfman uses the env_logger crate to log messages to the terminal. By default, only error messages are logged, but that can be overwritten by setting the RUST_LOG environment variable. Valid values: error warn info debug trace Example: $ sudo RUST_LOG = info /usr/local/bin/bpfman [2022-08-08T20:29:31Z INFO bpfman] Log using env_logger [2022-08-08T20:29:31Z INFO bpfman::server] Loading static programs from /etc/bpfman/programs.d [2022-08-08T20:29:31Z INFO bpfman::server::bpf] Map veth12fa8e3 to 13 [2022-08-08T20:29:31Z INFO bpfman::server] Listening on [::1]:50051 [2022-08-08T20:29:31Z INFO bpfman::server::bpf] Program added: 1 programs attached to veth12fa8e3 [2022-08-08T20:29:31Z INFO bpfman::server] Loaded static program pass with UUID d9fd88df-d039-4e64-9f63-19f3e08915ce Systemd Service If bpfman is running as a systemd service, then bpfman will log to journald. As with env_logger, by default, info and higher messages are logged, but that can be overwritten by setting the RUST_LOG environment variable. Example: sudo vi /usr/lib/systemd/system/bpfman.service [Unit] Description=Run bpfman as a service DefaultDependencies=no After=network.target [Service] Environment=\"RUST_LOG=Info\" <==== Set Log Level Here ExecStart=/usr/sbin/bpfman system service AmbientCapabilities=CAP_BPF CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SYS_ADMIN CAP_SYS_RESOURCE CapabilityBoundingSet=CAP_BPF CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SYS_ADMIN CAP_SYS_RESOURCE Start the service: sudo systemctl start bpfman.service Check the logs: $ sudo journalctl -f -u bpfman Aug 08 16:25:04 ebpf03 systemd[1]: Started bpfman.service - Run bpfman as a service. Aug 08 16:25:04 ebpf03 bpfman[180118]: Log using journald Aug 08 16:25:04 ebpf03 bpfman[180118]: Loading static programs from /etc/bpfman/programs.d Aug 08 16:25:04 ebpf03 bpfman[180118]: Map veth12fa8e3 to 13 Aug 08 16:25:04 ebpf03 bpfman[180118]: Listening on [::1]:50051 Aug 08 16:25:04 ebpf03 bpfman[180118]: Program added: 1 programs attached to veth12fa8e3 Aug 08 16:25:04 ebpf03 bpfman[180118]: Loaded static program pass with UUID a3ffa14a-786d-48ad-b0cd-a4802f0f10b6 Stop the service: sudo systemctl stop bpfman.service Kubernetes Deployment When bpfman is run in a Kubernetes deployment, there is the bpfman Daemonset that runs on every node and the bpd Operator that runs on the control plane: kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE bpfman bpfman-daemon-dgqzw 2/2 Running 0 3d22h bpfman bpfman-daemon-gqsgd 2/2 Running 0 3d22h bpfman bpfman-daemon-zx9xr 2/2 Running 0 3d22h bpfman bpfman-operator-7fbf4888c4-z8w76 2/2 Running 0 3d22h : bpfman Daemonset bpfman and bpfman-agent are running in the bpfman daemonset. View Logs To view the bpfman logs: kubectl logs -n bpfman bpfman-daemon-dgqzw -c bpfman [2023-05-05T14:41:26Z INFO bpfman] Log using env_logger [2023-05-05T14:41:26Z INFO bpfman] Has CAP_BPF: false [2023-05-05T14:41:26Z INFO bpfman] Has CAP_SYS_ADMIN: true : To view the bpfman-agent logs: kubectl logs -n bpfman bpfman-daemon-dgqzw -c bpfman-agent {\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"controller-runtime.metrics\",\"msg\":\"Metrics server is starting to listen\",\"addr\":\":8080\"} {\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"setup\",\"msg\":\"Waiting for active connection to bpfman\"} {\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"setup\",\"msg\":\"starting Bpfman-Agent\"} : Change Log Level To change the log level of the agent or daemon, edit the bpfman-config ConfigMap. The bpfman-operator will detect the change and restart the bpfman daemonset with the updated values. kubectl edit configmaps -n bpfman bpfman-config apiVersion: v1 data: bpfman.agent.image: quay.io/bpfman/bpfman-agent:latest bpfman.image: quay.io/bpfman/bpfman:latest bpfman.log.level: info <==== Set bpfman Log Level Here bpfman.agent.log.level: info <==== Set bpfman agent Log Level Here kind: ConfigMap metadata: creationTimestamp: \"2023-05-05T14:41:19Z\" name: bpfman-config namespace: bpfman resourceVersion: \"700803\" uid: 0cc04af4-032c-4712-b824-748b321d319b Valid values for the daemon ( bpfman.log.level ) are: error warn info debug trace trace can be very verbose. More information can be found regarding Rust's env_logger here . Valid values for the agent ( bpfman.agent.log.level ) are: info debug trace bpfman Operator The bpfman Operator is running as a Deployment with a ReplicaSet of one. It runs with the containers bpfman-operator and kube-rbac-proxy . View Logs To view the bpfman-operator logs: kubectl logs -n bpfman bpfman-operator-7fbf4888c4-z8w76 -c bpfman-operator {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"logger\":\"controller-runtime.metrics\",\"msg\":\"Metrics server is starting to listen\",\"addr\":\"127.0.0.1:8080\"} {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"logger\":\"setup\",\"msg\":\"starting manager\"} {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting server\",\"kind\":\"health probe\",\"addr\":\"[::]:8081\"} {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting server\",\"path\":\"/metrics\",\"kind\":\"metrics\",\"addr\":\"127.0.0.1:8080\"} I0509 18:37:11.262885 1 leaderelection.go:248] attempting to acquire leader lease bpfman/8730d955.bpfman.io... I0509 18:37:11.268918 1 leaderelection.go:258] successfully acquired lease bpfman/8730d955.bpfman.io {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting EventSource\",\"controller\":\"configmap\",\"controllerGroup\":\"\",\"controllerKind\":\"ConfigMap\",\"source\":\"kind source: *v1.ConfigMap\"} : To view the kube-rbac-proxy logs: kubectl logs -n bpfman bpfman-operator-7fbf4888c4-z8w76 -c kube-rbac-proxy I0509 18:37:11.063386 1 main.go:186] Valid token audiences: I0509 18:37:11.063485 1 main.go:316] Generating self signed cert as no cert is provided I0509 18:37:11.955256 1 main.go:366] Starting TCP socket on 0.0.0.0:8443 I0509 18:37:11.955849 1 main.go:373] Listening securely on 0.0.0.0:8443 Change Log Level To change the log level, edit the bpfman-operator Deployment. The change will get detected and the bpfman operator pod will get restarted with the updated log level. kubectl edit deployment -n bpfman bpfman-operator apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"manager\",\"app.kubernetes.io/create> creationTimestamp: \"2023-05-09T18:37:08Z\" generation: 1 : spec: : template: metadata: : spec: containers: - args: : - args: - --health-probe-bind-address=:8081 - --metrics-bind-address=127.0.0.1:8080 - --leader-elect command: - /bpfman-operator env: - name: GO_LOG value: info <==== Set Log Level Here image: quay.io/bpfman/bpfman-operator:latest imagePullPolicy: IfNotPresent : Valid values are: error info debug trace","title":"Logging"},{"location":"developer-guide/logging/#logging","text":"This section describes how to enable logging in different bpfman deployments.","title":"Logging"},{"location":"developer-guide/logging/#local-privileged-bpfman-process","text":"bpfman uses the env_logger crate to log messages to the terminal. By default, only error messages are logged, but that can be overwritten by setting the RUST_LOG environment variable. Valid values: error warn info debug trace Example: $ sudo RUST_LOG = info /usr/local/bin/bpfman [2022-08-08T20:29:31Z INFO bpfman] Log using env_logger [2022-08-08T20:29:31Z INFO bpfman::server] Loading static programs from /etc/bpfman/programs.d [2022-08-08T20:29:31Z INFO bpfman::server::bpf] Map veth12fa8e3 to 13 [2022-08-08T20:29:31Z INFO bpfman::server] Listening on [::1]:50051 [2022-08-08T20:29:31Z INFO bpfman::server::bpf] Program added: 1 programs attached to veth12fa8e3 [2022-08-08T20:29:31Z INFO bpfman::server] Loaded static program pass with UUID d9fd88df-d039-4e64-9f63-19f3e08915ce","title":"Local Privileged Bpfman Process"},{"location":"developer-guide/logging/#systemd-service","text":"If bpfman is running as a systemd service, then bpfman will log to journald. As with env_logger, by default, info and higher messages are logged, but that can be overwritten by setting the RUST_LOG environment variable. Example: sudo vi /usr/lib/systemd/system/bpfman.service [Unit] Description=Run bpfman as a service DefaultDependencies=no After=network.target [Service] Environment=\"RUST_LOG=Info\" <==== Set Log Level Here ExecStart=/usr/sbin/bpfman system service AmbientCapabilities=CAP_BPF CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SYS_ADMIN CAP_SYS_RESOURCE CapabilityBoundingSet=CAP_BPF CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_PERFMON CAP_SYS_ADMIN CAP_SYS_RESOURCE Start the service: sudo systemctl start bpfman.service Check the logs: $ sudo journalctl -f -u bpfman Aug 08 16:25:04 ebpf03 systemd[1]: Started bpfman.service - Run bpfman as a service. Aug 08 16:25:04 ebpf03 bpfman[180118]: Log using journald Aug 08 16:25:04 ebpf03 bpfman[180118]: Loading static programs from /etc/bpfman/programs.d Aug 08 16:25:04 ebpf03 bpfman[180118]: Map veth12fa8e3 to 13 Aug 08 16:25:04 ebpf03 bpfman[180118]: Listening on [::1]:50051 Aug 08 16:25:04 ebpf03 bpfman[180118]: Program added: 1 programs attached to veth12fa8e3 Aug 08 16:25:04 ebpf03 bpfman[180118]: Loaded static program pass with UUID a3ffa14a-786d-48ad-b0cd-a4802f0f10b6 Stop the service: sudo systemctl stop bpfman.service","title":"Systemd Service"},{"location":"developer-guide/logging/#kubernetes-deployment","text":"When bpfman is run in a Kubernetes deployment, there is the bpfman Daemonset that runs on every node and the bpd Operator that runs on the control plane: kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE bpfman bpfman-daemon-dgqzw 2/2 Running 0 3d22h bpfman bpfman-daemon-gqsgd 2/2 Running 0 3d22h bpfman bpfman-daemon-zx9xr 2/2 Running 0 3d22h bpfman bpfman-operator-7fbf4888c4-z8w76 2/2 Running 0 3d22h :","title":"Kubernetes Deployment"},{"location":"developer-guide/logging/#bpfman-daemonset","text":"bpfman and bpfman-agent are running in the bpfman daemonset.","title":"bpfman Daemonset"},{"location":"developer-guide/logging/#view-logs","text":"To view the bpfman logs: kubectl logs -n bpfman bpfman-daemon-dgqzw -c bpfman [2023-05-05T14:41:26Z INFO bpfman] Log using env_logger [2023-05-05T14:41:26Z INFO bpfman] Has CAP_BPF: false [2023-05-05T14:41:26Z INFO bpfman] Has CAP_SYS_ADMIN: true : To view the bpfman-agent logs: kubectl logs -n bpfman bpfman-daemon-dgqzw -c bpfman-agent {\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"controller-runtime.metrics\",\"msg\":\"Metrics server is starting to listen\",\"addr\":\":8080\"} {\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"setup\",\"msg\":\"Waiting for active connection to bpfman\"} {\"level\":\"info\",\"ts\":\"2023-12-20T20:15:34Z\",\"logger\":\"setup\",\"msg\":\"starting Bpfman-Agent\"} :","title":"View Logs"},{"location":"developer-guide/logging/#change-log-level","text":"To change the log level of the agent or daemon, edit the bpfman-config ConfigMap. The bpfman-operator will detect the change and restart the bpfman daemonset with the updated values. kubectl edit configmaps -n bpfman bpfman-config apiVersion: v1 data: bpfman.agent.image: quay.io/bpfman/bpfman-agent:latest bpfman.image: quay.io/bpfman/bpfman:latest bpfman.log.level: info <==== Set bpfman Log Level Here bpfman.agent.log.level: info <==== Set bpfman agent Log Level Here kind: ConfigMap metadata: creationTimestamp: \"2023-05-05T14:41:19Z\" name: bpfman-config namespace: bpfman resourceVersion: \"700803\" uid: 0cc04af4-032c-4712-b824-748b321d319b Valid values for the daemon ( bpfman.log.level ) are: error warn info debug trace trace can be very verbose. More information can be found regarding Rust's env_logger here . Valid values for the agent ( bpfman.agent.log.level ) are: info debug trace","title":"Change Log Level"},{"location":"developer-guide/logging/#bpfman-operator","text":"The bpfman Operator is running as a Deployment with a ReplicaSet of one. It runs with the containers bpfman-operator and kube-rbac-proxy .","title":"bpfman Operator"},{"location":"developer-guide/logging/#view-logs_1","text":"To view the bpfman-operator logs: kubectl logs -n bpfman bpfman-operator-7fbf4888c4-z8w76 -c bpfman-operator {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"logger\":\"controller-runtime.metrics\",\"msg\":\"Metrics server is starting to listen\",\"addr\":\"127.0.0.1:8080\"} {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"logger\":\"setup\",\"msg\":\"starting manager\"} {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting server\",\"kind\":\"health probe\",\"addr\":\"[::]:8081\"} {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting server\",\"path\":\"/metrics\",\"kind\":\"metrics\",\"addr\":\"127.0.0.1:8080\"} I0509 18:37:11.262885 1 leaderelection.go:248] attempting to acquire leader lease bpfman/8730d955.bpfman.io... I0509 18:37:11.268918 1 leaderelection.go:258] successfully acquired lease bpfman/8730d955.bpfman.io {\"level\":\"info\",\"ts\":\"2023-05-09T18:37:11Z\",\"msg\":\"Starting EventSource\",\"controller\":\"configmap\",\"controllerGroup\":\"\",\"controllerKind\":\"ConfigMap\",\"source\":\"kind source: *v1.ConfigMap\"} : To view the kube-rbac-proxy logs: kubectl logs -n bpfman bpfman-operator-7fbf4888c4-z8w76 -c kube-rbac-proxy I0509 18:37:11.063386 1 main.go:186] Valid token audiences: I0509 18:37:11.063485 1 main.go:316] Generating self signed cert as no cert is provided I0509 18:37:11.955256 1 main.go:366] Starting TCP socket on 0.0.0.0:8443 I0509 18:37:11.955849 1 main.go:373] Listening securely on 0.0.0.0:8443","title":"View Logs"},{"location":"developer-guide/logging/#change-log-level_1","text":"To change the log level, edit the bpfman-operator Deployment. The change will get detected and the bpfman operator pod will get restarted with the updated log level. kubectl edit deployment -n bpfman bpfman-operator apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \"1\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"manager\",\"app.kubernetes.io/create> creationTimestamp: \"2023-05-09T18:37:08Z\" generation: 1 : spec: : template: metadata: : spec: containers: - args: : - args: - --health-probe-bind-address=:8081 - --metrics-bind-address=127.0.0.1:8080 - --leader-elect command: - /bpfman-operator env: - name: GO_LOG value: info <==== Set Log Level Here image: quay.io/bpfman/bpfman-operator:latest imagePullPolicy: IfNotPresent : Valid values are: error info debug trace","title":"Change Log Level"},{"location":"developer-guide/operator-quick-start/","text":"Deploying the bpfman-operator The bpfman-operator repository exists in order to deploy and manage bpfman within a Kubernetes cluster. This operator was built utilizing some great tooling provided by the operator-sdk library . A great first step in understanding some of the functionality can be to just run make help . Deploy Locally via KIND After reviewing the possible make targets it's quick and easy to get bpfman deployed locally on your system via a KIND cluster with: cd bpfman/bpfman-operator make run-on-kind NOTE: By default, bpfman-operator deploys bpfman with CSI enabled. CSI requires Kubernetes v1.26 due to a PR ( kubernetes/kubernetes#112597 ) that addresses a gRPC Protocol Error that was seen in the CSI client code and it doesn't appear to have been backported. Deploy To Openshift Cluster First deploy the operator with one of the following two options: 1. Manually with Kustomize To install manually with Kustomize and raw manifests simply run the following commands. The Openshift cluster needs to be up and running and specified in ~/.kube/config file. cd bpfman/bpfman-operator make deploy-openshift Which can then be cleaned up at a later time with: make undeploy-openshift 2. Via the OLM bundle The other option for installing the bpfman-operator is to install it using OLM bundle . First setup the namespace and certificates for the operator with: cd bpfman/bpfman-operator oc apply -f ./hack/ocp-scc-hacks.yaml Then use operator-sdk to install the bundle like so: operator-sdk run bundle quay.io/bpfman/bpfman-operator-bundle:latest --namespace openshift-bpfman Which can then be cleaned up at a later time with: operator-sdk cleanup bpfman-operator followed by oc delete -f ./hack/ocp-scc-hacks.yaml Verify the Installation Independent of the method used to deploy, if the bpfman-operator came up successfully you will see the bpfman-daemon and bpfman-operator pods running without errors: kubectl get pods -n bpfman NAME READY STATUS RESTARTS AGE bpfman-daemon-bt5xm 3 /3 Running 0 130m bpfman-daemon-ts7dr 3 /3 Running 0 129m bpfman-daemon-w24pr 3 /3 Running 0 130m bpfman-operator-78cf9c44c6-rv7f2 2 /2 Running 0 132m Deploy an eBPF Program to the cluster To test the deployment simply deploy one of the sample xdpPrograms : cd bpfman/bpfman-operator/ kubectl apply -f config/samples/bpfman.io_v1alpha1_xdp_pass_xdpprogram.yaml If loading of the XDP Program to the selected nodes was successful it will be reported back to the user via the xdpProgram 's status field: kubectl get xdpprogram xdp-pass-all-nodes -o yaml apiVersion: bpfman.io/v1alpha1 kind: XdpProgram metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"bpfman.io/v1alpha1\" , \"kind\" : \"XdpProgram\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"app.kubernetes.io/name\" : \"xdpprogram\" } , \"name\" : \"xdp-pass-all-nodes\" } , \"spec\" : { \"bpffunctionname\" : \"pass\" , \"bytecode\" : { \"image\" : { \"url\" : \"quay.io/bpfman-bytecode/xdp_pass:latest\" }} , \"globaldata\" : { \"GLOBAL_u32\" : [ 13 ,12,11,10 ] , \"GLOBAL_u8\" : [ 1 ]} , \"interfaceselector\" : { \"primarynodeinterface\" :true } , \"nodeselector\" : {} , \"priority\" :0 }} creationTimestamp: \"2023-11-07T19:16:39Z\" finalizers: - bpfman.io.operator/finalizer generation: 2 labels: app.kubernetes.io/name: xdpprogram name: xdp-pass-all-nodes resourceVersion: \"157187\" uid: 21c71a61-4e73-44eb-9b49-07af2866d25b spec: bpffunctionname: pass bytecode: image: imagepullpolicy: IfNotPresent url: quay.io/bpfman-bytecode/xdp_pass:latest globaldata: GLOBAL_u8: AQ == GLOBAL_u32: DQwLCg == interfaceselector: primarynodeinterface: true mapownerselector: {} nodeselector: {} priority: 0 proceedon: - pass - dispatcher_return status: conditions: - lastTransitionTime: \"2023-11-07T19:16:42Z\" message: bpfProgramReconciliation Succeeded on all nodes reason: ReconcileSuccess status: \"True\" type: ReconcileSuccess To see information in listing form simply run: kubectl get xdpprogram -o wide NAME BPFFUNCTIONNAME NODESELECTOR PRIORITY INTERFACESELECTOR PROCEEDON xdp-pass-all-nodes pass {} 0 { \"primarynodeinterface\" :true } [ \"pass\" , \"dispatcher_return\" ] API Types Overview See api-spec.md for a more detailed description of all the bpfman Kubernetes API types. Multiple Program CRDs The multiple *Program CRDs are the bpfman Kubernetes API objects most relevant to users and can be used to understand clusterwide state for an eBPF program. It's designed to express how, and where eBPF programs are to be deployed within a Kubernetes cluster. Currently bpfman supports the use of xdpProgram , tcProgram and tracepointProgram objects. BpfProgram CRD The BpfProgram CRD is used internally by the bpfman-deployment to keep track of per node bpfman state such as map pin points, and to report node specific errors back to the user. Kubernetes users/controllers are only allowed to view these objects, NOT create or edit them. Applications wishing to use bpfman to deploy/manage their eBPF programs in Kubernetes will make use of this object to find references to the bpfMap pin points ( spec.maps ) in order to configure their eBPF programs.","title":"Deploying the bpfman-operator"},{"location":"developer-guide/operator-quick-start/#deploying-the-bpfman-operator","text":"The bpfman-operator repository exists in order to deploy and manage bpfman within a Kubernetes cluster. This operator was built utilizing some great tooling provided by the operator-sdk library . A great first step in understanding some of the functionality can be to just run make help .","title":"Deploying the bpfman-operator"},{"location":"developer-guide/operator-quick-start/#deploy-locally-via-kind","text":"After reviewing the possible make targets it's quick and easy to get bpfman deployed locally on your system via a KIND cluster with: cd bpfman/bpfman-operator make run-on-kind NOTE: By default, bpfman-operator deploys bpfman with CSI enabled. CSI requires Kubernetes v1.26 due to a PR ( kubernetes/kubernetes#112597 ) that addresses a gRPC Protocol Error that was seen in the CSI client code and it doesn't appear to have been backported.","title":"Deploy Locally via KIND"},{"location":"developer-guide/operator-quick-start/#deploy-to-openshift-cluster","text":"First deploy the operator with one of the following two options:","title":"Deploy To Openshift Cluster"},{"location":"developer-guide/operator-quick-start/#1-manually-with-kustomize","text":"To install manually with Kustomize and raw manifests simply run the following commands. The Openshift cluster needs to be up and running and specified in ~/.kube/config file. cd bpfman/bpfman-operator make deploy-openshift Which can then be cleaned up at a later time with: make undeploy-openshift","title":"1. Manually with Kustomize"},{"location":"developer-guide/operator-quick-start/#2-via-the-olm-bundle","text":"The other option for installing the bpfman-operator is to install it using OLM bundle . First setup the namespace and certificates for the operator with: cd bpfman/bpfman-operator oc apply -f ./hack/ocp-scc-hacks.yaml Then use operator-sdk to install the bundle like so: operator-sdk run bundle quay.io/bpfman/bpfman-operator-bundle:latest --namespace openshift-bpfman Which can then be cleaned up at a later time with: operator-sdk cleanup bpfman-operator followed by oc delete -f ./hack/ocp-scc-hacks.yaml","title":"2. Via the OLM bundle"},{"location":"developer-guide/operator-quick-start/#verify-the-installation","text":"Independent of the method used to deploy, if the bpfman-operator came up successfully you will see the bpfman-daemon and bpfman-operator pods running without errors: kubectl get pods -n bpfman NAME READY STATUS RESTARTS AGE bpfman-daemon-bt5xm 3 /3 Running 0 130m bpfman-daemon-ts7dr 3 /3 Running 0 129m bpfman-daemon-w24pr 3 /3 Running 0 130m bpfman-operator-78cf9c44c6-rv7f2 2 /2 Running 0 132m","title":"Verify the Installation"},{"location":"developer-guide/operator-quick-start/#deploy-an-ebpf-program-to-the-cluster","text":"To test the deployment simply deploy one of the sample xdpPrograms : cd bpfman/bpfman-operator/ kubectl apply -f config/samples/bpfman.io_v1alpha1_xdp_pass_xdpprogram.yaml If loading of the XDP Program to the selected nodes was successful it will be reported back to the user via the xdpProgram 's status field: kubectl get xdpprogram xdp-pass-all-nodes -o yaml apiVersion: bpfman.io/v1alpha1 kind: XdpProgram metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"bpfman.io/v1alpha1\" , \"kind\" : \"XdpProgram\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"app.kubernetes.io/name\" : \"xdpprogram\" } , \"name\" : \"xdp-pass-all-nodes\" } , \"spec\" : { \"bpffunctionname\" : \"pass\" , \"bytecode\" : { \"image\" : { \"url\" : \"quay.io/bpfman-bytecode/xdp_pass:latest\" }} , \"globaldata\" : { \"GLOBAL_u32\" : [ 13 ,12,11,10 ] , \"GLOBAL_u8\" : [ 1 ]} , \"interfaceselector\" : { \"primarynodeinterface\" :true } , \"nodeselector\" : {} , \"priority\" :0 }} creationTimestamp: \"2023-11-07T19:16:39Z\" finalizers: - bpfman.io.operator/finalizer generation: 2 labels: app.kubernetes.io/name: xdpprogram name: xdp-pass-all-nodes resourceVersion: \"157187\" uid: 21c71a61-4e73-44eb-9b49-07af2866d25b spec: bpffunctionname: pass bytecode: image: imagepullpolicy: IfNotPresent url: quay.io/bpfman-bytecode/xdp_pass:latest globaldata: GLOBAL_u8: AQ == GLOBAL_u32: DQwLCg == interfaceselector: primarynodeinterface: true mapownerselector: {} nodeselector: {} priority: 0 proceedon: - pass - dispatcher_return status: conditions: - lastTransitionTime: \"2023-11-07T19:16:42Z\" message: bpfProgramReconciliation Succeeded on all nodes reason: ReconcileSuccess status: \"True\" type: ReconcileSuccess To see information in listing form simply run: kubectl get xdpprogram -o wide NAME BPFFUNCTIONNAME NODESELECTOR PRIORITY INTERFACESELECTOR PROCEEDON xdp-pass-all-nodes pass {} 0 { \"primarynodeinterface\" :true } [ \"pass\" , \"dispatcher_return\" ]","title":"Deploy an eBPF Program to the cluster"},{"location":"developer-guide/operator-quick-start/#api-types-overview","text":"See api-spec.md for a more detailed description of all the bpfman Kubernetes API types.","title":"API Types Overview"},{"location":"developer-guide/operator-quick-start/#multiple-program-crds","text":"The multiple *Program CRDs are the bpfman Kubernetes API objects most relevant to users and can be used to understand clusterwide state for an eBPF program. It's designed to express how, and where eBPF programs are to be deployed within a Kubernetes cluster. Currently bpfman supports the use of xdpProgram , tcProgram and tracepointProgram objects.","title":"Multiple Program CRDs"},{"location":"developer-guide/operator-quick-start/#bpfprogram-crd","text":"The BpfProgram CRD is used internally by the bpfman-deployment to keep track of per node bpfman state such as map pin points, and to report node specific errors back to the user. Kubernetes users/controllers are only allowed to view these objects, NOT create or edit them. Applications wishing to use bpfman to deploy/manage their eBPF programs in Kubernetes will make use of this object to find references to the bpfMap pin points ( spec.maps ) in order to configure their eBPF programs.","title":"BpfProgram CRD"},{"location":"developer-guide/release/","text":"Release Process Overview A release for the bpfman project is comprised of the following major components: bpfman binaries Core GRPC API protobuf definitions Kubernetes Custom Resource Definitions (CRDs) Corresponding go pkg in the form of github.com/bpfman/bpfman which includes the following: github.com/bpfman/bpfman/clients/gobpfman/v1 : The go client for the bpfman GRPC API github.com/bpfman/bpfman/bpfman-operator/apis : The go bindings for the bpfman CRD API github.com/bpfman/bpfman/bpfman-operator/pkg/client : The autogenerated clientset for the bpfman CRD API github.com/bpfman/bpfman/bpfman-operator/pkg/helpers : The provided bpfman CRD API helpers. Corresponding bpfman-api and bpfman rust crates which house the rust client for the bpfman GRPC API The following core component container images with tag : quay.io/bpfman/bpfman quay.io/bpfman/bpfman-operator quay.io/bpfman/bpfman-agent quay.io/bpfman/bpfman-operator-bundle quay.io/bpfman/xdp-dispatcher quay.io/bpfman/tc-dispatcher The relevant example bytecode container images with tag from source code located in the bpfman project: quay.io/bpfman-bytecode/go_xdp_counter quay.io/bpfman-bytecode/go_tc_counter quay.io/bpfman-bytecode/go_tracepoint_counter quay.io/bpfman-bytecode/xdp_pass quay.io/bpfman-bytecode/tc_pass quay.io/bpfman-bytecode/tracepoint quay.io/bpfman-bytecode/xdp_pass_private quay.io/bpfman-bytecode/uprobe quay.io/bpfman-bytecode/kprobe quay.io/bpfman-bytecode/uretprobe quay.io/bpfman-bytecode/kretprobe The relevant example userspace container images with tag from source code located in the bpfman project: quay.io/bpfman-userspace/go_xdp_counter quay.io/bpfman-userspace/go_tc_counter quay.io/bpfman-userspace/go_tracepoint_counter The OLM (Operator Lifecycle Manager) for the Kubernetes Operator. This includes a bundle directory on disk as well as the quay.io/bpfman/bpfman-operator-bundle with the tag . Versioning strategy Overview Each new release of bpfman is defined with a \"bundle version\" that represents the Git tag of a release, such as v0.4.0 . This contains the components described above Kubernetes API Versions (e.g. v1alpha2, v1beta1) Within the bpfman-operator, API versions are primarily used to indicate the stability of a resource. For example, if a resource has not yet graduated to beta, it is still possible that it could either be removed from the API or changed in backwards incompatible ways. For more information on API versions, refer to the full Kubernetes API versioning documentation . Releasing a new version Writing a Changelog To simplify release notes generation, we recommend using the Kubernetes release notes generator : go install k8s.io/release/cmd/release-notes@latest export GITHUB_TOKEN = your_token_here release-notes --start-sha EXAMPLE_COMMIT --end-sha EXAMPLE_COMMIT --branch main --repo bpfman --org bpfman This output will likely need to be reorganized and cleaned up a bit, but it provides a good starting point. Once you're satisfied with the changelog, create a PR. This must go through the regular PR review process and get merged into the main branch. Approval of the PR indicates community consensus for a new release. Release Steps The following steps must be done by one of the bpfman maintainers : For a PATCH release: Create a new branch in your fork named something like <githubuser>/release-x.x.x . Use the new branch in the upcoming steps. Use git to cherry-pick all relevant PRs into your branch. Create a branch from the major-minor tag of interest i.e: git checkout -b release-x.x.x <major.minor.patch> Create a pull request of the <githubuser>/release-x.x.x branch into the release-x.x branch upstream. Add a hold on this PR waiting for at least one maintainer/codeowner to provide a lgtm . This PR should: Add a new changelog for the release Update the cargo.toml versions for the bpfman-api and bpfman crates Update the bpfman-operator version in it's MAKEFILE and run make bundle to update the bundle version. This will generate a new /bpfman-operator/bundle directory which will ONLY be tracked in the release-x.x branch not main . Verify the CI tests pass and merge the PR into release-x.x . Create a tag using the HEAD of the release-x.x.x branch. This can be done using the git CLI or Github's release page. The Release will be automatically created, after that is complete do the following: run make build-release-yamls and attach the yamls for the version to the release. These will include: bpfman-crds-install-vx.x.x.yaml bpfman-operator-install-vx.x.x.yaml go-xdp-counter-install-vx.x.x.yaml go-tc-counter-install-vx.x.x.yaml go-tracepoint-counter-install-vx.x.x.yaml Update the community-operator and community-operators-prod repositories with the latest bundle manifests. See the following PRs as examples: https://github.com/redhat-openshift-ecosystem/community-operators-prod/pull/2696 https://github.com/k8s-operatorhub/community-operators/pull/2790 For a MAJOR or MINOR release: Open an update PR that: Adds a new changelog for the release Updates the cargo.toml versions for the bpfman-api and bpfman crates Updates the bpfman-operator version in it's MAKEFILE and run make bundle to update the bundle version Add's a new examples config directory for the release version Make sure CI is green and merge the update PR. Create a tag using the HEAD of the main branch. This can be done using the git CLI or Github's release page. Tag the release using the commit on main where the changelog update merged. This can be done using the git CLI or Github's release page. The Release will be automatically created, after that is complete do the following: run make build-release-yamls and attach the yamls for the version to the release. These will include: bpfman-crds-install-vx.x.x.yaml bpfman-operator-install-vx.x.x.yaml go-xdp-counter-install-vx.x.x.yaml go-tc-counter-install-vx.x.x.yaml go-tracepoint-counter-install-vx.x.x.yaml","title":"Releasing"},{"location":"developer-guide/release/#release-process","text":"","title":"Release Process"},{"location":"developer-guide/release/#overview","text":"A release for the bpfman project is comprised of the following major components: bpfman binaries Core GRPC API protobuf definitions Kubernetes Custom Resource Definitions (CRDs) Corresponding go pkg in the form of github.com/bpfman/bpfman which includes the following: github.com/bpfman/bpfman/clients/gobpfman/v1 : The go client for the bpfman GRPC API github.com/bpfman/bpfman/bpfman-operator/apis : The go bindings for the bpfman CRD API github.com/bpfman/bpfman/bpfman-operator/pkg/client : The autogenerated clientset for the bpfman CRD API github.com/bpfman/bpfman/bpfman-operator/pkg/helpers : The provided bpfman CRD API helpers. Corresponding bpfman-api and bpfman rust crates which house the rust client for the bpfman GRPC API The following core component container images with tag : quay.io/bpfman/bpfman quay.io/bpfman/bpfman-operator quay.io/bpfman/bpfman-agent quay.io/bpfman/bpfman-operator-bundle quay.io/bpfman/xdp-dispatcher quay.io/bpfman/tc-dispatcher The relevant example bytecode container images with tag from source code located in the bpfman project: quay.io/bpfman-bytecode/go_xdp_counter quay.io/bpfman-bytecode/go_tc_counter quay.io/bpfman-bytecode/go_tracepoint_counter quay.io/bpfman-bytecode/xdp_pass quay.io/bpfman-bytecode/tc_pass quay.io/bpfman-bytecode/tracepoint quay.io/bpfman-bytecode/xdp_pass_private quay.io/bpfman-bytecode/uprobe quay.io/bpfman-bytecode/kprobe quay.io/bpfman-bytecode/uretprobe quay.io/bpfman-bytecode/kretprobe The relevant example userspace container images with tag from source code located in the bpfman project: quay.io/bpfman-userspace/go_xdp_counter quay.io/bpfman-userspace/go_tc_counter quay.io/bpfman-userspace/go_tracepoint_counter The OLM (Operator Lifecycle Manager) for the Kubernetes Operator. This includes a bundle directory on disk as well as the quay.io/bpfman/bpfman-operator-bundle with the tag .","title":"Overview"},{"location":"developer-guide/release/#versioning-strategy","text":"","title":"Versioning strategy"},{"location":"developer-guide/release/#overview_1","text":"Each new release of bpfman is defined with a \"bundle version\" that represents the Git tag of a release, such as v0.4.0 . This contains the components described above","title":"Overview"},{"location":"developer-guide/release/#kubernetes-api-versions-eg-v1alpha2-v1beta1","text":"Within the bpfman-operator, API versions are primarily used to indicate the stability of a resource. For example, if a resource has not yet graduated to beta, it is still possible that it could either be removed from the API or changed in backwards incompatible ways. For more information on API versions, refer to the full Kubernetes API versioning documentation .","title":"Kubernetes API Versions (e.g. v1alpha2, v1beta1)"},{"location":"developer-guide/release/#releasing-a-new-version","text":"","title":"Releasing a new version"},{"location":"developer-guide/release/#writing-a-changelog","text":"To simplify release notes generation, we recommend using the Kubernetes release notes generator : go install k8s.io/release/cmd/release-notes@latest export GITHUB_TOKEN = your_token_here release-notes --start-sha EXAMPLE_COMMIT --end-sha EXAMPLE_COMMIT --branch main --repo bpfman --org bpfman This output will likely need to be reorganized and cleaned up a bit, but it provides a good starting point. Once you're satisfied with the changelog, create a PR. This must go through the regular PR review process and get merged into the main branch. Approval of the PR indicates community consensus for a new release.","title":"Writing a Changelog"},{"location":"developer-guide/release/#release-steps","text":"The following steps must be done by one of the bpfman maintainers : For a PATCH release: Create a new branch in your fork named something like <githubuser>/release-x.x.x . Use the new branch in the upcoming steps. Use git to cherry-pick all relevant PRs into your branch. Create a branch from the major-minor tag of interest i.e: git checkout -b release-x.x.x <major.minor.patch> Create a pull request of the <githubuser>/release-x.x.x branch into the release-x.x branch upstream. Add a hold on this PR waiting for at least one maintainer/codeowner to provide a lgtm . This PR should: Add a new changelog for the release Update the cargo.toml versions for the bpfman-api and bpfman crates Update the bpfman-operator version in it's MAKEFILE and run make bundle to update the bundle version. This will generate a new /bpfman-operator/bundle directory which will ONLY be tracked in the release-x.x branch not main . Verify the CI tests pass and merge the PR into release-x.x . Create a tag using the HEAD of the release-x.x.x branch. This can be done using the git CLI or Github's release page. The Release will be automatically created, after that is complete do the following: run make build-release-yamls and attach the yamls for the version to the release. These will include: bpfman-crds-install-vx.x.x.yaml bpfman-operator-install-vx.x.x.yaml go-xdp-counter-install-vx.x.x.yaml go-tc-counter-install-vx.x.x.yaml go-tracepoint-counter-install-vx.x.x.yaml Update the community-operator and community-operators-prod repositories with the latest bundle manifests. See the following PRs as examples: https://github.com/redhat-openshift-ecosystem/community-operators-prod/pull/2696 https://github.com/k8s-operatorhub/community-operators/pull/2790 For a MAJOR or MINOR release: Open an update PR that: Adds a new changelog for the release Updates the cargo.toml versions for the bpfman-api and bpfman crates Updates the bpfman-operator version in it's MAKEFILE and run make bundle to update the bundle version Add's a new examples config directory for the release version Make sure CI is green and merge the update PR. Create a tag using the HEAD of the main branch. This can be done using the git CLI or Github's release page. Tag the release using the commit on main where the changelog update merged. This can be done using the git CLI or Github's release page. The Release will be automatically created, after that is complete do the following: run make build-release-yamls and attach the yamls for the version to the release. These will include: bpfman-crds-install-vx.x.x.yaml bpfman-operator-install-vx.x.x.yaml go-xdp-counter-install-vx.x.x.yaml go-tc-counter-install-vx.x.x.yaml go-tracepoint-counter-install-vx.x.x.yaml","title":"Release Steps"},{"location":"developer-guide/shipping-bytecode/","text":"eBPF Bytecode Image Specifications Introduction The eBPF Bytecode Image specification defines how to package eBPF bytecode as container images. The initial primary use case focuses on the containerization and deployment of eBPF programs within container orchestration systems such as Kubernetes, where it is necessary to provide a portable way to distribute bytecode to all nodes which need it. Specifications We provide two distinct spec variants here to ensure interoperatiblity with existing registries and packages which do no support the new custom media types defined here. custom-data-type-spec backwards-compatable-spec Backwards compatible OCI compliant spec This variant makes use of existing OCI conventions to represent eBPF Bytecode as container images. Image Layers The container images following this variant must contain exactly one layer who's media type is one of the following: application/vnd.oci.image.layer.v1.tar+gzip or the compliant application/vnd.docker.image.rootfs.diff.tar.gzip Additionally the image layer must contain a valid eBPF object file (generally containing a .o extension) placed at the root of the layer ./ . Image Labels To provide relevant metadata regarding the bytecode to any consumers, some relevant labels MUST be defined on the image. These labels are defined as follows: io.ebpf.program_type : The eBPF program type (i.e xdp , tc , sockops , ...). io.ebpf.filename : The Filename of the bytecode stored in the image. io.ebpf.program_name : The name of the eBPF Program represented in the bytecode. io.ebpf.bpf_function_name : The name of the function that is the entry point for the BPF program. Building a Backwards compatible OCI compliant image An Example Containerfile can be found at /packaging/container/deployment/Containerfile.bytecode To use the provided templated Containerfile simply run a docker build command like the following: docker build \\ --build-arg PROGRAM_NAME = xdp_pass \\ --build-arg BPF_FUNCTION_NAME = pass \\ --build-arg PROGRAM_TYPE = xdp \\ --build-arg BYTECODE_FILENAME = pass.bpf.o \\ --build-arg KERNEL_COMPILE_VER = $( uname -r ) \\ -f Containerfile.bytecode \\ /home/<USER>/bytecode -t quay.io/<USER>/xdp_pass:latest Where /home/<USER>/bytecode is the directory the bytecode object file is located. Users can also use skopeo to ensure the image follows the backwards compatible version of the spec: skopeo inspect will show the correctly configured labels stored in the configuration layer ( application/vnd.oci.image.config.v1+json ) of the image. skopeo inspect docker://quay.io/astoycos/xdp_pass:latest { \"Name\" : \"quay.io/<USER>/xdp_pass\" , \"Digest\" : \"sha256:db1f7dd03f9fba0913e07493238fcfaf0bf08de37b8e992cc5902775dfb9086a\" , \"RepoTags\" : [ \"latest\" ] , \"Created\" : \"2022-08-14T14:27:20.147468277Z\" , \"DockerVersion\" : \"\" , \"Labels\" : { \"io.buildah.version\" : \"1.26.1\" , \"io.ebpf.filename\" : \"pass.bpf.o\" , \"io.ebpf.program_name\" : \"xdp_counter\" , \"io.ebpf.program_type\" : \"xdp\" , \"io.ebpf.bpf_function_name\" : \"pass\" } , \"Architecture\" : \"amd64\" , \"Os\" : \"linux\" , \"Layers\" : [ \"sha256:5f6dae6f567601fdad15a936d844baac1f30c31bd3df8df0c5b5429f3e048000\" ] , \"Env\" : [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ] } skopeo inspect --raw will show the correct layer type is used in the image. skopeo inspect --raw docker://quay.io/astoycos/xdp_pass:latest { \"schemaVersion\" :2, \"mediaType\" : \"application/vnd.oci.image.manifest.v1+json\" , \"config\" : { \"mediaType\" : \"application/vnd.oci.image.config.v1+json\" , \"digest\" : \"sha256:ff4108b8405a877b2df3e06f9287c509b9d62d6c241c9a5213d81a9abee80361\" , \"size\" :2385 } , \"layers\" : [{ \"mediaType\" : \"application/vnd.oci.image.layer.v1.tar+gzip\" , \"digest\" : \"sha256:5f6dae6f567601fdad15a936d844baac1f30c31bd3df8df0c5b5429f3e048000\" , \"size\" :1539 }] , \"annotations\" : { \"org.opencontainers.image.base.digest\" : \"sha256:86b59a6cf7046c624c47e40a5618b383d763be712df2c0e7aaf9391c2c9ef559\" , \"org.opencontainers.image.base.name\" : \"\" }} Custom OCI compatible spec This variant of the eBPF bytecode image spec uses custom OCI medium types to represent eBPF bytecode as container images. Many toolchains and registries may not support this yet. TODO(astoycos)","title":"eBPF Bytecode Image Specifications"},{"location":"developer-guide/shipping-bytecode/#ebpf-bytecode-image-specifications","text":"","title":"eBPF Bytecode Image Specifications"},{"location":"developer-guide/shipping-bytecode/#introduction","text":"The eBPF Bytecode Image specification defines how to package eBPF bytecode as container images. The initial primary use case focuses on the containerization and deployment of eBPF programs within container orchestration systems such as Kubernetes, where it is necessary to provide a portable way to distribute bytecode to all nodes which need it.","title":"Introduction"},{"location":"developer-guide/shipping-bytecode/#specifications","text":"We provide two distinct spec variants here to ensure interoperatiblity with existing registries and packages which do no support the new custom media types defined here. custom-data-type-spec backwards-compatable-spec","title":"Specifications"},{"location":"developer-guide/shipping-bytecode/#backwards-compatible-oci-compliant-spec","text":"This variant makes use of existing OCI conventions to represent eBPF Bytecode as container images.","title":"Backwards compatible OCI compliant spec"},{"location":"developer-guide/shipping-bytecode/#image-layers","text":"The container images following this variant must contain exactly one layer who's media type is one of the following: application/vnd.oci.image.layer.v1.tar+gzip or the compliant application/vnd.docker.image.rootfs.diff.tar.gzip Additionally the image layer must contain a valid eBPF object file (generally containing a .o extension) placed at the root of the layer ./ .","title":"Image Layers"},{"location":"developer-guide/shipping-bytecode/#image-labels","text":"To provide relevant metadata regarding the bytecode to any consumers, some relevant labels MUST be defined on the image. These labels are defined as follows: io.ebpf.program_type : The eBPF program type (i.e xdp , tc , sockops , ...). io.ebpf.filename : The Filename of the bytecode stored in the image. io.ebpf.program_name : The name of the eBPF Program represented in the bytecode. io.ebpf.bpf_function_name : The name of the function that is the entry point for the BPF program.","title":"Image Labels"},{"location":"developer-guide/shipping-bytecode/#building-a-backwards-compatible-oci-compliant-image","text":"An Example Containerfile can be found at /packaging/container/deployment/Containerfile.bytecode To use the provided templated Containerfile simply run a docker build command like the following: docker build \\ --build-arg PROGRAM_NAME = xdp_pass \\ --build-arg BPF_FUNCTION_NAME = pass \\ --build-arg PROGRAM_TYPE = xdp \\ --build-arg BYTECODE_FILENAME = pass.bpf.o \\ --build-arg KERNEL_COMPILE_VER = $( uname -r ) \\ -f Containerfile.bytecode \\ /home/<USER>/bytecode -t quay.io/<USER>/xdp_pass:latest Where /home/<USER>/bytecode is the directory the bytecode object file is located. Users can also use skopeo to ensure the image follows the backwards compatible version of the spec: skopeo inspect will show the correctly configured labels stored in the configuration layer ( application/vnd.oci.image.config.v1+json ) of the image. skopeo inspect docker://quay.io/astoycos/xdp_pass:latest { \"Name\" : \"quay.io/<USER>/xdp_pass\" , \"Digest\" : \"sha256:db1f7dd03f9fba0913e07493238fcfaf0bf08de37b8e992cc5902775dfb9086a\" , \"RepoTags\" : [ \"latest\" ] , \"Created\" : \"2022-08-14T14:27:20.147468277Z\" , \"DockerVersion\" : \"\" , \"Labels\" : { \"io.buildah.version\" : \"1.26.1\" , \"io.ebpf.filename\" : \"pass.bpf.o\" , \"io.ebpf.program_name\" : \"xdp_counter\" , \"io.ebpf.program_type\" : \"xdp\" , \"io.ebpf.bpf_function_name\" : \"pass\" } , \"Architecture\" : \"amd64\" , \"Os\" : \"linux\" , \"Layers\" : [ \"sha256:5f6dae6f567601fdad15a936d844baac1f30c31bd3df8df0c5b5429f3e048000\" ] , \"Env\" : [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ] } skopeo inspect --raw will show the correct layer type is used in the image. skopeo inspect --raw docker://quay.io/astoycos/xdp_pass:latest { \"schemaVersion\" :2, \"mediaType\" : \"application/vnd.oci.image.manifest.v1+json\" , \"config\" : { \"mediaType\" : \"application/vnd.oci.image.config.v1+json\" , \"digest\" : \"sha256:ff4108b8405a877b2df3e06f9287c509b9d62d6c241c9a5213d81a9abee80361\" , \"size\" :2385 } , \"layers\" : [{ \"mediaType\" : \"application/vnd.oci.image.layer.v1.tar+gzip\" , \"digest\" : \"sha256:5f6dae6f567601fdad15a936d844baac1f30c31bd3df8df0c5b5429f3e048000\" , \"size\" :1539 }] , \"annotations\" : { \"org.opencontainers.image.base.digest\" : \"sha256:86b59a6cf7046c624c47e40a5618b383d763be712df2c0e7aaf9391c2c9ef559\" , \"org.opencontainers.image.base.name\" : \"\" }}","title":"Building a Backwards compatible OCI compliant image"},{"location":"developer-guide/shipping-bytecode/#custom-oci-compatible-spec","text":"This variant of the eBPF bytecode image spec uses custom OCI medium types to represent eBPF bytecode as container images. Many toolchains and registries may not support this yet. TODO(astoycos)","title":"Custom OCI compatible spec"},{"location":"developer-guide/testing/","text":"Testing This document describes the automated testing that is done for each pull request submitted to bpfman . Unit Testing Unit testing is executed as part of the build job by running the cargo test command in the top-level bpfman directory. Go Example Tests Tests are run for each of the example programs found in directory examples Detailed description TBD Basic Integration Tests The full set of basic integration tests are executed by running the following command in the top-level bpfman directory. cargo xtask integration-test Optionally, a subset of the integration tests can be run by adding the \"--\" and a list of one or more names at the end of the command as shown below. cargo xtask integration-test -- test_load_unload_xdp test_proceed_on_xdp The integration tests start a bpfman daemon process, and issue CLI commands to verify a range of functionality. For XDP and TC programs that are installed on network interfaces, the integration test code creates a test network namespace connected to the host by a veth pair on which the programs are attached. The test code uses the IP subnet 172.37.37.1/24 for the namespace. If that address conflicts with an existing network on the host, it can be changed by setting the BPFMAN_IP_PREFIX environment variable to one that is available as shown below. export BPFMAN_IP_PREFIX = \"192.168.50\" If bpfman logs are needed to help debug an integration test, set RUST_LOG either globally or for a given test. export RUST_LOG = info OR RUST_LOG = info cargo xtask integration-test -- test_load_unload_xdp test_proceed_on_xdp There are two categories of integration tests: basic and e2e. The basic tests verify basic CLI functionality such as loading, listing, and unloading programs. The e2e tests verify more advanced functionality such as the setting of global variables, priority, and proceed-on by installing the programs, creating traffic if needed, and examining logs to confirm that things are running as expected. Most eBPF test programs are loaded from container images stored on quay.io . The source code for the eBPF test programs can be found in the tests/integration-test/bpf directory. These programs are compiled by executing cargo xtask build-ebpf --libbpf-dir <libbpf dir> We also load some tests from local files to test the load-from-file option. The bpf directory also contains a script called build_push_images.sh that can be used to build and push new images to quay if the code is changed. Images get pushed automatically when code gets merged, however, it's still useful to be able to push them manually sometimes. For example, when a new test case requires that both the eBPF and integration code be changed together. It is also a useful template for new eBPF test code that needs to be pushed. However, as a word of caution, be aware that existing integration tests will start using the new programs immediately, so this should only be done if the modified program is backward compatible. Kubernetes Integration Tests Detailed decription TBD","title":"Testing"},{"location":"developer-guide/testing/#testing","text":"This document describes the automated testing that is done for each pull request submitted to bpfman .","title":"Testing"},{"location":"developer-guide/testing/#unit-testing","text":"Unit testing is executed as part of the build job by running the cargo test command in the top-level bpfman directory.","title":"Unit Testing"},{"location":"developer-guide/testing/#go-example-tests","text":"Tests are run for each of the example programs found in directory examples Detailed description TBD","title":"Go Example Tests"},{"location":"developer-guide/testing/#basic-integration-tests","text":"The full set of basic integration tests are executed by running the following command in the top-level bpfman directory. cargo xtask integration-test Optionally, a subset of the integration tests can be run by adding the \"--\" and a list of one or more names at the end of the command as shown below. cargo xtask integration-test -- test_load_unload_xdp test_proceed_on_xdp The integration tests start a bpfman daemon process, and issue CLI commands to verify a range of functionality. For XDP and TC programs that are installed on network interfaces, the integration test code creates a test network namespace connected to the host by a veth pair on which the programs are attached. The test code uses the IP subnet 172.37.37.1/24 for the namespace. If that address conflicts with an existing network on the host, it can be changed by setting the BPFMAN_IP_PREFIX environment variable to one that is available as shown below. export BPFMAN_IP_PREFIX = \"192.168.50\" If bpfman logs are needed to help debug an integration test, set RUST_LOG either globally or for a given test. export RUST_LOG = info OR RUST_LOG = info cargo xtask integration-test -- test_load_unload_xdp test_proceed_on_xdp There are two categories of integration tests: basic and e2e. The basic tests verify basic CLI functionality such as loading, listing, and unloading programs. The e2e tests verify more advanced functionality such as the setting of global variables, priority, and proceed-on by installing the programs, creating traffic if needed, and examining logs to confirm that things are running as expected. Most eBPF test programs are loaded from container images stored on quay.io . The source code for the eBPF test programs can be found in the tests/integration-test/bpf directory. These programs are compiled by executing cargo xtask build-ebpf --libbpf-dir <libbpf dir> We also load some tests from local files to test the load-from-file option. The bpf directory also contains a script called build_push_images.sh that can be used to build and push new images to quay if the code is changed. Images get pushed automatically when code gets merged, however, it's still useful to be able to push them manually sometimes. For example, when a new test case requires that both the eBPF and integration code be changed together. It is also a useful template for new eBPF test code that needs to be pushed. However, as a word of caution, be aware that existing integration tests will start using the new programs immediately, so this should only be done if the modified program is backward compatible.","title":"Basic Integration Tests"},{"location":"developer-guide/testing/#kubernetes-integration-tests","text":"Detailed decription TBD","title":"Kubernetes Integration Tests"},{"location":"getting-started/building-bpfman/","text":"Setup and Building bpfman This section describes how to build bpfman. If this is the first time building bpfman, jump to the Development Environment Setup section for help installing the tooling. There is also an option to run images from a given release as opposed to building locally. Jump to the Run bpfman From Release Image section for installing from a fixed release. Kernel Versions eBPF is still a relatively new technology and being actively developed. To take advantage of this constantly evolving technology, it is best to use the newest kernel version possible. If bpfman needs to be run on an older kernel, this section describes some of the kernel features bpfman relies on to work and which kernel the feature was first introduced. Major kernel features leveraged by bpfman: Program Extensions: Program Extensions allows bpfman to load multiple XDP or TC eBPF programs on an interface, which is not natively supported in the kernel. A dispatcher program is loaded as the one program on a given interface, and the user's XDP or TC programs are loaded as extensions to the dispatcher program. Introduced in Kernel 5.6. Pinning: Pinning allows the eBPF program to remain loaded when the loading process (bpfman) is stopped or restarted. Introduced in Kernel 4.11. BPF Perf Link: Support BPF perf link for tracing programs (Tracepoint, Uprobe and Kprobe) which enables pinning for these program types. Introduced in Kernel 5.15. Tested kernel versions: Fedora 34: Kernel 5.17.6-100.fc34.x86_64 XDP, TC, Tracepoint, Uprobe and Kprobe programs all loaded with bpfman running on localhost and running as systemd service. Fedora 33: Kernel 5.14.18-100.fc33.x86_64 XDP and TC programs loaded with bpfman running on localhost and running as systemd service once SELinux was disabled (see https://github.com/fedora-selinux/selinux-policy/pull/806). Tracepoint, Uprobe and Kprobe programs failed to load because they require the BPF Perf Link support. Fedora 32: Kernel 5.11.22-100.fc32.x86_64 XDP and TC programs loaded with bpfman running on localhost once SELinux was disabled (see https://github.com/fedora-selinux/selinux-policy/pull/806). bpfman fails to run as a systemd service because of some capabilities issues in the bpfman.service file. Tracepoint, Uprobe and Kprobe programs failed to load because they require the BPF Perf Link support. Fedora 31: Kernel 5.8.18-100.fc31.x86_64 bpfman was able to start on localhost, but XDP and TC programs wouldn't load because BPF_LINK_CREATE call was updated in newer kernels. bpfman fails to run as a systemd service because of some capabilities issues in the bpfman.service file. Clone the bpfman Repo You can build and run bpfman from anywhere. However, if you plan to make changes to the bpfman operator, it will need to be under your GOPATH because Kubernetes Code-generator does not work outside of GOPATH issue 86753 . Assuming your GOPATH is set to the typical $HOME/go , your repo should live in $HOME/go/src/github.com/bpfman/bpfman mkdir -p $HOME/go/src/github.com/bpfman cd $HOME/go/src/github.com/bpfman git clone git@github.com:bpfman/bpfman.git Building bpfman To just test with the latest bpfman, containerized image are stored in quay.io/bpfman (see bpfman Container Images ). To build with local changes, use the following commands. If you are building bpfman for the first time OR the eBPF code has changed: cargo xtask build-ebpf --libbpf-dir /path/to/libbpf If protobuf files have changed: cargo xtask build-proto To build bpfman: cargo build Development Environment Setup To build bpfman, the following packages must be installed. Install Rust Toolchain For further detailed instructions, see Rust Stable & Rust Nightly . curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source \"$HOME/.cargo/env\" rustup toolchain install nightly -c rustfmt,clippy,rust-src Install LLVM LLVM 11 or later must be installed. Linux package managers should provide a recent enough release. dnf based OS: sudo dnf install llvm-devel clang-devel elfutils-libelf-devel apt based OS: sudo apt install clang lldb lld libelf-dev gcc-multilib Install Protobuf Compiler For further detailed instructions, see protoc . dnf based OS: sudo dnf install protobuf-compiler apt based OS: sudo apt install protobuf-compiler Install GO protobuf Compiler Extensions See Quick Start Guide for gRPC in Go for installation instructions. Local libbpf Checkout a local copy of libbpf. git clone https://github.com/libbpf/libbpf --branch v0.8.0 Install perl Install perl : dnf based OS: sudo dnf install perl apt based OS: sudo apt install perl Install Yaml Formatter As part of CI, the Yaml files are validated with a Yaml formatter. Optionally, to verify locally, install the YAML Language Support by Red Hat VsCode Extension, or to format in bulk, install prettier . To install prettier : npm install -g prettier Then to flag which files are violating the formatting guide, run: prettier -l \"*.yaml\" And to write changes in place, run: prettier -f \"*.yaml\"","title":"Setup and Building"},{"location":"getting-started/building-bpfman/#setup-and-building-bpfman","text":"This section describes how to build bpfman. If this is the first time building bpfman, jump to the Development Environment Setup section for help installing the tooling. There is also an option to run images from a given release as opposed to building locally. Jump to the Run bpfman From Release Image section for installing from a fixed release.","title":"Setup and Building bpfman"},{"location":"getting-started/building-bpfman/#kernel-versions","text":"eBPF is still a relatively new technology and being actively developed. To take advantage of this constantly evolving technology, it is best to use the newest kernel version possible. If bpfman needs to be run on an older kernel, this section describes some of the kernel features bpfman relies on to work and which kernel the feature was first introduced. Major kernel features leveraged by bpfman: Program Extensions: Program Extensions allows bpfman to load multiple XDP or TC eBPF programs on an interface, which is not natively supported in the kernel. A dispatcher program is loaded as the one program on a given interface, and the user's XDP or TC programs are loaded as extensions to the dispatcher program. Introduced in Kernel 5.6. Pinning: Pinning allows the eBPF program to remain loaded when the loading process (bpfman) is stopped or restarted. Introduced in Kernel 4.11. BPF Perf Link: Support BPF perf link for tracing programs (Tracepoint, Uprobe and Kprobe) which enables pinning for these program types. Introduced in Kernel 5.15. Tested kernel versions: Fedora 34: Kernel 5.17.6-100.fc34.x86_64 XDP, TC, Tracepoint, Uprobe and Kprobe programs all loaded with bpfman running on localhost and running as systemd service. Fedora 33: Kernel 5.14.18-100.fc33.x86_64 XDP and TC programs loaded with bpfman running on localhost and running as systemd service once SELinux was disabled (see https://github.com/fedora-selinux/selinux-policy/pull/806). Tracepoint, Uprobe and Kprobe programs failed to load because they require the BPF Perf Link support. Fedora 32: Kernel 5.11.22-100.fc32.x86_64 XDP and TC programs loaded with bpfman running on localhost once SELinux was disabled (see https://github.com/fedora-selinux/selinux-policy/pull/806). bpfman fails to run as a systemd service because of some capabilities issues in the bpfman.service file. Tracepoint, Uprobe and Kprobe programs failed to load because they require the BPF Perf Link support. Fedora 31: Kernel 5.8.18-100.fc31.x86_64 bpfman was able to start on localhost, but XDP and TC programs wouldn't load because BPF_LINK_CREATE call was updated in newer kernels. bpfman fails to run as a systemd service because of some capabilities issues in the bpfman.service file.","title":"Kernel Versions"},{"location":"getting-started/building-bpfman/#clone-the-bpfman-repo","text":"You can build and run bpfman from anywhere. However, if you plan to make changes to the bpfman operator, it will need to be under your GOPATH because Kubernetes Code-generator does not work outside of GOPATH issue 86753 . Assuming your GOPATH is set to the typical $HOME/go , your repo should live in $HOME/go/src/github.com/bpfman/bpfman mkdir -p $HOME/go/src/github.com/bpfman cd $HOME/go/src/github.com/bpfman git clone git@github.com:bpfman/bpfman.git","title":"Clone the bpfman Repo"},{"location":"getting-started/building-bpfman/#building-bpfman","text":"To just test with the latest bpfman, containerized image are stored in quay.io/bpfman (see bpfman Container Images ). To build with local changes, use the following commands. If you are building bpfman for the first time OR the eBPF code has changed: cargo xtask build-ebpf --libbpf-dir /path/to/libbpf If protobuf files have changed: cargo xtask build-proto To build bpfman: cargo build","title":"Building bpfman"},{"location":"getting-started/building-bpfman/#development-environment-setup","text":"To build bpfman, the following packages must be installed.","title":"Development Environment Setup"},{"location":"getting-started/building-bpfman/#install-rust-toolchain","text":"For further detailed instructions, see Rust Stable & Rust Nightly . curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source \"$HOME/.cargo/env\" rustup toolchain install nightly -c rustfmt,clippy,rust-src","title":"Install Rust Toolchain"},{"location":"getting-started/building-bpfman/#install-llvm","text":"LLVM 11 or later must be installed. Linux package managers should provide a recent enough release. dnf based OS: sudo dnf install llvm-devel clang-devel elfutils-libelf-devel apt based OS: sudo apt install clang lldb lld libelf-dev gcc-multilib","title":"Install LLVM"},{"location":"getting-started/building-bpfman/#install-protobuf-compiler","text":"For further detailed instructions, see protoc . dnf based OS: sudo dnf install protobuf-compiler apt based OS: sudo apt install protobuf-compiler","title":"Install Protobuf Compiler"},{"location":"getting-started/building-bpfman/#install-go-protobuf-compiler-extensions","text":"See Quick Start Guide for gRPC in Go for installation instructions.","title":"Install GO protobuf Compiler Extensions"},{"location":"getting-started/building-bpfman/#local-libbpf","text":"Checkout a local copy of libbpf. git clone https://github.com/libbpf/libbpf --branch v0.8.0","title":"Local libbpf"},{"location":"getting-started/building-bpfman/#install-perl","text":"Install perl : dnf based OS: sudo dnf install perl apt based OS: sudo apt install perl","title":"Install perl"},{"location":"getting-started/building-bpfman/#install-yaml-formatter","text":"As part of CI, the Yaml files are validated with a Yaml formatter. Optionally, to verify locally, install the YAML Language Support by Red Hat VsCode Extension, or to format in bulk, install prettier . To install prettier : npm install -g prettier Then to flag which files are violating the formatting guide, run: prettier -l \"*.yaml\" And to write changes in place, run: prettier -f \"*.yaml\"","title":"Install Yaml Formatter"},{"location":"getting-started/cli-guide/","text":"CLI Guide bpfman offers several CLI commands to interact with the bpfman daemon. The CLI allows you to load , unload , get and list eBPF programs. Notes For This Guide As described in other sections, bpfman can be run as either a privileged process or a systemd service. If run as a privileged process, bpfman will most likely be run from your local development branch and will require sudo . Example: sudo ./target/debug/bpfman list If run as a systemd service, bpfman will most likely be installed in your $PATH, and will also require sudo . Example: sudo bpfman list The examples here use sudo bpfman in place of sudo ./target/debug/bpfman for readability, use as your system is deployed. eBPF object files used in the examples are taken from the examples and integration-test directories from the bpfman repository. Basic Syntax Below are the commands supported by bpfman . sudo bpfman --help A system daemon for loading BPF programs Usage: bpfman <COMMAND> Commands: load Load an eBPF program from a local .o file unload Unload an eBPF program using the program id list List all eBPF programs loaded via bpfman get Get an eBPF program using the program id image eBPF Bytecode Image related commands system Run bpfman as a service help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version bpfman load The bpfman load file and bpfman load image commands are used to load eBPF programs. The bpfman load file command is used to load a locally built eBPF program. The bpfman load image command is used to load an eBPF program packaged in a OCI container image from a given registry. Each program type (i.e. <COMMAND> ) has it's own set of attributes specific to the program type, and those attributes MUST come after the program type is entered. There are a common set of attributes, and those MUST come before the program type is entered. sudo bpfman load file --help Load an eBPF program from a local .o file Usage: bpfman load file [OPTIONS] --path <PATH> --name <NAME> <COMMAND> ------ Commands: --------- xdp Install an eBPF program on the XDP hook point for a given interface tc Install an eBPF program on the TC hook point for a given interface tracepoint Install an eBPF program on a Tracepoint kprobe Install an eBPF kprobe or kretprobe uprobe Install an eBPF uprobe or uretprobe help Print this message or the help of the given subcommand(s) Options: -------- -p, --path <PATH> Required: Location of local bytecode file as fully qualified file path. Example: --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n, --name <NAME> Required: The name of the function that is the entry point for the BPF program -g, --global <GLOBAL>... Optional: Global variables to be set when program is loaded. Format: <NAME>=<Hex Value> This is a very low level primitive. The caller is responsible for formatting the byte string appropriately considering such things as size, endianness, alignment and packing of data structures. -m, --metadata <METADATA> Optional: Specify Key/Value metadata to be attached to a program when it is loaded by bpfman. Format: <KEY>=<VALUE> This can later be used to list a certain subset of programs which contain the specified metadata. --map-owner-id <MAP_OWNER_ID> Optional: Program id of loaded eBPF program this eBPF program will share a map with. Only used when multiple eBPF programs need to share a map. Example: --map-owner-id 63178 -h, --help Print help (see a summary with '-h') and sudo bpfman load image --help Load an eBPF program packaged in a OCI container image from a given registry Usage: bpfman load image [OPTIONS] --image-url <IMAGE_URL> <COMMAND> Commands: xdp Install an eBPF program on the XDP hook point for a given interface tc Install an eBPF program on the TC hook point for a given interface tracepoint Install an eBPF program on a Tracepoint kprobe Install an eBPF kprobe or kretprobe uprobe Install an eBPF uprobe or uretprobe help Print this message or the help of the given subcommand(s) Options: -i, --image-url <IMAGE_URL> Required: Container Image URL. Example: --image-url quay.io/bpfman-bytecode/xdp_pass:latest -r, --registry-auth <REGISTRY_AUTH> Optional: Registry auth for authenticating with the specified image registry. This should be base64 encoded from the '<username>:<password>' string just like it's stored in the docker/podman host config. Example: --registry_auth \"YnjrcKw63PhDcQodiU9hYxQ2\" -p, --pull-policy <PULL_POLICY> Optional: Pull policy for remote images. [possible values: Always, IfNotPresent, Never] [default: IfNotPresent] -n, --name <NAME> Optional: The name of the function that is the entry point for the BPF program. If not provided, the program name defined as part of the bytecode image will be used. [default: ] -g, --global <GLOBAL>... Optional: Global variables to be set when program is loaded. Format: <NAME>=<Hex Value> This is a very low level primitive. The caller is responsible for formatting the byte string appropriately considering such things as size, endianness, alignment and packing of data structures. -m, --metadata <METADATA> Optional: Specify Key/Value metadata to be attached to a program when it is loaded by bpfman. Format: <KEY>=<VALUE> This can later be used to list a certain subset of programs which contain the specified metadata. Example: --metadata owner=acme --map-owner-id <MAP_OWNER_ID> Optional: Program id of loaded eBPF program this eBPF program will share a map with. Only used when multiple eBPF programs need to share a map. Example: --map-owner-id 63178 -h, --help Print help (see a summary with '-h') When using either load command, --path , --image-url , --registry-auth , --pull-policy , --name , --global , --metadata and --map-owner-id must be entered before the <COMMAND> ( xdp , tc , tracepoint , etc) is entered. Then each <COMMAND> has its own custom parameters (same for both bpfman load file and bpfman load image ): sudo bpfman load file xdp --help Install an eBPF program on the XDP hook point for a given interface Usage: bpfman load file --path <PATH> --name <NAME> xdp [OPTIONS] --iface <IFACE> --priority <PRIORITY> ------ Options: -------- -i, --iface <IFACE> Required: Interface to load program on -p, --priority <PRIORITY> Required: Priority to run program in chain. Lower value runs first --proceed-on <PROCEED_ON>... Optional: Proceed to call other programs in chain on this exit code. Multiple values supported by repeating the parameter. Example: --proceed-on \"pass\" --proceed-on \"drop\" [possible values: aborted, drop, pass, tx, redirect, dispatcher_return] [default: pass, dispatcher_return] -h, --help Print help (see a summary with '-h') Example loading from local file ( --path is the fully qualified path): sudo bpfman load file --path $HOME/src/bpfman/tests/integration-test/bpf/.output/xdp_pass.bpf.o --name \"pass\" xdp --iface vethb2795c7 --priority 100 Example from image in remote repository (Note: --name is built into the image and is not required): sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethb2795c7 --priority 100 The tc command is similar to xdp , but it also requires the direction option and the proceed-on values are different. sudo bpfman load file tc -h Install an eBPF program on the TC hook point for a given interface Usage: bpfman load file --path <PATH> --name <NAME> tc [OPTIONS] --direction <DIRECTION> --iface <IFACE> --priority <PRIORITY> ------ Options: -------- -d, --direction <DIRECTION> Required: Direction to apply program. [possible values: ingress, egress] -i, --iface <IFACE> Required: Interface to load program on -p, --priority <PRIORITY> Required: Priority to run program in chain. Lower value runs first --proceed-on <PROCEED_ON>... Optional: Proceed to call other programs in chain on this exit code. Multiple values supported by repeating the parameter. Example: --proceed-on \"ok\" --proceed-on \"pipe\" [possible values: unspec, ok, reclassify, shot, pipe, stolen, queued, repeat, redirect, trap, dispatcher_return] [default: ok, pipe, dispatcher_return] -h, --help Print help (see a summary with '-h') The following is an example of the tc command using short option names: sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/tc_pass.bpf.o -n \"pass\" tc -d ingress -i mynet1 -p 40 For the tc_pass.bpf.o program loaded with the command above, the name would be set as shown in the following snippet: SEC ( \"classifier/pass\" ) int accept ( struct __sk_buff * skb ) { Additional Load Examples Below are some additional examples of bpfman load commands: XDP sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o --name \"xdp_stats\" xdp --iface vethb2795c7 --priority 35 TC sudo bpfman load file --path $HOME/src/bpfman/examples/go-tc-counter/bpf_bpfel.o --name \"stats\"\" tc --direction ingress --iface vethb2795c7 --priority 110 Kprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/kprobe:latest kprobe -f try_to_wake_up Kretprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/kretprobe:latest kprobe -f try_to_wake_up -r Uprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/uprobe:latest uprobe -f \"malloc\" -t \"libc\" Uretprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/uretprobe:latest uprobe -f \"malloc\" -t \"libc\" -r Setting Global Variables in eBPF Programs Global variables can be set for any eBPF program type when loading as follows: sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/tc_pass.bpf.o -g GLOBAL_u8=01020304 GLOBAL_u32=0A0B0C0D -n \"pass\" tc -d ingress -i mynet1 -p 40 Note, that when setting global variables, the eBPF program being loaded must have global variables named with the strings given, and the size of the value provided must match the size of the given variable. For example, the above command can be used to update the following global variables in an eBPF program. volatile const __u32 GLOBAL_u8 = 0 ; volatile const __u32 GLOBAL_u32 = 0 ; Modifying the Proceed-On Behavior The proceed-on setting applies to xdp and tc programs. For both of these program types, an ordered list of eBPF programs is maintained per attach point. The proceed-on setting determines whether processing will \"proceed\" to the next eBPF program in the list, or terminate processing and return, based on the program's return value. For example, the default proceed-on configuration for an xdp program can be modified as follows: sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/xdp_pass.bpf.o -n \"pass\" xdp -i mynet1 -p 30 --proceed-on drop pass dispatcher_return Sharing Maps Between eBPF Programs WARNING Currently for the map sharing feature to work the LIBBPF_PIN_BY_NAME flag MUST be set in the shared bpf map definitions. Please see this aya issue for future work that will change this requirement. To share maps between eBPF programs, first load the eBPF program that owns the maps. One eBPF program must own the maps. sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n \"xdp_stats\" xdp --iface vethb2795c7 --priority 100 6371 Next, load additional eBPF programs that will share the existing maps by passing the program id of the eBPF program that owns the maps using the --map-owner-id parameter: sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n \"xdp_stats\" --map-owner-id 6371 xdp --iface vethff657c7 --priority 100 6373 Use the bpfman get <ID> command to display the configuration: sudo bpfman list Program ID Name Type Load Time 6371 xdp_stats xdp 2023-07-18T16:50:46-0400 6373 xdp_stats xdp 2023-07-18T16:51:06-0400 sudo bpfman get 6371 Bpfman State --------------- Name: xdp_stats Path: /home/<$USER>/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6371 Map Owner ID: None Map Used By: 6371 6373 Priority: 50 Iface: vethff657c7 Position: 1 Proceed On: pass, dispatcher_return : sudo bpfman get 6373 Bpfman State --------------- Name: xdp_stats Path: /home/<$USER>/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6371 Map Owner ID: 6371 Map Used By: 6371 6373 Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return : As the output shows, the first program ( 6371 ) owns the map, with Map Owner ID of None and the Map Pin Path ( /run/bpfman/fs/maps/6371 ) that includes its own ID. The second program ( 6373 ) references the first program via the Map Owner ID set to 6371 and the Map Pin Path ( /run/bpfman/fs/maps/6371 ) set to same directory as the first program, which includes the first program's ID. The output for both commands shows the map is being used by both programs via the Map Used By with values of 6371 and 6373 . The eBPF programs can be unloaded any order, the Map Pin Path will not be deleted until all the programs referencing the maps are unloaded: sudo bpfman unload 6371 sudo bpfman unload 6373 bpfman list The bpfman list command lists all the bpfman loaded eBPF programs: sudo bpfman list Program ID Name Type Load Time 6201 pass xdp 2023-07-17T17:17:53-0400 6202 sys_enter_openat tracepoint 2023-07-17T17:19:09-0400 6204 stats tc 2023-07-17T17:20:14-0400 To see all eBPF programs loaded on the system, include the --all option. sudo bpfman list --all Program ID Name Type Load Time 52 restrict_filesy lsm 2023-05-03T12:53:34-0400 166 dump_bpf_map tracing 2023-05-03T12:53:52-0400 167 dump_bpf_prog tracing 2023-05-03T12:53:52-0400 455 cgroup_device 2023-05-03T12:58:26-0400 : 6190 cgroup_skb 2023-07-17T17:15:23-0400 6191 cgroup_device 2023-07-17T17:15:23-0400 6192 cgroup_skb 2023-07-17T17:15:23-0400 6193 cgroup_skb 2023-07-17T17:15:23-0400 6194 cgroup_device 2023-07-17T17:15:23-0400 6201 pass xdp 2023-07-17T17:17:53-0400 6202 sys_enter_openat tracepoint 2023-07-17T17:19:09-0400 6203 dispatcher tc 2023-07-17T17:20:14-0400 6204 stats tc 2023-07-17T17:20:14-0400 6207 xdp xdp 2023-07-17T17:27:13-0400 To filter on a given program type, include the --program-type parameter: sudo bpfman list --all --program-type tc Program ID Name Type Load Time 6203 dispatcher tc 2023-07-17T17:20:14-0400 6204 stats tc 2023-07-17T17:20:14-0400 bpfman get To retrieve detailed information for a loaded eBPF program, use the bpfman get <ID> command. If the eBPF program was loaded via bpfman, then there will be a Bpfman State section with bpfman related attributes and a Kernel State section with kernel information. If the eBPF program was loaded outside of bpfman, then the Bpfman State section will be empty and Kernel State section will be populated. sudo bpfman get 6204 Bpfman State --------------- Name: stats Image URL: quay.io/bpfman-bytecode/go-tc-counter:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6204 Map Owner ID: None Map Used By: 6204 Priority: 100 Iface: vethff657c7 Position: 0 Direction: eg Proceed On: pipe, dispatcher_return Kernel State ---------------------------------- ID: 6204 Name: stats Type: tc Loaded At: 2023-07-17T17:20:14-0400 Tag: ead94553702a3742 GPL Compatible: true Map IDs: [2705] BTF ID: 2821 Size Translated (bytes): 176 JITed: true Size JITed (bytes): 116 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 24 sudo bpfman get 6190 Bpfman State --------------- NONE Kernel State ---------------------------------- ID: 6190 Name: None Type: cgroup_skb Loaded At: 2023-07-17T17:15:23-0400 Tag: 6deef7357e7b4530 GPL Compatible: true Map IDs: [] BTF ID: 0 Size Translated (bytes): 64 JITed: true Size JITed (bytes): 55 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 8 bpfman unload The bpfman unload command takes the program id from the load or list command as a parameter, and unloads the requested eBPF program: sudo bpfman unload 6204 sudo bpfman list Program ID Name Type Load Time 6201 pass xdp 2023-07-17T17:17:53-0400 6202 sys_enter_openat tracepoint 2023-07-17T17:19:09-0400 bpfman image pull The bpfman image pull command pulls a given bytecode image for future use by a load command. sudo bpfman image pull --help Pull a bytecode image for future use by a load command Usage: bpfman image pull [OPTIONS] --image-url <IMAGE_URL> Options: -i, --image-url <IMAGE_URL> Required: Container Image URL. Example: --image-url quay.io/bpfman-bytecode/xdp_pass:latest -r, --registry-auth <REGISTRY_AUTH> Optional: Registry auth for authenticating with the specified image registry. This should be base64 encoded from the '<username>:<password>' string just like it's stored in the docker/podman host config. Example: --registry_auth \"YnjrcKw63PhDcQodiU9hYxQ2\" -p, --pull-policy <PULL_POLICY> Optional: Pull policy for remote images. [possible values: Always, IfNotPresent, Never] [default: IfNotPresent] -h, --help Print help (see a summary with '-h') Example usage: sudo bpfman image pull --image-url quay.io/bpfman-bytecode/xdp_pass:latest Successfully downloaded bytecode Then when loaded, the local image will be used: sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest --pull-policy IfNotPresent xdp --iface vethff657c7 --priority 100 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/406681 Map Owner ID: None Maps Used By: None Priority: 100 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 406681 Name: pass Type: xdp Loaded At: 1917-01-27T01:37:06-0500 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [736646] BTF ID: 555560 Size Translated (bytes): 96 JITted: true Size JITted: 67 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 9","title":"CLI Guide"},{"location":"getting-started/cli-guide/#cli-guide","text":"bpfman offers several CLI commands to interact with the bpfman daemon. The CLI allows you to load , unload , get and list eBPF programs.","title":"CLI Guide"},{"location":"getting-started/cli-guide/#notes-for-this-guide","text":"As described in other sections, bpfman can be run as either a privileged process or a systemd service. If run as a privileged process, bpfman will most likely be run from your local development branch and will require sudo . Example: sudo ./target/debug/bpfman list If run as a systemd service, bpfman will most likely be installed in your $PATH, and will also require sudo . Example: sudo bpfman list The examples here use sudo bpfman in place of sudo ./target/debug/bpfman for readability, use as your system is deployed. eBPF object files used in the examples are taken from the examples and integration-test directories from the bpfman repository.","title":"Notes For This Guide"},{"location":"getting-started/cli-guide/#basic-syntax","text":"Below are the commands supported by bpfman . sudo bpfman --help A system daemon for loading BPF programs Usage: bpfman <COMMAND> Commands: load Load an eBPF program from a local .o file unload Unload an eBPF program using the program id list List all eBPF programs loaded via bpfman get Get an eBPF program using the program id image eBPF Bytecode Image related commands system Run bpfman as a service help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version","title":"Basic Syntax"},{"location":"getting-started/cli-guide/#bpfman-load","text":"The bpfman load file and bpfman load image commands are used to load eBPF programs. The bpfman load file command is used to load a locally built eBPF program. The bpfman load image command is used to load an eBPF program packaged in a OCI container image from a given registry. Each program type (i.e. <COMMAND> ) has it's own set of attributes specific to the program type, and those attributes MUST come after the program type is entered. There are a common set of attributes, and those MUST come before the program type is entered. sudo bpfman load file --help Load an eBPF program from a local .o file Usage: bpfman load file [OPTIONS] --path <PATH> --name <NAME> <COMMAND> ------ Commands: --------- xdp Install an eBPF program on the XDP hook point for a given interface tc Install an eBPF program on the TC hook point for a given interface tracepoint Install an eBPF program on a Tracepoint kprobe Install an eBPF kprobe or kretprobe uprobe Install an eBPF uprobe or uretprobe help Print this message or the help of the given subcommand(s) Options: -------- -p, --path <PATH> Required: Location of local bytecode file as fully qualified file path. Example: --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n, --name <NAME> Required: The name of the function that is the entry point for the BPF program -g, --global <GLOBAL>... Optional: Global variables to be set when program is loaded. Format: <NAME>=<Hex Value> This is a very low level primitive. The caller is responsible for formatting the byte string appropriately considering such things as size, endianness, alignment and packing of data structures. -m, --metadata <METADATA> Optional: Specify Key/Value metadata to be attached to a program when it is loaded by bpfman. Format: <KEY>=<VALUE> This can later be used to list a certain subset of programs which contain the specified metadata. --map-owner-id <MAP_OWNER_ID> Optional: Program id of loaded eBPF program this eBPF program will share a map with. Only used when multiple eBPF programs need to share a map. Example: --map-owner-id 63178 -h, --help Print help (see a summary with '-h') and sudo bpfman load image --help Load an eBPF program packaged in a OCI container image from a given registry Usage: bpfman load image [OPTIONS] --image-url <IMAGE_URL> <COMMAND> Commands: xdp Install an eBPF program on the XDP hook point for a given interface tc Install an eBPF program on the TC hook point for a given interface tracepoint Install an eBPF program on a Tracepoint kprobe Install an eBPF kprobe or kretprobe uprobe Install an eBPF uprobe or uretprobe help Print this message or the help of the given subcommand(s) Options: -i, --image-url <IMAGE_URL> Required: Container Image URL. Example: --image-url quay.io/bpfman-bytecode/xdp_pass:latest -r, --registry-auth <REGISTRY_AUTH> Optional: Registry auth for authenticating with the specified image registry. This should be base64 encoded from the '<username>:<password>' string just like it's stored in the docker/podman host config. Example: --registry_auth \"YnjrcKw63PhDcQodiU9hYxQ2\" -p, --pull-policy <PULL_POLICY> Optional: Pull policy for remote images. [possible values: Always, IfNotPresent, Never] [default: IfNotPresent] -n, --name <NAME> Optional: The name of the function that is the entry point for the BPF program. If not provided, the program name defined as part of the bytecode image will be used. [default: ] -g, --global <GLOBAL>... Optional: Global variables to be set when program is loaded. Format: <NAME>=<Hex Value> This is a very low level primitive. The caller is responsible for formatting the byte string appropriately considering such things as size, endianness, alignment and packing of data structures. -m, --metadata <METADATA> Optional: Specify Key/Value metadata to be attached to a program when it is loaded by bpfman. Format: <KEY>=<VALUE> This can later be used to list a certain subset of programs which contain the specified metadata. Example: --metadata owner=acme --map-owner-id <MAP_OWNER_ID> Optional: Program id of loaded eBPF program this eBPF program will share a map with. Only used when multiple eBPF programs need to share a map. Example: --map-owner-id 63178 -h, --help Print help (see a summary with '-h') When using either load command, --path , --image-url , --registry-auth , --pull-policy , --name , --global , --metadata and --map-owner-id must be entered before the <COMMAND> ( xdp , tc , tracepoint , etc) is entered. Then each <COMMAND> has its own custom parameters (same for both bpfman load file and bpfman load image ): sudo bpfman load file xdp --help Install an eBPF program on the XDP hook point for a given interface Usage: bpfman load file --path <PATH> --name <NAME> xdp [OPTIONS] --iface <IFACE> --priority <PRIORITY> ------ Options: -------- -i, --iface <IFACE> Required: Interface to load program on -p, --priority <PRIORITY> Required: Priority to run program in chain. Lower value runs first --proceed-on <PROCEED_ON>... Optional: Proceed to call other programs in chain on this exit code. Multiple values supported by repeating the parameter. Example: --proceed-on \"pass\" --proceed-on \"drop\" [possible values: aborted, drop, pass, tx, redirect, dispatcher_return] [default: pass, dispatcher_return] -h, --help Print help (see a summary with '-h') Example loading from local file ( --path is the fully qualified path): sudo bpfman load file --path $HOME/src/bpfman/tests/integration-test/bpf/.output/xdp_pass.bpf.o --name \"pass\" xdp --iface vethb2795c7 --priority 100 Example from image in remote repository (Note: --name is built into the image and is not required): sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethb2795c7 --priority 100 The tc command is similar to xdp , but it also requires the direction option and the proceed-on values are different. sudo bpfman load file tc -h Install an eBPF program on the TC hook point for a given interface Usage: bpfman load file --path <PATH> --name <NAME> tc [OPTIONS] --direction <DIRECTION> --iface <IFACE> --priority <PRIORITY> ------ Options: -------- -d, --direction <DIRECTION> Required: Direction to apply program. [possible values: ingress, egress] -i, --iface <IFACE> Required: Interface to load program on -p, --priority <PRIORITY> Required: Priority to run program in chain. Lower value runs first --proceed-on <PROCEED_ON>... Optional: Proceed to call other programs in chain on this exit code. Multiple values supported by repeating the parameter. Example: --proceed-on \"ok\" --proceed-on \"pipe\" [possible values: unspec, ok, reclassify, shot, pipe, stolen, queued, repeat, redirect, trap, dispatcher_return] [default: ok, pipe, dispatcher_return] -h, --help Print help (see a summary with '-h') The following is an example of the tc command using short option names: sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/tc_pass.bpf.o -n \"pass\" tc -d ingress -i mynet1 -p 40 For the tc_pass.bpf.o program loaded with the command above, the name would be set as shown in the following snippet: SEC ( \"classifier/pass\" ) int accept ( struct __sk_buff * skb ) {","title":"bpfman load"},{"location":"getting-started/cli-guide/#additional-load-examples","text":"Below are some additional examples of bpfman load commands: XDP sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o --name \"xdp_stats\" xdp --iface vethb2795c7 --priority 35 TC sudo bpfman load file --path $HOME/src/bpfman/examples/go-tc-counter/bpf_bpfel.o --name \"stats\"\" tc --direction ingress --iface vethb2795c7 --priority 110 Kprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/kprobe:latest kprobe -f try_to_wake_up Kretprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/kretprobe:latest kprobe -f try_to_wake_up -r Uprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/uprobe:latest uprobe -f \"malloc\" -t \"libc\" Uretprobe sudo bpfman load image --image-url quay.io/bpfman-bytecode/uretprobe:latest uprobe -f \"malloc\" -t \"libc\" -r","title":"Additional Load Examples"},{"location":"getting-started/cli-guide/#setting-global-variables-in-ebpf-programs","text":"Global variables can be set for any eBPF program type when loading as follows: sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/tc_pass.bpf.o -g GLOBAL_u8=01020304 GLOBAL_u32=0A0B0C0D -n \"pass\" tc -d ingress -i mynet1 -p 40 Note, that when setting global variables, the eBPF program being loaded must have global variables named with the strings given, and the size of the value provided must match the size of the given variable. For example, the above command can be used to update the following global variables in an eBPF program. volatile const __u32 GLOBAL_u8 = 0 ; volatile const __u32 GLOBAL_u32 = 0 ;","title":"Setting Global Variables in eBPF Programs"},{"location":"getting-started/cli-guide/#modifying-the-proceed-on-behavior","text":"The proceed-on setting applies to xdp and tc programs. For both of these program types, an ordered list of eBPF programs is maintained per attach point. The proceed-on setting determines whether processing will \"proceed\" to the next eBPF program in the list, or terminate processing and return, based on the program's return value. For example, the default proceed-on configuration for an xdp program can be modified as follows: sudo bpfman load file -p $HOME/src/bpfman/tests/integration-test/bpf/.output/xdp_pass.bpf.o -n \"pass\" xdp -i mynet1 -p 30 --proceed-on drop pass dispatcher_return","title":"Modifying the Proceed-On Behavior"},{"location":"getting-started/cli-guide/#sharing-maps-between-ebpf-programs","text":"WARNING Currently for the map sharing feature to work the LIBBPF_PIN_BY_NAME flag MUST be set in the shared bpf map definitions. Please see this aya issue for future work that will change this requirement. To share maps between eBPF programs, first load the eBPF program that owns the maps. One eBPF program must own the maps. sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n \"xdp_stats\" xdp --iface vethb2795c7 --priority 100 6371 Next, load additional eBPF programs that will share the existing maps by passing the program id of the eBPF program that owns the maps using the --map-owner-id parameter: sudo bpfman load file --path $HOME/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o -n \"xdp_stats\" --map-owner-id 6371 xdp --iface vethff657c7 --priority 100 6373 Use the bpfman get <ID> command to display the configuration: sudo bpfman list Program ID Name Type Load Time 6371 xdp_stats xdp 2023-07-18T16:50:46-0400 6373 xdp_stats xdp 2023-07-18T16:51:06-0400 sudo bpfman get 6371 Bpfman State --------------- Name: xdp_stats Path: /home/<$USER>/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6371 Map Owner ID: None Map Used By: 6371 6373 Priority: 50 Iface: vethff657c7 Position: 1 Proceed On: pass, dispatcher_return : sudo bpfman get 6373 Bpfman State --------------- Name: xdp_stats Path: /home/<$USER>/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6371 Map Owner ID: 6371 Map Used By: 6371 6373 Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return : As the output shows, the first program ( 6371 ) owns the map, with Map Owner ID of None and the Map Pin Path ( /run/bpfman/fs/maps/6371 ) that includes its own ID. The second program ( 6373 ) references the first program via the Map Owner ID set to 6371 and the Map Pin Path ( /run/bpfman/fs/maps/6371 ) set to same directory as the first program, which includes the first program's ID. The output for both commands shows the map is being used by both programs via the Map Used By with values of 6371 and 6373 . The eBPF programs can be unloaded any order, the Map Pin Path will not be deleted until all the programs referencing the maps are unloaded: sudo bpfman unload 6371 sudo bpfman unload 6373","title":"Sharing Maps Between eBPF Programs"},{"location":"getting-started/cli-guide/#bpfman-list","text":"The bpfman list command lists all the bpfman loaded eBPF programs: sudo bpfman list Program ID Name Type Load Time 6201 pass xdp 2023-07-17T17:17:53-0400 6202 sys_enter_openat tracepoint 2023-07-17T17:19:09-0400 6204 stats tc 2023-07-17T17:20:14-0400 To see all eBPF programs loaded on the system, include the --all option. sudo bpfman list --all Program ID Name Type Load Time 52 restrict_filesy lsm 2023-05-03T12:53:34-0400 166 dump_bpf_map tracing 2023-05-03T12:53:52-0400 167 dump_bpf_prog tracing 2023-05-03T12:53:52-0400 455 cgroup_device 2023-05-03T12:58:26-0400 : 6190 cgroup_skb 2023-07-17T17:15:23-0400 6191 cgroup_device 2023-07-17T17:15:23-0400 6192 cgroup_skb 2023-07-17T17:15:23-0400 6193 cgroup_skb 2023-07-17T17:15:23-0400 6194 cgroup_device 2023-07-17T17:15:23-0400 6201 pass xdp 2023-07-17T17:17:53-0400 6202 sys_enter_openat tracepoint 2023-07-17T17:19:09-0400 6203 dispatcher tc 2023-07-17T17:20:14-0400 6204 stats tc 2023-07-17T17:20:14-0400 6207 xdp xdp 2023-07-17T17:27:13-0400 To filter on a given program type, include the --program-type parameter: sudo bpfman list --all --program-type tc Program ID Name Type Load Time 6203 dispatcher tc 2023-07-17T17:20:14-0400 6204 stats tc 2023-07-17T17:20:14-0400","title":"bpfman list"},{"location":"getting-started/cli-guide/#bpfman-get","text":"To retrieve detailed information for a loaded eBPF program, use the bpfman get <ID> command. If the eBPF program was loaded via bpfman, then there will be a Bpfman State section with bpfman related attributes and a Kernel State section with kernel information. If the eBPF program was loaded outside of bpfman, then the Bpfman State section will be empty and Kernel State section will be populated. sudo bpfman get 6204 Bpfman State --------------- Name: stats Image URL: quay.io/bpfman-bytecode/go-tc-counter:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6204 Map Owner ID: None Map Used By: 6204 Priority: 100 Iface: vethff657c7 Position: 0 Direction: eg Proceed On: pipe, dispatcher_return Kernel State ---------------------------------- ID: 6204 Name: stats Type: tc Loaded At: 2023-07-17T17:20:14-0400 Tag: ead94553702a3742 GPL Compatible: true Map IDs: [2705] BTF ID: 2821 Size Translated (bytes): 176 JITed: true Size JITed (bytes): 116 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 24 sudo bpfman get 6190 Bpfman State --------------- NONE Kernel State ---------------------------------- ID: 6190 Name: None Type: cgroup_skb Loaded At: 2023-07-17T17:15:23-0400 Tag: 6deef7357e7b4530 GPL Compatible: true Map IDs: [] BTF ID: 0 Size Translated (bytes): 64 JITed: true Size JITed (bytes): 55 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 8","title":"bpfman get"},{"location":"getting-started/cli-guide/#bpfman-unload","text":"The bpfman unload command takes the program id from the load or list command as a parameter, and unloads the requested eBPF program: sudo bpfman unload 6204 sudo bpfman list Program ID Name Type Load Time 6201 pass xdp 2023-07-17T17:17:53-0400 6202 sys_enter_openat tracepoint 2023-07-17T17:19:09-0400","title":"bpfman unload"},{"location":"getting-started/cli-guide/#bpfman-image-pull","text":"The bpfman image pull command pulls a given bytecode image for future use by a load command. sudo bpfman image pull --help Pull a bytecode image for future use by a load command Usage: bpfman image pull [OPTIONS] --image-url <IMAGE_URL> Options: -i, --image-url <IMAGE_URL> Required: Container Image URL. Example: --image-url quay.io/bpfman-bytecode/xdp_pass:latest -r, --registry-auth <REGISTRY_AUTH> Optional: Registry auth for authenticating with the specified image registry. This should be base64 encoded from the '<username>:<password>' string just like it's stored in the docker/podman host config. Example: --registry_auth \"YnjrcKw63PhDcQodiU9hYxQ2\" -p, --pull-policy <PULL_POLICY> Optional: Pull policy for remote images. [possible values: Always, IfNotPresent, Never] [default: IfNotPresent] -h, --help Print help (see a summary with '-h') Example usage: sudo bpfman image pull --image-url quay.io/bpfman-bytecode/xdp_pass:latest Successfully downloaded bytecode Then when loaded, the local image will be used: sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest --pull-policy IfNotPresent xdp --iface vethff657c7 --priority 100 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/406681 Map Owner ID: None Maps Used By: None Priority: 100 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 406681 Name: pass Type: xdp Loaded At: 1917-01-27T01:37:06-0500 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [736646] BTF ID: 555560 Size Translated (bytes): 96 JITted: true Size JITted: 67 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 9","title":"bpfman image pull"},{"location":"getting-started/example-bpf-k8s/","text":"Deploying Example eBPF Programs On Kubernetes This section will describe loading bytecode on a Kubernetes cluster and launching the userspace program. The approach is slightly different when running on a Kubernetes cluster. The eBPF bytecode should be loaded by an administrator, not the userspace program itself. This section assumes there is already a Kubernetes cluster running and bpfman is running in the cluster. See Deploying the bpfman-operator for details on deploying bpfman on a Kubernetes cluster, but the quickest solution is to run a Kubernetes KIND Cluster: cd bpfman/bpfman-operator/ make run-on-kind Loading eBPF Bytecode On Kubernetes Instead of using the userspace program or CLI to load the eBPF bytecode as done in previous sections, the bytecode will be loaded by creating a Kubernetes CRD object. There is a CRD object for each eBPF program type bpfman supports. Edit the sample yaml files to customize any configuration values: TcProgram CRD: go-tc-counter/bytecode.yaml TracepointProgram CRD: go-tracepoint-counter/bytecode.yaml XdpProgram CRD: go-xdp-counter/bytecode.yaml KprobeProgram CRD: bpfman-operator/config/samples/bpfman.io_v1alpha1_kprobe_kprobeprogram.yaml UprobeProgram CRD: bpfman-operator/config/samples/bpfman.io_v1alpha1_uprobe_uprobeprogram.yaml Sample bytecode yaml with XdpProgram CRD: cat examples/config/base/go-xdp-counter/bytecode.yaml apiVersion: bpfman.io/v1alpha1 kind: XdpProgram metadata: labels: app.kubernetes.io/name: xdpprogram name: go-xdp-counter-example spec: name: xdp_stats # Select all nodes nodeselector: {} interfaceselector: primarynodeinterface: true priority: 55 bytecode: image: url: quay.io/bpfman-bytecode/go-xdp-counter:latest Note that all the sample yaml files are configured with the bytecode running on all nodes ( nodeselector: {} ). This can be change to run on specific nodes, but the DaemonSet yaml for the userspace program, which is described below, should have an equivalent change. Make any changes to the go-xdp-counter-bytecode.yaml , then repeat for go-tc-counter-bytecode.yaml and go-tracepoint-counter-bytecode.yaml and then apply the updated yamls: kubectl apply -f examples/config/base/go-xdp-counter/bytecode.yaml xdpprogram.bpfman.io/go-xdp-counter-example created kubectl apply -f examples/config/base/go-tc-counter/bytecode.yaml tcprogram.bpfman.io/go-tc-counter-example created kubectl apply -f examples/config/base/go-tracepoint-counter/bytecode.yaml tracepointprogram.bpfman.io/go-tracepoint-counter-example created Following the diagram for XDP example (Blue numbers): The user creates a XdpProgram object with the parameters associated with the eBPF bytecode, like interface, priority and BFP bytecode image. The name of the XdpProgram object in this example is go-xdp-counter-example . The XdpProgram is applied using kubectl , but in a more practical deployment, the XdpProgram would be applied by the application or a controller. bpfman-agent , running on each node, is watching for all changes to XdpProgram objects. When it sees a XdpProgram object created or modified, it makes sure a BpfProgram object for that node exists. The name of the BpfProgram object is the XdpProgram object name with the node name and interface or attach point appended. bpfman-agent then determines if it should be running on the given node, loads or unloads as needed by making gRPC calls the bpfman . bpfman behaves the same as described in the running locally example. bpfman-agent finally updates the status of the BpfProgram object. bpfman-operator watches all BpfProgram objects, and updates the status of the XdpProgram object indicating if the eBPF program has been applied to all the desired nodes or not. To retrieve information on the XdpProgram objects: kubectl get xdpprograms NAME PRIORITY DIRECTION go-xdp-counter-example 55 kubectl get xdpprograms go-xdp-counter-example -o yaml apiVersion: bpfman.io/v1alpha1 kind: XdpProgram metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"bpfman.io/v1alpha1\",\"kind\":\"XdpProgram\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/name\":\"xdpprogram\"},\"name\":\"go-xdp-counter-example\"},\"spec\":{\"bpffunctionname\":\"xdp_stats\",\"bytecode\":{\"image\":{\"url\":\"quay.io/bpfman-bytecode/go-xdp-counter:latest\"}},\"interfaceselector\":{\"primarynodeinterface\":true},\"nodeselector\":{},\"priority\":55}} creationTimestamp: \"2023-11-06T21:05:15Z\" finalizers: - bpfman.io.operator/finalizer generation: 2 labels: app.kubernetes.io/name: xdpprogram name: go-xdp-counter-example resourceVersion: \"3103\" uid: edd45e2e-a40b-4668-ac76-c1f1eb63a23b spec: bpffunctionname: xdp_stats bytecode: image: imagepullpolicy: IfNotPresent url: quay.io/bpfman-bytecode/go-xdp-counter:latest interfaceselector: primarynodeinterface: true mapownerselector: {} nodeselector: {} priority: 55 proceedon: - pass - dispatcher_return status: conditions: - lastTransitionTime: \"2023-11-06T21:05:21Z\" message: bpfProgramReconciliation Succeeded on all nodes reason: ReconcileSuccess status: \"True\" type: ReconcileSuccess To retrieve information on the BpfProgram objects: kubectl get bpfprograms NAME AGE : 4822-bpfman-deployment-control-plane 60m 4825-bpfman-deployment-control-plane 60m go-tc-counter-example-bpfman-deployment-control-plane-eth0 61m go-tracepoint-counter-example-bpfman-deployment-control-plane-syscalls-sys-enter-kill 61m go-xdp-counter-example-bpfman-deployment-control-plane-eth0 61m go-xdp-counter-sharing-map-example-bpfman-deployment-control-plane-eth0 60m tc-dispatcher-4805-bpfman-deployment-control-plane 60m xdp-dispatcher-4816-bpfman-deployment-control-plane 60m kubectl get go-xdp-counter-example-bpfman-deployment-control-plane-eth0 -o yaml apiVersion: bpfman.io/v1alpha1 kind: BpfProgram metadata: annotations: bpfman.io.xdpprogramcontroller/interface: eth0 bpfman.io/ProgramId: \"4801\" creationTimestamp: \"2023-11-06T21:05:15Z\" finalizers: - bpfman.io.xdpprogramcontroller/finalizer generation: 1 labels: bpfman.io/ownedByProgram: go-xdp-counter-example kubernetes.io/hostname: bpfman-deployment-control-plane name: go-xdp-counter-example-bpfman-deployment-control-plane-eth0 ownerReferences: - apiVersion: bpfman.io/v1alpha1 blockOwnerDeletion: true controller: true kind: XdpProgram name: go-xdp-counter-example uid: edd45e2e-a40b-4668-ac76-c1f1eb63a23b resourceVersion: \"3102\" uid: f7ffd156-168b-4dc8-be38-18c42626a631 spec: type: xdp status: conditions: - lastTransitionTime: \"2023-11-06T21:05:21Z\" message: Successfully loaded bpfProgram reason: bpfmanLoaded status: \"True\" type: Loaded Loading Userspace Container On Kubernetes Here, a userspace container is deployed to consume the map data generated by the eBPF counter program. bpfman provides a Container Storage Interface (CSI) driver for exposing eBPF maps into a userspace container. To avoid having to mount a host directory that contains the map pinned file into the container and forcing the container to have permissions to access that host directory, the CSI driver mounts the map at a specified location in the container. All the examples use CSI, here is go-xdp-counter/deployment.yaml for reference: cd bpfman/examples/ cat config/base/go-xdp-counter/deployment.yaml : --- apiVersion: apps/v1 kind: DaemonSet metadata: name: go-xdp-counter-ds namespace: go-xdp-counter labels: k8s-app: go-xdp-counter spec: : template: : spec: : containers: - name: go-xdp-counter : volumeMounts: - name: go-xdp-counter-maps <==== 2) VolumeMount in container mountPath: /run/xdp/maps <==== 2a) Mount path in the container readOnly: true volumes: - name: go-xdp-counter-maps <==== 1) Volume describing the map csi: driver: csi.bpfman.io <==== 1a) bpfman CSI Driver volumeAttributes: csi.bpfman.io/program: go-xdp-counter-example <==== 1b) eBPF Program owning the map csi.bpfman.io/maps: xdp_stats_map <==== 1c) Map to be exposed to the container Loading A Userspace Container Image The userspace programs have been pre-built and can be found here: quay.io/bpfman-userspace/go-tc-counter:latest quay.io/bpfman-userspace/go-tracepoint-counter:latest quay.io/bpfman-userspace/go-xdp-counter:latest The example yaml files below are loading from these image. go-tc-counter/deployment.yaml go-tracepoint-counter/deployment.yaml go-xdp-counter/deployment.yaml The userspace program in a Kubernetes Deployment doesn't interacts directly with bpfman like it did in the local host deployment. Instead, the userspace program running on each node, if needed, reads the BpfProgram object from the KubeApiServer to gather additional information about the loaded eBPF program. To interact with the KubeApiServer, RBAC must be setup properly to access the BpfProgram object. The bpfman-operator defined the yaml for several ClusterRoles that can be used to access the different bpfman related CRD objects with different access rights. The example userspace containers will use the bpfprogram-viewer-role , which allows Read-Only access to the BpfProgram object. This ClusterRole is created automatically by the bpfman-operator . The remaining objects (NameSpace, ServiceAccount, ClusterRoleBinding and examples DaemonSet) can be created for each program type as follows: cd bpfman/ kubectl create -f examples/config/base/go-xdp-counter/deployment.yaml kubectl create -f examples/config/base/go-tc-counter/deployment.yaml kubectl create -f examples/config/base/go-tracepoint-counter/deployment.yaml Following the diagram for the XDP example (Green numbers): The userspace program queries the KubeApiServer for a specific BpfProgram object. The userspace program verifies the BpfProgram has been loaded and uses the map to periodically read the counter values. To see if the userspace programs are working, view the logs: NAMESPACE NAME READY STATUS RESTARTS AGE bpfman bpfman-daemon-jsgdh 3/3 Running 0 11m bpfman bpfman-operator-6c5c8887f7-qk28x 2/2 Running 0 12m go-tc-counter go-tc-counter-ds-9jv4g 1/1 Running 0 5m37s go-tracepoint-counter go-tracepoint-counter-ds-2gzbt 1/1 Running 0 5m35s go-xdp-counter go-xdp-counter-ds-2hs6g 1/1 Running 0 6m12s : kubectl logs -n go-xdp-counter go-xdp-counter-ds-2hs6g 2023/11/06 20:27:16 2429 packets received 2023/11/06 20:27:16 1328474 bytes received 2023/11/06 20:27:19 2429 packets received 2023/11/06 20:27:19 1328474 bytes received 2023/11/06 20:27:22 2430 packets received 2023/11/06 20:27:22 1328552 bytes received : To cleanup: kubectl delete -f examples/config/base/go-xdp-counter/deployment.yaml kubectl delete -f examples/config/base/go-xdp-counter/bytecode.yaml kubectl delete -f examples/config/base/go-tc-counter/deployment.yaml kubectl delete -f examples/config/base/go-tc-counter/bytecode.yaml kubectl delete -f examples/config/base/go-tracepoint-counter/deployment.yaml kubectl delete -f examples/config/base/go-tracepoint-counter/bytecode.yaml Automated Deployment The steps above are automated in the Makefile in the examples directory. Run make deploy to load each of the example bytecode and userspace yaml files, then make undeploy to unload them. cd bpfman/examples/ make deploy sed 's@URL_BC@quay.io/bpfman-bytecode/go-tc-counter:latest@' config/default/go-tc-counter/patch.yaml.env > config/default/go-tc-counter/patch.yaml cd config/default/go-tc-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tc-counter=quay.io/bpfman-userspace/go-tc-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tc-counter | kubectl apply -f - namespace/go-tc-counter created serviceaccount/bpfman-app-go-tc-counter created clusterrolebinding.rbac.authorization.k8s.io/bpfman-app-rolebinding-go-tc-counter created clusterrolebinding.rbac.authorization.k8s.io/privileged-scc-tc created daemonset.apps/go-tc-counter-ds created tcprogram.bpfman.io/go-tc-counter-example created sed 's@URL_BC@quay.io/bpfman-bytecode/go-tracepoint-counter:latest@' config/default/go-tracepoint-counter/patch.yaml.env > config/default/go-tracepoint-counter/patch.yaml cd config/default/go-tracepoint-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tracepoint-counter=quay.io/bpfman-userspace/go-tracepoint-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tracepoint-counter | kubectl apply -f - namespace/go-tracepoint-counter created serviceaccount/bpfman-app-go-tracepoint-counter created clusterrolebinding.rbac.authorization.k8s.io/bpfman-app-rolebinding-go-tracepoint-counter created clusterrolebinding.rbac.authorization.k8s.io/privileged-scc-tracepoint created daemonset.apps/go-tracepoint-counter-ds created tracepointprogram.bpfman.io/go-tracepoint-counter-example created sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter/patch.yaml.env > config/default/go-xdp-counter/patch.yaml cd config/default/go-xdp-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter | kubectl apply -f - namespace/go-xdp-counter unchanged serviceaccount/bpfman-app-go-xdp-counter unchanged clusterrolebinding.rbac.authorization.k8s.io/bpfman-app-rolebinding-go-xdp-counter unchanged clusterrolebinding.rbac.authorization.k8s.io/privileged-scc-xdp unchanged daemonset.apps/go-xdp-counter-ds configured xdpprogram.bpfman.io/go-xdp-counter-example unchanged sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter-sharing-map/patch.yaml.env > config/default/go-xdp-counter-sharing-map/patch.yaml cd config/default/go-xdp-counter-sharing-map && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter-sharing-map | kubectl apply -f - xdpprogram.bpfman.io/go-xdp-counter-sharing-map-example created # Test Away ... make undeploy sed 's@URL_BC@quay.io/bpfman-bytecode/go-tc-counter:latest@' config/default/go-tc-counter/patch.yaml.env > config/default/go-tc-counter/patch.yaml cd config/default/go-tc-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tc-counter=quay.io/bpfman-userspace/go-tc-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tc-counter | kubectl delete --ignore-not-found=false -f - namespace \"go-tc-counter\" deleted serviceaccount \"bpfman-app-go-tc-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"bpfman-app-rolebinding-go-tc-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"privileged-scc-tc\" deleted daemonset.apps \"go-tc-counter-ds\" deleted tcprogram.bpfman.io \"go-tc-counter-example\" deleted sed 's@URL_BC@quay.io/bpfman-bytecode/go-tracepoint-counter:latest@' config/default/go-tracepoint-counter/patch.yaml.env > config/default/go-tracepoint-counter/patch.yaml cd config/default/go-tracepoint-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tracepoint-counter=quay.io/bpfman-userspace/go-tracepoint-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tracepoint-counter | kubectl delete --ignore-not-found=false -f - namespace \"go-tracepoint-counter\" deleted serviceaccount \"bpfman-app-go-tracepoint-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"bpfman-app-rolebinding-go-tracepoint-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"privileged-scc-tracepoint\" deleted daemonset.apps \"go-tracepoint-counter-ds\" deleted tracepointprogram.bpfman.io \"go-tracepoint-counter-example\" deleted sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter/patch.yaml.env > config/default/go-xdp-counter/patch.yaml cd config/default/go-xdp-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter | kubectl delete --ignore-not-found=false -f - namespace \"go-xdp-counter\" deleted serviceaccount \"bpfman-app-go-xdp-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"bpfman-app-rolebinding-go-xdp-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"privileged-scc-xdp\" deleted daemonset.apps \"go-xdp-counter-ds\" deleted xdpprogram.bpfman.io \"go-xdp-counter-example\" deleted sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter-sharing-map/patch.yaml.env > config/default/go-xdp-counter-sharing-map/patch.yaml cd config/default/go-xdp-counter-sharing-map && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter-sharing-map | kubectl delete --ignore-not-found=false -f - xdpprogram.bpfman.io \"go-xdp-counter-sharing-map-example\" deleted Individual examples can be loaded and unloaded as well, for example make deploy-xdp and make undeploy-xdp . To see the full set of available commands, run make help : make help Usage: make <target> make deploy TAG=v0.2.0 make deploy-xdp IMAGE_XDP_US=quay.io/user1/go-xdp-counter-userspace:test General help Display this help. Local Dependencies kustomize Download kustomize locally if necessary. Development fmt Run go fmt against code. verify Verify all the autogenerated code Build build Build all the userspace example code. generate Run `go generate` to build the bytecode for each of the examples. build-us-images Build all example userspace images build-bc-images Build bytecode example userspace images push-us-images Push all example userspace images push-bc-images Push all example userspace images load-us-images-kind Build and load all example userspace images into kind Deployment Variables (not commands) TAG Used to set all images to a fixed tag. Example: make deploy TAG=v0.2.0 IMAGE_TC_BC TC Bytecode image. Example: make deploy-tc IMAGE_TC_BC=quay.io/user1/go-tc-counter-bytecode:test IMAGE_TC_US TC Userspace image. Example: make deploy-tc IMAGE_TC_US=quay.io/user1/go-tc-counter-userspace:test IMAGE_TP_BC Tracepoint Bytecode image. Example: make deploy-tracepoint IMAGE_TP_BC=quay.io/user1/go-tracepoint-counter-bytecode:test IMAGE_TP_US Tracepoint Userspace image. Example: make deploy-tracepoint IMAGE_TP_US=quay.io/user1/go-tracepoint-counter-userspace:test IMAGE_XDP_BC XDP Bytecode image. Example: make deploy-xdp IMAGE_XDP_BC=quay.io/user1/go-xdp-counter-bytecode:test IMAGE_XDP_US XDP Userspace image. Example: make deploy-xdp IMAGE_XDP_US=quay.io/user1/go-xdp-counter-userspace:test KIND_CLUSTER_NAME Name of the deployed cluster to load example images to, defaults to `bpfman-deployment` ignore-not-found For any undeploy command, set to true to ignore resource not found errors during deletion. Example: make undeploy ignore-not-found=true Deployment deploy-tc Deploy go-tc-counter to the cluster specified in ~/.kube/config. undeploy-tc Undeploy go-tc-counter from the cluster specified in ~/.kube/config. deploy-tracepoint Deploy go-tracepoint-counter to the cluster specified in ~/.kube/config. undeploy-tracepoint Undeploy go-tracepoint-counter from the cluster specified in ~/.kube/config. deploy-xdp Deploy go-xdp-counter to the cluster specified in ~/.kube/config. undeploy-xdp Undeploy go-xdp-counter from the cluster specified in ~/.kube/config. deploy-xdp-ms Deploy go-xdp-counter-sharing-map (shares map with go-xdp-counter) to the cluster specified in ~/.kube/config. undeploy-xdp-ms Undeploy go-xdp-counter-sharing-map from the cluster specified in ~/.kube/config. deploy Deploy all examples to the cluster specified in ~/.kube/config. undeploy Undeploy all examples to the cluster specified in ~/.kube/config. Building A Userspace Container Image To build the userspace examples in a container instead of using the pre-built ones, from the bpfman code source directory ( quay.io/bpfman-userspace/ ), run the following build commands: cd bpfman/examples make IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\ IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\ IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\ build-us-images Then EITHER push images to a remote repository: docker login quay.io cd bpfman/examples make IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\ IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\ IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\ push-us-images OR load the images directly to a specified kind cluster: cd bpfman/examples make IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\ IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\ IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\ KIND_CLUSTER_NAME=bpfman-deployment \\ load-us-images-kind Lastly, update the yaml to use the private images or override the yaml files using the Makefile: cd bpfman/examples/ make deploy-xdp IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest make undeploy-xdp make deploy-tc IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest make undeploy-tc make deploy-tracepoint IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest make undeploy-tracepoint","title":"Deploying Example eBPF Programs On Kubernetes"},{"location":"getting-started/example-bpf-k8s/#deploying-example-ebpf-programs-on-kubernetes","text":"This section will describe loading bytecode on a Kubernetes cluster and launching the userspace program. The approach is slightly different when running on a Kubernetes cluster. The eBPF bytecode should be loaded by an administrator, not the userspace program itself. This section assumes there is already a Kubernetes cluster running and bpfman is running in the cluster. See Deploying the bpfman-operator for details on deploying bpfman on a Kubernetes cluster, but the quickest solution is to run a Kubernetes KIND Cluster: cd bpfman/bpfman-operator/ make run-on-kind","title":"Deploying Example eBPF Programs On Kubernetes"},{"location":"getting-started/example-bpf-k8s/#loading-ebpf-bytecode-on-kubernetes","text":"Instead of using the userspace program or CLI to load the eBPF bytecode as done in previous sections, the bytecode will be loaded by creating a Kubernetes CRD object. There is a CRD object for each eBPF program type bpfman supports. Edit the sample yaml files to customize any configuration values: TcProgram CRD: go-tc-counter/bytecode.yaml TracepointProgram CRD: go-tracepoint-counter/bytecode.yaml XdpProgram CRD: go-xdp-counter/bytecode.yaml KprobeProgram CRD: bpfman-operator/config/samples/bpfman.io_v1alpha1_kprobe_kprobeprogram.yaml UprobeProgram CRD: bpfman-operator/config/samples/bpfman.io_v1alpha1_uprobe_uprobeprogram.yaml Sample bytecode yaml with XdpProgram CRD: cat examples/config/base/go-xdp-counter/bytecode.yaml apiVersion: bpfman.io/v1alpha1 kind: XdpProgram metadata: labels: app.kubernetes.io/name: xdpprogram name: go-xdp-counter-example spec: name: xdp_stats # Select all nodes nodeselector: {} interfaceselector: primarynodeinterface: true priority: 55 bytecode: image: url: quay.io/bpfman-bytecode/go-xdp-counter:latest Note that all the sample yaml files are configured with the bytecode running on all nodes ( nodeselector: {} ). This can be change to run on specific nodes, but the DaemonSet yaml for the userspace program, which is described below, should have an equivalent change. Make any changes to the go-xdp-counter-bytecode.yaml , then repeat for go-tc-counter-bytecode.yaml and go-tracepoint-counter-bytecode.yaml and then apply the updated yamls: kubectl apply -f examples/config/base/go-xdp-counter/bytecode.yaml xdpprogram.bpfman.io/go-xdp-counter-example created kubectl apply -f examples/config/base/go-tc-counter/bytecode.yaml tcprogram.bpfman.io/go-tc-counter-example created kubectl apply -f examples/config/base/go-tracepoint-counter/bytecode.yaml tracepointprogram.bpfman.io/go-tracepoint-counter-example created Following the diagram for XDP example (Blue numbers): The user creates a XdpProgram object with the parameters associated with the eBPF bytecode, like interface, priority and BFP bytecode image. The name of the XdpProgram object in this example is go-xdp-counter-example . The XdpProgram is applied using kubectl , but in a more practical deployment, the XdpProgram would be applied by the application or a controller. bpfman-agent , running on each node, is watching for all changes to XdpProgram objects. When it sees a XdpProgram object created or modified, it makes sure a BpfProgram object for that node exists. The name of the BpfProgram object is the XdpProgram object name with the node name and interface or attach point appended. bpfman-agent then determines if it should be running on the given node, loads or unloads as needed by making gRPC calls the bpfman . bpfman behaves the same as described in the running locally example. bpfman-agent finally updates the status of the BpfProgram object. bpfman-operator watches all BpfProgram objects, and updates the status of the XdpProgram object indicating if the eBPF program has been applied to all the desired nodes or not. To retrieve information on the XdpProgram objects: kubectl get xdpprograms NAME PRIORITY DIRECTION go-xdp-counter-example 55 kubectl get xdpprograms go-xdp-counter-example -o yaml apiVersion: bpfman.io/v1alpha1 kind: XdpProgram metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"bpfman.io/v1alpha1\",\"kind\":\"XdpProgram\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/name\":\"xdpprogram\"},\"name\":\"go-xdp-counter-example\"},\"spec\":{\"bpffunctionname\":\"xdp_stats\",\"bytecode\":{\"image\":{\"url\":\"quay.io/bpfman-bytecode/go-xdp-counter:latest\"}},\"interfaceselector\":{\"primarynodeinterface\":true},\"nodeselector\":{},\"priority\":55}} creationTimestamp: \"2023-11-06T21:05:15Z\" finalizers: - bpfman.io.operator/finalizer generation: 2 labels: app.kubernetes.io/name: xdpprogram name: go-xdp-counter-example resourceVersion: \"3103\" uid: edd45e2e-a40b-4668-ac76-c1f1eb63a23b spec: bpffunctionname: xdp_stats bytecode: image: imagepullpolicy: IfNotPresent url: quay.io/bpfman-bytecode/go-xdp-counter:latest interfaceselector: primarynodeinterface: true mapownerselector: {} nodeselector: {} priority: 55 proceedon: - pass - dispatcher_return status: conditions: - lastTransitionTime: \"2023-11-06T21:05:21Z\" message: bpfProgramReconciliation Succeeded on all nodes reason: ReconcileSuccess status: \"True\" type: ReconcileSuccess To retrieve information on the BpfProgram objects: kubectl get bpfprograms NAME AGE : 4822-bpfman-deployment-control-plane 60m 4825-bpfman-deployment-control-plane 60m go-tc-counter-example-bpfman-deployment-control-plane-eth0 61m go-tracepoint-counter-example-bpfman-deployment-control-plane-syscalls-sys-enter-kill 61m go-xdp-counter-example-bpfman-deployment-control-plane-eth0 61m go-xdp-counter-sharing-map-example-bpfman-deployment-control-plane-eth0 60m tc-dispatcher-4805-bpfman-deployment-control-plane 60m xdp-dispatcher-4816-bpfman-deployment-control-plane 60m kubectl get go-xdp-counter-example-bpfman-deployment-control-plane-eth0 -o yaml apiVersion: bpfman.io/v1alpha1 kind: BpfProgram metadata: annotations: bpfman.io.xdpprogramcontroller/interface: eth0 bpfman.io/ProgramId: \"4801\" creationTimestamp: \"2023-11-06T21:05:15Z\" finalizers: - bpfman.io.xdpprogramcontroller/finalizer generation: 1 labels: bpfman.io/ownedByProgram: go-xdp-counter-example kubernetes.io/hostname: bpfman-deployment-control-plane name: go-xdp-counter-example-bpfman-deployment-control-plane-eth0 ownerReferences: - apiVersion: bpfman.io/v1alpha1 blockOwnerDeletion: true controller: true kind: XdpProgram name: go-xdp-counter-example uid: edd45e2e-a40b-4668-ac76-c1f1eb63a23b resourceVersion: \"3102\" uid: f7ffd156-168b-4dc8-be38-18c42626a631 spec: type: xdp status: conditions: - lastTransitionTime: \"2023-11-06T21:05:21Z\" message: Successfully loaded bpfProgram reason: bpfmanLoaded status: \"True\" type: Loaded","title":"Loading eBPF Bytecode On Kubernetes"},{"location":"getting-started/example-bpf-k8s/#loading-userspace-container-on-kubernetes","text":"Here, a userspace container is deployed to consume the map data generated by the eBPF counter program. bpfman provides a Container Storage Interface (CSI) driver for exposing eBPF maps into a userspace container. To avoid having to mount a host directory that contains the map pinned file into the container and forcing the container to have permissions to access that host directory, the CSI driver mounts the map at a specified location in the container. All the examples use CSI, here is go-xdp-counter/deployment.yaml for reference: cd bpfman/examples/ cat config/base/go-xdp-counter/deployment.yaml : --- apiVersion: apps/v1 kind: DaemonSet metadata: name: go-xdp-counter-ds namespace: go-xdp-counter labels: k8s-app: go-xdp-counter spec: : template: : spec: : containers: - name: go-xdp-counter : volumeMounts: - name: go-xdp-counter-maps <==== 2) VolumeMount in container mountPath: /run/xdp/maps <==== 2a) Mount path in the container readOnly: true volumes: - name: go-xdp-counter-maps <==== 1) Volume describing the map csi: driver: csi.bpfman.io <==== 1a) bpfman CSI Driver volumeAttributes: csi.bpfman.io/program: go-xdp-counter-example <==== 1b) eBPF Program owning the map csi.bpfman.io/maps: xdp_stats_map <==== 1c) Map to be exposed to the container","title":"Loading Userspace Container On Kubernetes"},{"location":"getting-started/example-bpf-k8s/#loading-a-userspace-container-image","text":"The userspace programs have been pre-built and can be found here: quay.io/bpfman-userspace/go-tc-counter:latest quay.io/bpfman-userspace/go-tracepoint-counter:latest quay.io/bpfman-userspace/go-xdp-counter:latest The example yaml files below are loading from these image. go-tc-counter/deployment.yaml go-tracepoint-counter/deployment.yaml go-xdp-counter/deployment.yaml The userspace program in a Kubernetes Deployment doesn't interacts directly with bpfman like it did in the local host deployment. Instead, the userspace program running on each node, if needed, reads the BpfProgram object from the KubeApiServer to gather additional information about the loaded eBPF program. To interact with the KubeApiServer, RBAC must be setup properly to access the BpfProgram object. The bpfman-operator defined the yaml for several ClusterRoles that can be used to access the different bpfman related CRD objects with different access rights. The example userspace containers will use the bpfprogram-viewer-role , which allows Read-Only access to the BpfProgram object. This ClusterRole is created automatically by the bpfman-operator . The remaining objects (NameSpace, ServiceAccount, ClusterRoleBinding and examples DaemonSet) can be created for each program type as follows: cd bpfman/ kubectl create -f examples/config/base/go-xdp-counter/deployment.yaml kubectl create -f examples/config/base/go-tc-counter/deployment.yaml kubectl create -f examples/config/base/go-tracepoint-counter/deployment.yaml Following the diagram for the XDP example (Green numbers): The userspace program queries the KubeApiServer for a specific BpfProgram object. The userspace program verifies the BpfProgram has been loaded and uses the map to periodically read the counter values. To see if the userspace programs are working, view the logs: NAMESPACE NAME READY STATUS RESTARTS AGE bpfman bpfman-daemon-jsgdh 3/3 Running 0 11m bpfman bpfman-operator-6c5c8887f7-qk28x 2/2 Running 0 12m go-tc-counter go-tc-counter-ds-9jv4g 1/1 Running 0 5m37s go-tracepoint-counter go-tracepoint-counter-ds-2gzbt 1/1 Running 0 5m35s go-xdp-counter go-xdp-counter-ds-2hs6g 1/1 Running 0 6m12s : kubectl logs -n go-xdp-counter go-xdp-counter-ds-2hs6g 2023/11/06 20:27:16 2429 packets received 2023/11/06 20:27:16 1328474 bytes received 2023/11/06 20:27:19 2429 packets received 2023/11/06 20:27:19 1328474 bytes received 2023/11/06 20:27:22 2430 packets received 2023/11/06 20:27:22 1328552 bytes received : To cleanup: kubectl delete -f examples/config/base/go-xdp-counter/deployment.yaml kubectl delete -f examples/config/base/go-xdp-counter/bytecode.yaml kubectl delete -f examples/config/base/go-tc-counter/deployment.yaml kubectl delete -f examples/config/base/go-tc-counter/bytecode.yaml kubectl delete -f examples/config/base/go-tracepoint-counter/deployment.yaml kubectl delete -f examples/config/base/go-tracepoint-counter/bytecode.yaml","title":"Loading A Userspace Container Image"},{"location":"getting-started/example-bpf-k8s/#automated-deployment","text":"The steps above are automated in the Makefile in the examples directory. Run make deploy to load each of the example bytecode and userspace yaml files, then make undeploy to unload them. cd bpfman/examples/ make deploy sed 's@URL_BC@quay.io/bpfman-bytecode/go-tc-counter:latest@' config/default/go-tc-counter/patch.yaml.env > config/default/go-tc-counter/patch.yaml cd config/default/go-tc-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tc-counter=quay.io/bpfman-userspace/go-tc-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tc-counter | kubectl apply -f - namespace/go-tc-counter created serviceaccount/bpfman-app-go-tc-counter created clusterrolebinding.rbac.authorization.k8s.io/bpfman-app-rolebinding-go-tc-counter created clusterrolebinding.rbac.authorization.k8s.io/privileged-scc-tc created daemonset.apps/go-tc-counter-ds created tcprogram.bpfman.io/go-tc-counter-example created sed 's@URL_BC@quay.io/bpfman-bytecode/go-tracepoint-counter:latest@' config/default/go-tracepoint-counter/patch.yaml.env > config/default/go-tracepoint-counter/patch.yaml cd config/default/go-tracepoint-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tracepoint-counter=quay.io/bpfman-userspace/go-tracepoint-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tracepoint-counter | kubectl apply -f - namespace/go-tracepoint-counter created serviceaccount/bpfman-app-go-tracepoint-counter created clusterrolebinding.rbac.authorization.k8s.io/bpfman-app-rolebinding-go-tracepoint-counter created clusterrolebinding.rbac.authorization.k8s.io/privileged-scc-tracepoint created daemonset.apps/go-tracepoint-counter-ds created tracepointprogram.bpfman.io/go-tracepoint-counter-example created sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter/patch.yaml.env > config/default/go-xdp-counter/patch.yaml cd config/default/go-xdp-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter | kubectl apply -f - namespace/go-xdp-counter unchanged serviceaccount/bpfman-app-go-xdp-counter unchanged clusterrolebinding.rbac.authorization.k8s.io/bpfman-app-rolebinding-go-xdp-counter unchanged clusterrolebinding.rbac.authorization.k8s.io/privileged-scc-xdp unchanged daemonset.apps/go-xdp-counter-ds configured xdpprogram.bpfman.io/go-xdp-counter-example unchanged sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter-sharing-map/patch.yaml.env > config/default/go-xdp-counter-sharing-map/patch.yaml cd config/default/go-xdp-counter-sharing-map && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter-sharing-map | kubectl apply -f - xdpprogram.bpfman.io/go-xdp-counter-sharing-map-example created # Test Away ... make undeploy sed 's@URL_BC@quay.io/bpfman-bytecode/go-tc-counter:latest@' config/default/go-tc-counter/patch.yaml.env > config/default/go-tc-counter/patch.yaml cd config/default/go-tc-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tc-counter=quay.io/bpfman-userspace/go-tc-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tc-counter | kubectl delete --ignore-not-found=false -f - namespace \"go-tc-counter\" deleted serviceaccount \"bpfman-app-go-tc-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"bpfman-app-rolebinding-go-tc-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"privileged-scc-tc\" deleted daemonset.apps \"go-tc-counter-ds\" deleted tcprogram.bpfman.io \"go-tc-counter-example\" deleted sed 's@URL_BC@quay.io/bpfman-bytecode/go-tracepoint-counter:latest@' config/default/go-tracepoint-counter/patch.yaml.env > config/default/go-tracepoint-counter/patch.yaml cd config/default/go-tracepoint-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-tracepoint-counter=quay.io/bpfman-userspace/go-tracepoint-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-tracepoint-counter | kubectl delete --ignore-not-found=false -f - namespace \"go-tracepoint-counter\" deleted serviceaccount \"bpfman-app-go-tracepoint-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"bpfman-app-rolebinding-go-tracepoint-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"privileged-scc-tracepoint\" deleted daemonset.apps \"go-tracepoint-counter-ds\" deleted tracepointprogram.bpfman.io \"go-tracepoint-counter-example\" deleted sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter/patch.yaml.env > config/default/go-xdp-counter/patch.yaml cd config/default/go-xdp-counter && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter | kubectl delete --ignore-not-found=false -f - namespace \"go-xdp-counter\" deleted serviceaccount \"bpfman-app-go-xdp-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"bpfman-app-rolebinding-go-xdp-counter\" deleted clusterrolebinding.rbac.authorization.k8s.io \"privileged-scc-xdp\" deleted daemonset.apps \"go-xdp-counter-ds\" deleted xdpprogram.bpfman.io \"go-xdp-counter-example\" deleted sed 's@URL_BC@quay.io/bpfman-bytecode/go-xdp-counter:latest@' config/default/go-xdp-counter-sharing-map/patch.yaml.env > config/default/go-xdp-counter-sharing-map/patch.yaml cd config/default/go-xdp-counter-sharing-map && /home/bmcfall/src/bpfman/examples/bin/kustomize edit set image quay.io/bpfman-userspace/go-xdp-counter=quay.io/bpfman-userspace/go-xdp-counter:latest /home/bmcfall/src/bpfman/examples/bin/kustomize build config/default/go-xdp-counter-sharing-map | kubectl delete --ignore-not-found=false -f - xdpprogram.bpfman.io \"go-xdp-counter-sharing-map-example\" deleted Individual examples can be loaded and unloaded as well, for example make deploy-xdp and make undeploy-xdp . To see the full set of available commands, run make help : make help Usage: make <target> make deploy TAG=v0.2.0 make deploy-xdp IMAGE_XDP_US=quay.io/user1/go-xdp-counter-userspace:test General help Display this help. Local Dependencies kustomize Download kustomize locally if necessary. Development fmt Run go fmt against code. verify Verify all the autogenerated code Build build Build all the userspace example code. generate Run `go generate` to build the bytecode for each of the examples. build-us-images Build all example userspace images build-bc-images Build bytecode example userspace images push-us-images Push all example userspace images push-bc-images Push all example userspace images load-us-images-kind Build and load all example userspace images into kind Deployment Variables (not commands) TAG Used to set all images to a fixed tag. Example: make deploy TAG=v0.2.0 IMAGE_TC_BC TC Bytecode image. Example: make deploy-tc IMAGE_TC_BC=quay.io/user1/go-tc-counter-bytecode:test IMAGE_TC_US TC Userspace image. Example: make deploy-tc IMAGE_TC_US=quay.io/user1/go-tc-counter-userspace:test IMAGE_TP_BC Tracepoint Bytecode image. Example: make deploy-tracepoint IMAGE_TP_BC=quay.io/user1/go-tracepoint-counter-bytecode:test IMAGE_TP_US Tracepoint Userspace image. Example: make deploy-tracepoint IMAGE_TP_US=quay.io/user1/go-tracepoint-counter-userspace:test IMAGE_XDP_BC XDP Bytecode image. Example: make deploy-xdp IMAGE_XDP_BC=quay.io/user1/go-xdp-counter-bytecode:test IMAGE_XDP_US XDP Userspace image. Example: make deploy-xdp IMAGE_XDP_US=quay.io/user1/go-xdp-counter-userspace:test KIND_CLUSTER_NAME Name of the deployed cluster to load example images to, defaults to `bpfman-deployment` ignore-not-found For any undeploy command, set to true to ignore resource not found errors during deletion. Example: make undeploy ignore-not-found=true Deployment deploy-tc Deploy go-tc-counter to the cluster specified in ~/.kube/config. undeploy-tc Undeploy go-tc-counter from the cluster specified in ~/.kube/config. deploy-tracepoint Deploy go-tracepoint-counter to the cluster specified in ~/.kube/config. undeploy-tracepoint Undeploy go-tracepoint-counter from the cluster specified in ~/.kube/config. deploy-xdp Deploy go-xdp-counter to the cluster specified in ~/.kube/config. undeploy-xdp Undeploy go-xdp-counter from the cluster specified in ~/.kube/config. deploy-xdp-ms Deploy go-xdp-counter-sharing-map (shares map with go-xdp-counter) to the cluster specified in ~/.kube/config. undeploy-xdp-ms Undeploy go-xdp-counter-sharing-map from the cluster specified in ~/.kube/config. deploy Deploy all examples to the cluster specified in ~/.kube/config. undeploy Undeploy all examples to the cluster specified in ~/.kube/config.","title":"Automated Deployment"},{"location":"getting-started/example-bpf-k8s/#building-a-userspace-container-image","text":"To build the userspace examples in a container instead of using the pre-built ones, from the bpfman code source directory ( quay.io/bpfman-userspace/ ), run the following build commands: cd bpfman/examples make IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\ IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\ IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\ build-us-images Then EITHER push images to a remote repository: docker login quay.io cd bpfman/examples make IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\ IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\ IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\ push-us-images OR load the images directly to a specified kind cluster: cd bpfman/examples make IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest \\ IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest \\ IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest \\ KIND_CLUSTER_NAME=bpfman-deployment \\ load-us-images-kind Lastly, update the yaml to use the private images or override the yaml files using the Makefile: cd bpfman/examples/ make deploy-xdp IMAGE_XDP_US=quay.io/$USER/go-xdp-counter:latest make undeploy-xdp make deploy-tc IMAGE_TC_US=quay.io/$USER/go-tc-counter:latest make undeploy-tc make deploy-tracepoint IMAGE_TP_US=quay.io/$USER/go-tracepoint-counter:latest make undeploy-tracepoint","title":"Building A Userspace Container Image"},{"location":"getting-started/example-bpf-local/","text":"Deploying Example eBPF Programs On Local Host This section describes running bpfman and the example eBPF programs on a local host. When running bpfman, it can be run as a process or run as a systemd service. Examples run the same, independent of how bpfman is deployed. Building To build directly on a system, make sure all the prerequisites are met, then build. Prerequisites This assumes bpfman is already installed and running on the system. If not, see Setup and Building bpfman . All requirements defined by the cilium/ebpf package libbpf development package to get the required eBPF c headers Fedora: sudo dnf install libbpf-devel Ubuntu: sudo apt-get install libbpf-dev Cilium's bpf2go binary go install github.com/cilium/ebpf/cmd/bpf2go@master Building Locally To build all the C based eBPF counter bytecode, run: cd bpfman/examples/ make generate To build all the Userspace GO Client examples, run: cd bpfman/examples/ make build To build only a single example: cd bpfman/examples/go-tc-counter/ go generate go build cd bpfman/examples/go-tracepoint-counter/ go generate go build cd bpfman/examples/go-xdp-counter/ go generate go build Running On Host The most basic way to deploy this example is running directly on a host system. First, start or ensure bpfman is up and running. Tutorial will guide you through deploying bpfman . In all the examples of running on a host system, a bpfman-client certificate is used that is generated by bpfman to encrypt the application's connection to bpfman . The diagram below shows go-xdp-counter example, but the go-tc-counter and go-tracepoint-counter examples operate exactly the same way. Following the diagram (Purple numbers): When go-xdp-counter userspace is started, it will send a gRPC request over unix socket to bpfman requesting bpfman to load the go-xdp-counter eBPF bytecode located on disk at bpfman/examples/go-xdp-counter/bpf_bpfel.o at a priority of 50 and on interface ens3 . These values are configurable as we will see later, but for now we will use the defaults (except interface, which is required to be entered). bpfman will load it's dispatcher eBPF program, which links to the go-xdp-counter eBPF program and return a UUID referencing the running program. bpfman list can be used to show that the eBPF program was loaded. Once the go-xdp-counter eBPF bytecode is loaded, the eBPF program will write packet counts and byte counts to a shared map. go-xdp-counter userspace program periodically reads counters from the shared map and logs the value. Running Privileged To run the go-xdp-counter program, determine the host interface to attach the eBPF program to and then start the go program with: cd bpfman/examples/go-xdp-counter/ sudo ./go-xdp-counter -iface <INTERNET INTERFACE NAME> or ( NOTE: TC programs also require a direction, ingress or egress) cd bpfman/examples/go-tc-counter/ sudo ./go-tc-counter -direction ingress -iface <INTERNET INTERFACE NAME> or cd bpfman/examples/go-tracepoint-counter/ sudo ./go-tracepoint-counter The output should show the count and total bytes of packets as they pass through the interface as shown below: sudo ./go-xdp-counter --iface vethff657c7 2023/07/17 17:43:58 Using Input: Interface=vethff657c7 Priority=50 Source=/home/<$USER>/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o 2023/07/17 17:43:58 Program registered with id 6211 2023/07/17 17:44:01 4 packets received 2023/07/17 17:44:01 580 bytes received 2023/07/17 17:44:04 4 packets received 2023/07/17 17:44:04 580 bytes received 2023/07/17 17:44:07 8 packets received 2023/07/17 17:44:07 1160 bytes received : Use the CLI to show the go-xdp-counter eBPF bytecode was loaded. sudo bpfman list Program ID Name Type Load Time 6211 xdp_stats xdp 2023-07-17T17:43:58-0400 Finally, press <CTRL>+c when finished with go-xdp-counter . : 2023/07/17 17:44:34 28 packets received 2023/07/17 17:44:34 4060 bytes received ^C2023/07/17 17:44:35 Exiting... 2023/07/17 17:44:35 Unloading Program: 6211 Passing eBPF Bytecode In A Container Image bpfman can load eBPF bytecode from a container image built following the spec described in eBPF Bytecode Image Specifications . Pre-built eBPF container images for the examples can be loaded from: quay.io/bpfman-bytecode/go-xdp-counter:latest quay.io/bpfman-bytecode/go-tc-counter:latest quay.io/bpfman-bytecode/go-tracepoint-counter:latest To use the container image, pass the URL to the userspace program: sudo ./go-xdp-counter -iface ens3 -image quay.io/bpfman-bytecode/go-xdp-counter:latest 2022/12/02 16:28:32 Using Input: Interface=ens3 Priority=50 Source=quay.io/bpfman-bytecode/go-xdp-counter:latest 2022/12/02 16:28:34 Program registered with id 6223 2022/12/02 16:28:37 4 packets received 2022/12/02 16:28:37 580 bytes received 2022/12/02 16:28:40 4 packets received 2022/12/02 16:28:40 580 bytes received ^C2022/12/02 16:28:42 Exiting... 2022/12/02 16:28:42 Unloading Program: 6223 Building eBPF Bytecode Container Image eBPF Bytecode Image Specifications provides detailed instructions on building and shipping bytecode in a container image. To build go-xdp-counter and go-tc-counter eBPF bytecode container image, first make sure the bytecode has been built (i.e. bpf_bpfel.o has been built - see Building ), then run the build commands below: cd bpfman/examples/go-xdp-counter/ go generate docker build \\ --build-arg PROGRAM_NAME=go-xdp-counter \\ --build-arg BPF_FUNCTION_NAME=xdp_stats \\ --build-arg PROGRAM_TYPE=xdp \\ --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\ --build-arg KERNEL_COMPILE_VER=$(uname -r) \\ -f ../../Containerfile.bytecode . -t quay.io/$USER/go-xdp-counter-bytecode:latest and cd bpfman/examples/go-tc-counter/ go generate docker build \\ --build-arg PROGRAM_NAME=go-tc-counter \\ --build-arg BPF_FUNCTION_NAME=stats \\ --build-arg PROGRAM_TYPE=tc \\ --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\ --build-arg KERNEL_COMPILE_VER=$(uname -r) \\ -f ../../Containerfile.bytecode . -t quay.io/$USER/go-tc-counter-bytecode:latest and cd bpfman/examples/go-tracepoint-counter/ go generate docker build \\ --build-arg PROGRAM_NAME=go-tracepoint-counter \\ --build-arg BPF_FUNCTION_NAME=tracepoint_kill_recorder \\ --build-arg PROGRAM_TYPE=tracepoint \\ --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\ --build-arg KERNEL_COMPILE_VER=$(uname -r) \\ -f ../../Containerfile.bytecode . -t quay.io/$USER/go-tracepoint-counter-bytecode:latest bpfman currently does not provide a method for pre-loading bytecode images (see issue #603 ), so push the bytecode image to a remote repository. For example: docker login quay.io docker push quay.io/$USER/go-xdp-counter-bytecode:latest docker push quay.io/$USER/go-tc-counter-bytecode:latest Then run with the privately built bytecode container image: sudo ./go-tc-counter -iface ens3 -direction ingress -location image://quay.io/$USER/go-tc-counter-bytecode:latest 2022/12/02 16:38:44 Using Input: Interface=ens3 Priority=50 Source=quay.io/$USER/go-tc-counter-bytecode:latest 2022/12/02 16:38:45 Program registered with id 6225 2022/12/02 16:38:48 4 packets received 2022/12/02 16:38:48 580 bytes received 2022/12/02 16:38:51 4 packets received 2022/12/02 16:38:51 580 bytes received ^C2022/12/02 16:38:51 Exiting... 2022/12/02 16:38:51 Unloading Program: 6225 Preloading eBPF Bytecode Another way to load the eBPF bytecode is to pre-load the eBPF bytecode and pass the associated bpfman program id to the userspace program. This is similar to how eBPF programs will be loaded in Kubernetes, except kubectl commands will be used to create Kubernetes CRD objects instead of using the CLI, but that is covered in the next section. The userspace programs will skip the loading portion and use the program id to find the shared map and continue from there. Referring back to the diagram above, the load and unload are being done by the CLI and not go-xdp-counter userspace program. First, use the CLI to load the go-xdp-counter eBPF bytecode: sudo bpfman load image --image-url quay.io/bpfman-bytecode/go-xdp-counter:latest xdp --iface ens3 --priority 50 Bpfman State --------------- Name: xdp_stats Image URL: quay.io/bpfman-bytecode/go-xdp-counter:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6229 Map Owner ID: None Map Used By: 6229 Priority: 50 Iface: ens3 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6229 Name: xdp_stats Type: xdp Loaded At: 2023-07-17T17:48:10-0400 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [2724] BTF ID: 2834 Size Translated (bytes): 168 JITed: true Size JITed (bytes): 104 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 21 Then run the go-xdp-counter userspace program, passing in the UUID: sudo ./go-xdp-counter -iface ens3 -id 6229 2022/12/02 17:01:38 Using Input: Interface=ens3 Source=6229 2022/12/02 17:01:41 180 packets received 2022/12/02 17:01:41 26100 bytes received 2022/12/02 17:01:44 184 packets received 2022/12/02 17:01:44 26680 bytes received ^C2022/12/02 17:01:46 Exiting... 2022/12/02 17:01:46 Closing Connection for Program: 6229 Then use the CLI to unload the eBPF bytecode: sudo bpfman unload 6229","title":"Deploying Example eBPF Programs On Local Host"},{"location":"getting-started/example-bpf-local/#deploying-example-ebpf-programs-on-local-host","text":"This section describes running bpfman and the example eBPF programs on a local host. When running bpfman, it can be run as a process or run as a systemd service. Examples run the same, independent of how bpfman is deployed.","title":"Deploying Example eBPF Programs On Local Host"},{"location":"getting-started/example-bpf-local/#building","text":"To build directly on a system, make sure all the prerequisites are met, then build.","title":"Building"},{"location":"getting-started/example-bpf-local/#prerequisites","text":"This assumes bpfman is already installed and running on the system. If not, see Setup and Building bpfman . All requirements defined by the cilium/ebpf package libbpf development package to get the required eBPF c headers Fedora: sudo dnf install libbpf-devel Ubuntu: sudo apt-get install libbpf-dev Cilium's bpf2go binary go install github.com/cilium/ebpf/cmd/bpf2go@master","title":"Prerequisites"},{"location":"getting-started/example-bpf-local/#building-locally","text":"To build all the C based eBPF counter bytecode, run: cd bpfman/examples/ make generate To build all the Userspace GO Client examples, run: cd bpfman/examples/ make build To build only a single example: cd bpfman/examples/go-tc-counter/ go generate go build cd bpfman/examples/go-tracepoint-counter/ go generate go build cd bpfman/examples/go-xdp-counter/ go generate go build","title":"Building Locally"},{"location":"getting-started/example-bpf-local/#running-on-host","text":"The most basic way to deploy this example is running directly on a host system. First, start or ensure bpfman is up and running. Tutorial will guide you through deploying bpfman . In all the examples of running on a host system, a bpfman-client certificate is used that is generated by bpfman to encrypt the application's connection to bpfman . The diagram below shows go-xdp-counter example, but the go-tc-counter and go-tracepoint-counter examples operate exactly the same way. Following the diagram (Purple numbers): When go-xdp-counter userspace is started, it will send a gRPC request over unix socket to bpfman requesting bpfman to load the go-xdp-counter eBPF bytecode located on disk at bpfman/examples/go-xdp-counter/bpf_bpfel.o at a priority of 50 and on interface ens3 . These values are configurable as we will see later, but for now we will use the defaults (except interface, which is required to be entered). bpfman will load it's dispatcher eBPF program, which links to the go-xdp-counter eBPF program and return a UUID referencing the running program. bpfman list can be used to show that the eBPF program was loaded. Once the go-xdp-counter eBPF bytecode is loaded, the eBPF program will write packet counts and byte counts to a shared map. go-xdp-counter userspace program periodically reads counters from the shared map and logs the value.","title":"Running On Host"},{"location":"getting-started/example-bpf-local/#running-privileged","text":"To run the go-xdp-counter program, determine the host interface to attach the eBPF program to and then start the go program with: cd bpfman/examples/go-xdp-counter/ sudo ./go-xdp-counter -iface <INTERNET INTERFACE NAME> or ( NOTE: TC programs also require a direction, ingress or egress) cd bpfman/examples/go-tc-counter/ sudo ./go-tc-counter -direction ingress -iface <INTERNET INTERFACE NAME> or cd bpfman/examples/go-tracepoint-counter/ sudo ./go-tracepoint-counter The output should show the count and total bytes of packets as they pass through the interface as shown below: sudo ./go-xdp-counter --iface vethff657c7 2023/07/17 17:43:58 Using Input: Interface=vethff657c7 Priority=50 Source=/home/<$USER>/src/bpfman/examples/go-xdp-counter/bpf_bpfel.o 2023/07/17 17:43:58 Program registered with id 6211 2023/07/17 17:44:01 4 packets received 2023/07/17 17:44:01 580 bytes received 2023/07/17 17:44:04 4 packets received 2023/07/17 17:44:04 580 bytes received 2023/07/17 17:44:07 8 packets received 2023/07/17 17:44:07 1160 bytes received : Use the CLI to show the go-xdp-counter eBPF bytecode was loaded. sudo bpfman list Program ID Name Type Load Time 6211 xdp_stats xdp 2023-07-17T17:43:58-0400 Finally, press <CTRL>+c when finished with go-xdp-counter . : 2023/07/17 17:44:34 28 packets received 2023/07/17 17:44:34 4060 bytes received ^C2023/07/17 17:44:35 Exiting... 2023/07/17 17:44:35 Unloading Program: 6211","title":"Running Privileged"},{"location":"getting-started/example-bpf-local/#passing-ebpf-bytecode-in-a-container-image","text":"bpfman can load eBPF bytecode from a container image built following the spec described in eBPF Bytecode Image Specifications . Pre-built eBPF container images for the examples can be loaded from: quay.io/bpfman-bytecode/go-xdp-counter:latest quay.io/bpfman-bytecode/go-tc-counter:latest quay.io/bpfman-bytecode/go-tracepoint-counter:latest To use the container image, pass the URL to the userspace program: sudo ./go-xdp-counter -iface ens3 -image quay.io/bpfman-bytecode/go-xdp-counter:latest 2022/12/02 16:28:32 Using Input: Interface=ens3 Priority=50 Source=quay.io/bpfman-bytecode/go-xdp-counter:latest 2022/12/02 16:28:34 Program registered with id 6223 2022/12/02 16:28:37 4 packets received 2022/12/02 16:28:37 580 bytes received 2022/12/02 16:28:40 4 packets received 2022/12/02 16:28:40 580 bytes received ^C2022/12/02 16:28:42 Exiting... 2022/12/02 16:28:42 Unloading Program: 6223","title":"Passing eBPF Bytecode In A Container Image"},{"location":"getting-started/example-bpf-local/#building-ebpf-bytecode-container-image","text":"eBPF Bytecode Image Specifications provides detailed instructions on building and shipping bytecode in a container image. To build go-xdp-counter and go-tc-counter eBPF bytecode container image, first make sure the bytecode has been built (i.e. bpf_bpfel.o has been built - see Building ), then run the build commands below: cd bpfman/examples/go-xdp-counter/ go generate docker build \\ --build-arg PROGRAM_NAME=go-xdp-counter \\ --build-arg BPF_FUNCTION_NAME=xdp_stats \\ --build-arg PROGRAM_TYPE=xdp \\ --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\ --build-arg KERNEL_COMPILE_VER=$(uname -r) \\ -f ../../Containerfile.bytecode . -t quay.io/$USER/go-xdp-counter-bytecode:latest and cd bpfman/examples/go-tc-counter/ go generate docker build \\ --build-arg PROGRAM_NAME=go-tc-counter \\ --build-arg BPF_FUNCTION_NAME=stats \\ --build-arg PROGRAM_TYPE=tc \\ --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\ --build-arg KERNEL_COMPILE_VER=$(uname -r) \\ -f ../../Containerfile.bytecode . -t quay.io/$USER/go-tc-counter-bytecode:latest and cd bpfman/examples/go-tracepoint-counter/ go generate docker build \\ --build-arg PROGRAM_NAME=go-tracepoint-counter \\ --build-arg BPF_FUNCTION_NAME=tracepoint_kill_recorder \\ --build-arg PROGRAM_TYPE=tracepoint \\ --build-arg BYTECODE_FILENAME=bpf_bpfel.o \\ --build-arg KERNEL_COMPILE_VER=$(uname -r) \\ -f ../../Containerfile.bytecode . -t quay.io/$USER/go-tracepoint-counter-bytecode:latest bpfman currently does not provide a method for pre-loading bytecode images (see issue #603 ), so push the bytecode image to a remote repository. For example: docker login quay.io docker push quay.io/$USER/go-xdp-counter-bytecode:latest docker push quay.io/$USER/go-tc-counter-bytecode:latest Then run with the privately built bytecode container image: sudo ./go-tc-counter -iface ens3 -direction ingress -location image://quay.io/$USER/go-tc-counter-bytecode:latest 2022/12/02 16:38:44 Using Input: Interface=ens3 Priority=50 Source=quay.io/$USER/go-tc-counter-bytecode:latest 2022/12/02 16:38:45 Program registered with id 6225 2022/12/02 16:38:48 4 packets received 2022/12/02 16:38:48 580 bytes received 2022/12/02 16:38:51 4 packets received 2022/12/02 16:38:51 580 bytes received ^C2022/12/02 16:38:51 Exiting... 2022/12/02 16:38:51 Unloading Program: 6225","title":"Building eBPF Bytecode Container Image"},{"location":"getting-started/example-bpf-local/#preloading-ebpf-bytecode","text":"Another way to load the eBPF bytecode is to pre-load the eBPF bytecode and pass the associated bpfman program id to the userspace program. This is similar to how eBPF programs will be loaded in Kubernetes, except kubectl commands will be used to create Kubernetes CRD objects instead of using the CLI, but that is covered in the next section. The userspace programs will skip the loading portion and use the program id to find the shared map and continue from there. Referring back to the diagram above, the load and unload are being done by the CLI and not go-xdp-counter userspace program. First, use the CLI to load the go-xdp-counter eBPF bytecode: sudo bpfman load image --image-url quay.io/bpfman-bytecode/go-xdp-counter:latest xdp --iface ens3 --priority 50 Bpfman State --------------- Name: xdp_stats Image URL: quay.io/bpfman-bytecode/go-xdp-counter:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6229 Map Owner ID: None Map Used By: 6229 Priority: 50 Iface: ens3 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6229 Name: xdp_stats Type: xdp Loaded At: 2023-07-17T17:48:10-0400 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [2724] BTF ID: 2834 Size Translated (bytes): 168 JITed: true Size JITed (bytes): 104 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 21 Then run the go-xdp-counter userspace program, passing in the UUID: sudo ./go-xdp-counter -iface ens3 -id 6229 2022/12/02 17:01:38 Using Input: Interface=ens3 Source=6229 2022/12/02 17:01:41 180 packets received 2022/12/02 17:01:41 26100 bytes received 2022/12/02 17:01:44 184 packets received 2022/12/02 17:01:44 26680 bytes received ^C2022/12/02 17:01:46 Exiting... 2022/12/02 17:01:46 Closing Connection for Program: 6229 Then use the CLI to unload the eBPF bytecode: sudo bpfman unload 6229","title":"Preloading eBPF Bytecode"},{"location":"getting-started/example-bpf/","text":"Example eBPF Programs Example applications that use the bpfman-go bindings can be found in the examples/ directory. Current examples include: examples/go-tc-counter/ examples/go-tracepoint-counter/ examples/go-xdp-counter/ These examples and the associated documentation is intended to provide the basics on how to deploy and manage an eBPF program using bpfman. Each of the examples contain an eBPF Program written in C ( tc_counter.c , tracepoint_counter.c and xdp_counter.c ) that is compiled into eBPF bytecode. Each time the eBPF program is called, it increments the packet and byte counts in a map that is accessible by the userspace portion. Each of the examples also have a userspace portion written in GO. When run locally, the userspace program makes gRPC calls to bpfman requesting bpfman to load the eBPF program at the requested hook point (XDP hook point, TC hook point or Tracepoint). When run in a Kubernetes deployment, the bpfman-agent makes gRPC calls to bpfman requesting bpfman to load the eBPF program based on a Custom Resource Definition (CRD), which is described in more detail in that section. Independent of the deployment, the userspace program then polls the eBPF map every 3 seconds and logs the current counts. The userspace code is leveraging the cilium/ebpf library to manage the maps shared with the eBPF program. The example eBPF programs are very similar in functionality, and only vary where in the Linux networking stack they are inserted. Read more about XDP and TC programs here . There are two ways to deploy these example applications: Run locally on one machine: Deploying Example eBPF Programs On Local Host Deploy to multiple nodes in a Kubernetes cluster: Deploying Example eBPF Programs On Kubernetes Notes Notes regarding this document: Source of images used in the example documentation can be found in bpfman Upstream Images . Request access if required.","title":"Example eBPF Programs"},{"location":"getting-started/example-bpf/#example-ebpf-programs","text":"Example applications that use the bpfman-go bindings can be found in the examples/ directory. Current examples include: examples/go-tc-counter/ examples/go-tracepoint-counter/ examples/go-xdp-counter/ These examples and the associated documentation is intended to provide the basics on how to deploy and manage an eBPF program using bpfman. Each of the examples contain an eBPF Program written in C ( tc_counter.c , tracepoint_counter.c and xdp_counter.c ) that is compiled into eBPF bytecode. Each time the eBPF program is called, it increments the packet and byte counts in a map that is accessible by the userspace portion. Each of the examples also have a userspace portion written in GO. When run locally, the userspace program makes gRPC calls to bpfman requesting bpfman to load the eBPF program at the requested hook point (XDP hook point, TC hook point or Tracepoint). When run in a Kubernetes deployment, the bpfman-agent makes gRPC calls to bpfman requesting bpfman to load the eBPF program based on a Custom Resource Definition (CRD), which is described in more detail in that section. Independent of the deployment, the userspace program then polls the eBPF map every 3 seconds and logs the current counts. The userspace code is leveraging the cilium/ebpf library to manage the maps shared with the eBPF program. The example eBPF programs are very similar in functionality, and only vary where in the Linux networking stack they are inserted. Read more about XDP and TC programs here . There are two ways to deploy these example applications: Run locally on one machine: Deploying Example eBPF Programs On Local Host Deploy to multiple nodes in a Kubernetes cluster: Deploying Example eBPF Programs On Kubernetes","title":"Example eBPF Programs"},{"location":"getting-started/example-bpf/#notes","text":"Notes regarding this document: Source of images used in the example documentation can be found in bpfman Upstream Images . Request access if required.","title":"Notes"},{"location":"getting-started/running-release/","text":"Run bpfman From Release Image This section describes how to deploy bpfman from a given release. See Releases for the set of bpfman releases. Jump to the Setup and Building bpfman section for help building from the latest code or building from a release branch. Tutorial contains more details on the different modes to run bpfman in on the host and how to test. Use Local Host or Systemd Service below for deploying released version of bpfman and then use Tutorial for further information on how to test and interact with bpfman . Deploying the bpfman-operator contains more details on deploying bpfman in a Kubernetes deployment and Deploying Example eBPF Programs On Kubernetes contains more details on interacting with bpfman running in a Kubernetes deployment. Use Deploying Release Version of the bpfman-operator below for deploying released version of bpfman in Kubernetes and then use the links above for further information on how to test and interact with bpfman . NOTE: The latest release, v0.3.1, was before the rename of bpfd to bpfman . So the commands below still refer to bpfd. Local Host To run bpfd in the foreground using sudo , download the release binary tar files and unpack them. export BPFMAN_REL=0.3.1 mkdir -p $HOME/src/bpfman-${BPFMAN_REL}/; cd $HOME/src/bpfman-${BPFMAN_REL}/ wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-linux-x86_64.tar.gz tar -xzvf bpfd-linux-x86_64.tar.gz; rm bpfd-linux-x86_64.tar.gz wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfctl-linux-x86_64.tar.gz tar -xzvf bpfctl-linux-x86_64.tar.gz; rm bpfctl-linux-x86_64.tar.gz $ tree . \u2514\u2500\u2500 target \u2514\u2500\u2500 x86_64-unknown-linux-musl \u2514\u2500\u2500 release \u251c\u2500\u2500 bpfctl \u2514\u2500\u2500 bpfd To deploy bpfd : sudo RUST_LOG=info ./target/x86_64-unknown-linux-musl/release/bpfd [2023-10-13T15:53:25Z INFO bpfd] Log using env_logger [2023-10-13T15:53:25Z INFO bpfd] Has CAP_BPF: true [2023-10-13T15:53:25Z INFO bpfd] Has CAP_SYS_ADMIN: true : To use the CLI: sudo ./target/x86_64-unknown-linux-musl/release/bpfctl list Program ID Name Type Load Time Continue in Tutorial if desired. Use the bpfctl commands in place of the bpfman commands described in Tutorial . Systemd Service To run bpfd as a systemd service, the binaries will be placed in a well known location ( /usr/sbin/. ) and a service configuration file will be added ( /usr/lib/systemd/system/bpfd.service ). There is a script that is used to install the service properly, so the source code needs to be downloaded to retrieve the script. Download and unpack the source code, then download and unpack the binaries. export BPFMAN_REL=0.3.1 mkdir -p $HOME/src/; cd $HOME/src/ wget https://github.com/bpfman/bpfman/archive/refs/tags/v${BPFMAN_REL}.tar.gz tar -xzvf v${BPFMAN_REL}.tar.gz; rm v${BPFMAN_REL}.tar.gz cd bpfman-${BPFMAN_REL} wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-linux-x86_64.tar.gz tar -xzvf bpfd-linux-x86_64.tar.gz; rm bpfd-linux-x86_64.tar.gz wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfctl-linux-x86_64.tar.gz tar -xzvf bpfctl-linux-x86_64.tar.gz; rm bpfctl-linux-x86_64.tar.gz Run the following command to copy the bpfd and bpfctl binaries to /usr/sbin/ and copy a default bpfd.service file to /usr/lib/systemd/system/ . This option will also start the systemd service bpfd.service by default. sudo ./scripts/setup.sh install NOTE: If running a release older than v0.3.1 , the install script is not coded to copy binaries from the release directory, so the binaries will need to be manually copied. Continue in Tutorial if desired. Deploying Release Version of the bpfman-operator The quickest solution for running bpfman in a Kubernetes deployment is to run a Kubernetes KIND Cluster: kind create cluster --name=test-bpfman Next, deploy the bpfman CRDs: export BPFMAN_REL=0.3.1 kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-crds-install-v${BPFMAN_REL}.yaml Next, deploy the bpfman-operator , which will also deploy the bpfman-daemon , which contains bpfman and bpfman-agent : kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-operator-install-v${BPFMAN_REL}.yaml Finally, deploy an example eBPF program. kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/go-xdp-counter-install-v${BPFMAN_REL}.yaml There are other example programs in the Releases page. Continue in Deploying the bpfman-operator or Deploying Example eBPF Programs On Kubernetes if desired. Keep in mind that the documentation describes bpfman while Release v0.3.1 is still using bpfd . Use the following command to teardown the cluster: kind delete cluster -n test-bpfman","title":"Run bpfman From Release Image"},{"location":"getting-started/running-release/#run-bpfman-from-release-image","text":"This section describes how to deploy bpfman from a given release. See Releases for the set of bpfman releases. Jump to the Setup and Building bpfman section for help building from the latest code or building from a release branch. Tutorial contains more details on the different modes to run bpfman in on the host and how to test. Use Local Host or Systemd Service below for deploying released version of bpfman and then use Tutorial for further information on how to test and interact with bpfman . Deploying the bpfman-operator contains more details on deploying bpfman in a Kubernetes deployment and Deploying Example eBPF Programs On Kubernetes contains more details on interacting with bpfman running in a Kubernetes deployment. Use Deploying Release Version of the bpfman-operator below for deploying released version of bpfman in Kubernetes and then use the links above for further information on how to test and interact with bpfman . NOTE: The latest release, v0.3.1, was before the rename of bpfd to bpfman . So the commands below still refer to bpfd.","title":"Run bpfman From Release Image"},{"location":"getting-started/running-release/#local-host","text":"To run bpfd in the foreground using sudo , download the release binary tar files and unpack them. export BPFMAN_REL=0.3.1 mkdir -p $HOME/src/bpfman-${BPFMAN_REL}/; cd $HOME/src/bpfman-${BPFMAN_REL}/ wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-linux-x86_64.tar.gz tar -xzvf bpfd-linux-x86_64.tar.gz; rm bpfd-linux-x86_64.tar.gz wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfctl-linux-x86_64.tar.gz tar -xzvf bpfctl-linux-x86_64.tar.gz; rm bpfctl-linux-x86_64.tar.gz $ tree . \u2514\u2500\u2500 target \u2514\u2500\u2500 x86_64-unknown-linux-musl \u2514\u2500\u2500 release \u251c\u2500\u2500 bpfctl \u2514\u2500\u2500 bpfd To deploy bpfd : sudo RUST_LOG=info ./target/x86_64-unknown-linux-musl/release/bpfd [2023-10-13T15:53:25Z INFO bpfd] Log using env_logger [2023-10-13T15:53:25Z INFO bpfd] Has CAP_BPF: true [2023-10-13T15:53:25Z INFO bpfd] Has CAP_SYS_ADMIN: true : To use the CLI: sudo ./target/x86_64-unknown-linux-musl/release/bpfctl list Program ID Name Type Load Time Continue in Tutorial if desired. Use the bpfctl commands in place of the bpfman commands described in Tutorial .","title":"Local Host"},{"location":"getting-started/running-release/#systemd-service","text":"To run bpfd as a systemd service, the binaries will be placed in a well known location ( /usr/sbin/. ) and a service configuration file will be added ( /usr/lib/systemd/system/bpfd.service ). There is a script that is used to install the service properly, so the source code needs to be downloaded to retrieve the script. Download and unpack the source code, then download and unpack the binaries. export BPFMAN_REL=0.3.1 mkdir -p $HOME/src/; cd $HOME/src/ wget https://github.com/bpfman/bpfman/archive/refs/tags/v${BPFMAN_REL}.tar.gz tar -xzvf v${BPFMAN_REL}.tar.gz; rm v${BPFMAN_REL}.tar.gz cd bpfman-${BPFMAN_REL} wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-linux-x86_64.tar.gz tar -xzvf bpfd-linux-x86_64.tar.gz; rm bpfd-linux-x86_64.tar.gz wget https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfctl-linux-x86_64.tar.gz tar -xzvf bpfctl-linux-x86_64.tar.gz; rm bpfctl-linux-x86_64.tar.gz Run the following command to copy the bpfd and bpfctl binaries to /usr/sbin/ and copy a default bpfd.service file to /usr/lib/systemd/system/ . This option will also start the systemd service bpfd.service by default. sudo ./scripts/setup.sh install NOTE: If running a release older than v0.3.1 , the install script is not coded to copy binaries from the release directory, so the binaries will need to be manually copied. Continue in Tutorial if desired.","title":"Systemd Service"},{"location":"getting-started/running-release/#deploying-release-version-of-the-bpfman-operator","text":"The quickest solution for running bpfman in a Kubernetes deployment is to run a Kubernetes KIND Cluster: kind create cluster --name=test-bpfman Next, deploy the bpfman CRDs: export BPFMAN_REL=0.3.1 kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-crds-install-v${BPFMAN_REL}.yaml Next, deploy the bpfman-operator , which will also deploy the bpfman-daemon , which contains bpfman and bpfman-agent : kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/bpfd-operator-install-v${BPFMAN_REL}.yaml Finally, deploy an example eBPF program. kubectl apply -f https://github.com/bpfman/bpfman/releases/download/v${BPFMAN_REL}/go-xdp-counter-install-v${BPFMAN_REL}.yaml There are other example programs in the Releases page. Continue in Deploying the bpfman-operator or Deploying Example eBPF Programs On Kubernetes if desired. Keep in mind that the documentation describes bpfman while Release v0.3.1 is still using bpfd . Use the following command to teardown the cluster: kind delete cluster -n test-bpfman","title":"Deploying Release Version of the bpfman-operator"},{"location":"getting-started/troubleshooting/","text":"Troubleshooting This section provides a list of common issues and solutions when working with bpfman . XDP XDP Program Fails to Load When attempting to load an XDP program and the program fails to load: $ sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface veth92cd99b --priority 100 Error: status: Aborted, message: \"An error occurred. dispatcher attach failed on interface veth92cd99b: `bpf_link_create` failed\", details: [], metadata: MetadataMap { headers: {\"content-type\": \"application/grpc\", \"date\": \"Tue, 28 Nov 2023 13:37:02 GMT\", \"content-length\": \"0\"} } The log may look something like this: Nov 28 08:36:58 ebpf03 bpfman[2081732]: The bytecode image: quay.io/bpfman-bytecode/xdp_pass:latest is signed Nov 28 08:36:59 ebpf03 bpfman[2081732]: Loading program bytecode from container image: quay.io/bpfman-bytecode/xdp_pass:latest Nov 28 08:37:01 ebpf03 bpfman[2081732]: The bytecode image: quay.io/bpfman/xdp-dispatcher:v2 is signed Nov 28 08:37:02 ebpf03 bpfman[2081732]: BPFMAN load error: Error( \"dispatcher attach failed on interface veth92cd99b: `bpf_link_create` failed\", ) The issue may be the there is already an external XDP program loaded on the given interface. bpfman allows multiple XDP programs on an interface by loading a dispatcher program which is the XDP program and additional programs are loaded as extensions to the dispatcher . Use bpftool to determine if any programs are already loaded on an interface: $ sudo bpftool net list dev veth92cd99b xdp: veth92cd99b(32) generic id 8733 tc: veth92cd99b(32) clsact/ingress tc_dispatcher id 8922 flow_dissector:","title":"Troubleshooting"},{"location":"getting-started/troubleshooting/#troubleshooting","text":"This section provides a list of common issues and solutions when working with bpfman .","title":"Troubleshooting"},{"location":"getting-started/troubleshooting/#xdp","text":"","title":"XDP"},{"location":"getting-started/troubleshooting/#xdp-program-fails-to-load","text":"When attempting to load an XDP program and the program fails to load: $ sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface veth92cd99b --priority 100 Error: status: Aborted, message: \"An error occurred. dispatcher attach failed on interface veth92cd99b: `bpf_link_create` failed\", details: [], metadata: MetadataMap { headers: {\"content-type\": \"application/grpc\", \"date\": \"Tue, 28 Nov 2023 13:37:02 GMT\", \"content-length\": \"0\"} } The log may look something like this: Nov 28 08:36:58 ebpf03 bpfman[2081732]: The bytecode image: quay.io/bpfman-bytecode/xdp_pass:latest is signed Nov 28 08:36:59 ebpf03 bpfman[2081732]: Loading program bytecode from container image: quay.io/bpfman-bytecode/xdp_pass:latest Nov 28 08:37:01 ebpf03 bpfman[2081732]: The bytecode image: quay.io/bpfman/xdp-dispatcher:v2 is signed Nov 28 08:37:02 ebpf03 bpfman[2081732]: BPFMAN load error: Error( \"dispatcher attach failed on interface veth92cd99b: `bpf_link_create` failed\", ) The issue may be the there is already an external XDP program loaded on the given interface. bpfman allows multiple XDP programs on an interface by loading a dispatcher program which is the XDP program and additional programs are loaded as extensions to the dispatcher . Use bpftool to determine if any programs are already loaded on an interface: $ sudo bpftool net list dev veth92cd99b xdp: veth92cd99b(32) generic id 8733 tc: veth92cd99b(32) clsact/ingress tc_dispatcher id 8922 flow_dissector:","title":"XDP Program Fails to Load"},{"location":"getting-started/tutorial/","text":"Tutorial This tutorial will show you how to use bpfman . There are several ways to launch and interact with bpfman and bpfman : Local Host - Run bpfman as a privileged process straight from build directory. See Local Host . Systemd Service - Run bpfman as a systemd service. See Systemd Service . Local Host Step 1: Build bpfman Perform the following steps to build bpfman . If this is your first time using bpfman, follow the instructions in Setup and Building bpfman to setup the prerequisites for building. cd $HOME/src/bpfman/ cargo xtask build-ebpf --libbpf-dir $HOME/src/libbpf cargo build Step 2: Setup bpfman environment bpfman supports both communication over a Unix socket. All examples, both using bpfman and the gRPC API use this socket. Step 3: Start bpfman While learning and experimenting with bpfman , it may be useful to run bpfman in the foreground (which requires a second terminal to run the bpfman commands below). For more details on how logging is handled in bpfman, see Logging . sudo RUST_LOG=info ./target/debug/bpfman Step 4: Load your first program We will load the simple xdp-pass program, which permits all traffic to the attached interface, vethff657c7 in this example. The section in the object file that contains the program is \"pass\". Finally, we will use the priority of 100. Find a deeper dive into CLI syntax in CLI Guide . sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 100 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6213 Map Owner ID: None Map Used By: 6213 Priority: 100 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6213 Name: pass Type: xdp Loaded At: 2023-07-17T17:48:10-0400 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [2724] BTF ID: 2834 Size Translated (bytes): 96 JITed: true Size JITed (bytes): 67 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 9 bpfman load image returns the same data as a bpfman get command. From the output, the id of 6213 can be found in the Kernel State section. This id can be used to perform a bpfman get to retrieve all relevant program data and a bpfman unload when the program needs to be unloaded. sudo ./target/debug/bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 We can recheck the details about the loaded program with the bpfman get command: sudo ./target/debug/bpfman get 6213 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6213 Map Owner ID: None Map Used By: 6213 Priority: 100 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6213 Name: pass Type: xdp Loaded At: 2023-07-17T17:48:10-0400 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [2724] BTF ID: 2834 Size Translated (bytes): 96 JITed: true Size JITed (bytes): 67 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 9 From the output above you can see the program was loaded to position 0 on our interface and thus will be executed first. Step 5: Loading more programs We will now load 2 more programs with different priorities to demonstrate how bpfman will ensure they are ordered correctly: sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 50 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6215 Map Owner ID: None Map Used By: 6215 Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6215 Name: pass Type: xdp : sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 200 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6217 Map Owner ID: None Map Used By: 6217 Priority: 200 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6217 Name: pass Type: xdp : Using bpfman list we can see all the programs that were loaded. sudo ./target/debug/bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 6215 pass xdp 2023-07-17T17:52:46-0400 6217 pass xdp 2023-07-17T17:53:57-0400 The lowest priority program is executed first, while the highest is executed last. As can be seen from the detailed output for each command below: Program 6215 is at position 0 with a priority of 50 Program 6213 is at position 1 with a priority of 100 Program 6217 is at position 2 with a priority of 200 sudo ./target/debug/bpfman get 6213 Bpfman State --------------- Name: pass : Priority: 100 Iface: vethff657c7 Position: 1 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6213 Name: pass Type: xdp : sudo ./target/debug/bpfman get 6215 Bpfman State --------------- Name: pass : Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6215 Name: pass Type: xdp : sudo ./target/debug/bpfman get 6217 Bpfman State --------------- Name: pass : Priority: 200 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6217 Name: pass Type: xdp : By default, the next program in the chain will only be executed if a given program returns pass (see proceed-on field in the bpfman get output above). If the next program in the chain should be called even if a different value is returned, then the program can be loaded with those additional return values using the proceed-on parameter (see bpfman load image xdp --help for list of valid values): sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 150 --proceed-on \"pass\" --proceed-on \"dispatcher_return\" Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6219 Map Owner ID: None Map Used By: 6219 Priority: 150 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6219 Name: pass Type: xdp : Which results in being loaded in position 2 because it was loaded at priority 150 , which is lower than the previous program at that position with a priority of 200 . Step 6: Delete a program Let's remove the program at position 1. sudo ./target/debug/bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 6215 pass xdp 2023-07-17T17:52:46-0400 6217 pass xdp 2023-07-17T17:53:57-0400 6219 pass xdp 2023-07-17T17:59:41-0400 sudo ./target/debug/bpfman unload 6213 And we can verify that it has been removed and the other programs re-ordered: sudo ./target/debug/bpfman list Program ID Name Type Load Time 6215 pass xdp 2023-07-17T17:52:46-0400 6217 pass xdp 2023-07-17T17:53:57-0400 6219 pass xdp 2023-07-17T17:59:41-0400 ./target/debug/bpfman get 6215 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6215 Map Owner ID: None Map Used By: 6215 Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6215 Name: pass Type: xdp : ./target/debug/bpfman get 6217 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6217 Map Owner ID: None Map Used By: 6217 Priority: 200 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6217 Name: pass Type: xdp : ./target/debug/bpfman get 6219 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6219 Map Owner ID: None Map Used By: 6219 Priority: 150 Iface: vethff657c7 Position: 1 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6219 Name: pass Type: xdp : When bpfman is stopped, all remaining programs will be unloaded automatically. Step 7: Clean-up To unwind all the changes, stop bpfman and then run the following script: sudo ./scripts/setup.sh uninstall WARNING: setup.sh uninstall cleans everything up, so /etc/bpfman/programs.d/ and /run/bpfman/bytecode/ are deleted. Save any changes or files that were created if needed. Systemd Service To run bpfman as a systemd service, the binaries will be placed in a well known location ( /usr/sbin/. ) and a service configuration file will be added ( /usr/lib/systemd/system/bpfman.service ). When run as a systemd service, the set of linux capabilities are limited to only the needed set. If permission errors are encountered, see Linux Capabilities for help debugging. Step 1 Same as Step 1 above, build bpfman if needed: cd $HOME/src/bpfman/ cargo xtask build-ebpf --libbpf-dir $HOME/src/libbpf cargo build Step 2: Setup bpfman environment Run the following command to copy the bpfman and bpfman binaries to /usr/sbin/ and copy a default bpfman.service file to /usr/lib/systemd/system/ . This option will also start the systemd service bpfman.service by default: sudo ./scripts/setup.sh install NOTE: Prior to kernel 5.19 , all eBPF sys calls required CAP_BPF, which are used to access maps shared between the BFP program and the userspace program. So userspace programs that are accessing maps and running on kernels older than 5.19 will require either sudo or the CAP_BPF capability ( sudo /sbin/setcap cap_bpf=ep ./<USERSPACE-PROGRAM> ). To update the configuration settings associated with running bpfman as a service, edit the service configuration file: sudo vi /usr/lib/systemd/system/bpfman.service sudo systemctl daemon-reload If bpfman or bpfman is rebuilt, the following command can be run to install the update binaries without regenerating the certifications. The bpfman service will is automatically restarted. sudo ./scripts/setup.sh reinstall Step 3: Start bpfman To manage bpfman as a systemd service, use systemctl . sudo ./scripts/setup.sh install will start the service, but the service can be manually stopped and started: sudo systemctl stop bpfman.service ... sudo systemctl start bpfman.service Step 4-6 Same as above except bpfman is now in $PATH: sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 100 : sudo bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 sudo bpfman unload 6213 Step 7: Clean-up To unwind all the changes performed while running bpfman as a systemd service, run the following script. This command cleans up everything, including stopping the bpfman service if it is still running. sudo ./scripts/setup.sh uninstall WARNING: setup.sh uninstall cleans everything up, so /etc/bpfman/programs.d/ and /run/bpfman/bytecode/ are deleted. Save any changes or files that were created if needed. Build and Run Local eBPF Programs In the examples above, all the eBPF programs were pulled from pre-built images. This tutorial uses examples from the xdp-tutorial . The pre-built container images can be found here: https://quay.io/organization/bpfman-bytecode To build these examples locally, check out the xdp-tutorial git repository and compile the examples. eBPF Bytecode Image Specifications describes how eBPF bytecode ispackaged in container images. To load these programs locally, use the bpfman load file command in place of the bpfman load image command. For example: sudo ./target/debug/bpfman load file --path /$HOME/src/xdp-tutorial/basic01-xdp-pass/xdp_pass_kern.o --name \"pass\" xdp --iface vethff657c7 --priority 100","title":"Bpfman on Linux Tutorial"},{"location":"getting-started/tutorial/#tutorial","text":"This tutorial will show you how to use bpfman . There are several ways to launch and interact with bpfman and bpfman : Local Host - Run bpfman as a privileged process straight from build directory. See Local Host . Systemd Service - Run bpfman as a systemd service. See Systemd Service .","title":"Tutorial"},{"location":"getting-started/tutorial/#local-host","text":"","title":"Local Host"},{"location":"getting-started/tutorial/#step-1-build-bpfman","text":"Perform the following steps to build bpfman . If this is your first time using bpfman, follow the instructions in Setup and Building bpfman to setup the prerequisites for building. cd $HOME/src/bpfman/ cargo xtask build-ebpf --libbpf-dir $HOME/src/libbpf cargo build","title":"Step 1: Build bpfman"},{"location":"getting-started/tutorial/#step-2-setup-bpfman-environment","text":"bpfman supports both communication over a Unix socket. All examples, both using bpfman and the gRPC API use this socket.","title":"Step 2: Setup bpfman environment"},{"location":"getting-started/tutorial/#step-3-start-bpfman","text":"While learning and experimenting with bpfman , it may be useful to run bpfman in the foreground (which requires a second terminal to run the bpfman commands below). For more details on how logging is handled in bpfman, see Logging . sudo RUST_LOG=info ./target/debug/bpfman","title":"Step 3: Start bpfman"},{"location":"getting-started/tutorial/#step-4-load-your-first-program","text":"We will load the simple xdp-pass program, which permits all traffic to the attached interface, vethff657c7 in this example. The section in the object file that contains the program is \"pass\". Finally, we will use the priority of 100. Find a deeper dive into CLI syntax in CLI Guide . sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 100 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6213 Map Owner ID: None Map Used By: 6213 Priority: 100 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6213 Name: pass Type: xdp Loaded At: 2023-07-17T17:48:10-0400 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [2724] BTF ID: 2834 Size Translated (bytes): 96 JITed: true Size JITed (bytes): 67 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 9 bpfman load image returns the same data as a bpfman get command. From the output, the id of 6213 can be found in the Kernel State section. This id can be used to perform a bpfman get to retrieve all relevant program data and a bpfman unload when the program needs to be unloaded. sudo ./target/debug/bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 We can recheck the details about the loaded program with the bpfman get command: sudo ./target/debug/bpfman get 6213 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6213 Map Owner ID: None Map Used By: 6213 Priority: 100 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6213 Name: pass Type: xdp Loaded At: 2023-07-17T17:48:10-0400 Tag: 4b9d1b2c140e87ce GPL Compatible: true Map IDs: [2724] BTF ID: 2834 Size Translated (bytes): 96 JITed: true Size JITed (bytes): 67 Kernel Allocated Memory (bytes): 4096 Verified Instruction Count: 9 From the output above you can see the program was loaded to position 0 on our interface and thus will be executed first.","title":"Step 4: Load your first program"},{"location":"getting-started/tutorial/#step-5-loading-more-programs","text":"We will now load 2 more programs with different priorities to demonstrate how bpfman will ensure they are ordered correctly: sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 50 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6215 Map Owner ID: None Map Used By: 6215 Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6215 Name: pass Type: xdp : sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 200 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6217 Map Owner ID: None Map Used By: 6217 Priority: 200 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6217 Name: pass Type: xdp : Using bpfman list we can see all the programs that were loaded. sudo ./target/debug/bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 6215 pass xdp 2023-07-17T17:52:46-0400 6217 pass xdp 2023-07-17T17:53:57-0400 The lowest priority program is executed first, while the highest is executed last. As can be seen from the detailed output for each command below: Program 6215 is at position 0 with a priority of 50 Program 6213 is at position 1 with a priority of 100 Program 6217 is at position 2 with a priority of 200 sudo ./target/debug/bpfman get 6213 Bpfman State --------------- Name: pass : Priority: 100 Iface: vethff657c7 Position: 1 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6213 Name: pass Type: xdp : sudo ./target/debug/bpfman get 6215 Bpfman State --------------- Name: pass : Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6215 Name: pass Type: xdp : sudo ./target/debug/bpfman get 6217 Bpfman State --------------- Name: pass : Priority: 200 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6217 Name: pass Type: xdp : By default, the next program in the chain will only be executed if a given program returns pass (see proceed-on field in the bpfman get output above). If the next program in the chain should be called even if a different value is returned, then the program can be loaded with those additional return values using the proceed-on parameter (see bpfman load image xdp --help for list of valid values): sudo ./target/debug/bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 150 --proceed-on \"pass\" --proceed-on \"dispatcher_return\" Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6219 Map Owner ID: None Map Used By: 6219 Priority: 150 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6219 Name: pass Type: xdp : Which results in being loaded in position 2 because it was loaded at priority 150 , which is lower than the previous program at that position with a priority of 200 .","title":"Step 5: Loading more programs"},{"location":"getting-started/tutorial/#step-6-delete-a-program","text":"Let's remove the program at position 1. sudo ./target/debug/bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 6215 pass xdp 2023-07-17T17:52:46-0400 6217 pass xdp 2023-07-17T17:53:57-0400 6219 pass xdp 2023-07-17T17:59:41-0400 sudo ./target/debug/bpfman unload 6213 And we can verify that it has been removed and the other programs re-ordered: sudo ./target/debug/bpfman list Program ID Name Type Load Time 6215 pass xdp 2023-07-17T17:52:46-0400 6217 pass xdp 2023-07-17T17:53:57-0400 6219 pass xdp 2023-07-17T17:59:41-0400 ./target/debug/bpfman get 6215 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6215 Map Owner ID: None Map Used By: 6215 Priority: 50 Iface: vethff657c7 Position: 0 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6215 Name: pass Type: xdp : ./target/debug/bpfman get 6217 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6217 Map Owner ID: None Map Used By: 6217 Priority: 200 Iface: vethff657c7 Position: 2 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6217 Name: pass Type: xdp : ./target/debug/bpfman get 6219 Bpfman State --------------- Name: pass Image URL: quay.io/bpfman-bytecode/xdp_pass:latest Pull Policy: IfNotPresent Global: None Metadata: None Map Pin Path: /run/bpfman/fs/maps/6219 Map Owner ID: None Map Used By: 6219 Priority: 150 Iface: vethff657c7 Position: 1 Proceed On: pass, dispatcher_return Kernel State ---------------------------------- ID: 6219 Name: pass Type: xdp : When bpfman is stopped, all remaining programs will be unloaded automatically.","title":"Step 6: Delete a program"},{"location":"getting-started/tutorial/#step-7-clean-up","text":"To unwind all the changes, stop bpfman and then run the following script: sudo ./scripts/setup.sh uninstall WARNING: setup.sh uninstall cleans everything up, so /etc/bpfman/programs.d/ and /run/bpfman/bytecode/ are deleted. Save any changes or files that were created if needed.","title":"Step 7: Clean-up"},{"location":"getting-started/tutorial/#systemd-service","text":"To run bpfman as a systemd service, the binaries will be placed in a well known location ( /usr/sbin/. ) and a service configuration file will be added ( /usr/lib/systemd/system/bpfman.service ). When run as a systemd service, the set of linux capabilities are limited to only the needed set. If permission errors are encountered, see Linux Capabilities for help debugging.","title":"Systemd Service"},{"location":"getting-started/tutorial/#step-1","text":"Same as Step 1 above, build bpfman if needed: cd $HOME/src/bpfman/ cargo xtask build-ebpf --libbpf-dir $HOME/src/libbpf cargo build","title":"Step 1"},{"location":"getting-started/tutorial/#step-2-setup-bpfman-environment_1","text":"Run the following command to copy the bpfman and bpfman binaries to /usr/sbin/ and copy a default bpfman.service file to /usr/lib/systemd/system/ . This option will also start the systemd service bpfman.service by default: sudo ./scripts/setup.sh install NOTE: Prior to kernel 5.19 , all eBPF sys calls required CAP_BPF, which are used to access maps shared between the BFP program and the userspace program. So userspace programs that are accessing maps and running on kernels older than 5.19 will require either sudo or the CAP_BPF capability ( sudo /sbin/setcap cap_bpf=ep ./<USERSPACE-PROGRAM> ). To update the configuration settings associated with running bpfman as a service, edit the service configuration file: sudo vi /usr/lib/systemd/system/bpfman.service sudo systemctl daemon-reload If bpfman or bpfman is rebuilt, the following command can be run to install the update binaries without regenerating the certifications. The bpfman service will is automatically restarted. sudo ./scripts/setup.sh reinstall","title":"Step 2: Setup bpfman environment"},{"location":"getting-started/tutorial/#step-3-start-bpfman_1","text":"To manage bpfman as a systemd service, use systemctl . sudo ./scripts/setup.sh install will start the service, but the service can be manually stopped and started: sudo systemctl stop bpfman.service ... sudo systemctl start bpfman.service","title":"Step 3: Start bpfman"},{"location":"getting-started/tutorial/#step-4-6","text":"Same as above except bpfman is now in $PATH: sudo bpfman load image --image-url quay.io/bpfman-bytecode/xdp_pass:latest xdp --iface vethff657c7 --priority 100 : sudo bpfman list Program ID Name Type Load Time 6213 pass xdp 2023-07-17T17:48:10-0400 sudo bpfman unload 6213","title":"Step 4-6"},{"location":"getting-started/tutorial/#step-7-clean-up_1","text":"To unwind all the changes performed while running bpfman as a systemd service, run the following script. This command cleans up everything, including stopping the bpfman service if it is still running. sudo ./scripts/setup.sh uninstall WARNING: setup.sh uninstall cleans everything up, so /etc/bpfman/programs.d/ and /run/bpfman/bytecode/ are deleted. Save any changes or files that were created if needed.","title":"Step 7: Clean-up"},{"location":"getting-started/tutorial/#build-and-run-local-ebpf-programs","text":"In the examples above, all the eBPF programs were pulled from pre-built images. This tutorial uses examples from the xdp-tutorial . The pre-built container images can be found here: https://quay.io/organization/bpfman-bytecode To build these examples locally, check out the xdp-tutorial git repository and compile the examples. eBPF Bytecode Image Specifications describes how eBPF bytecode ispackaged in container images. To load these programs locally, use the bpfman load file command in place of the bpfman load image command. For example: sudo ./target/debug/bpfman load file --path /$HOME/src/xdp-tutorial/basic01-xdp-pass/xdp_pass_kern.o --name \"pass\" xdp --iface vethff657c7 --priority 100","title":"Build and Run Local eBPF Programs"},{"location":"governance/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement directly. Maintainers are identified in the MAINTAINERS.md file and their contact information is on their GitHub profile page. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"governance/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"governance/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"governance/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"governance/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"governance/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement directly. Maintainers are identified in the MAINTAINERS.md file and their contact information is on their GitHub profile page. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"governance/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"governance/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"governance/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"governance/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"governance/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"governance/CONTRIBUTING/","text":"Contributing Guide New Contributor Guide Ways to Contribute Find an Issue Ask for Help Pull Request Lifecycle Development Environment Setup Signoff Your Commits Pull Request Checklist Welcome! We are glad that you want to contribute to our project! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setting up a new developer environment Gaps in our Quickstart Guide or documentation Bugs in our automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know! Ways to Contribute We welcome many different types of contributions including: New features Builds, CI/CD Bug fixes Documentation Issue Triage Answering questions on Slack/Mailing List Web design Communications / Social Media / Blog Posts Release management Not everything happens through a GitHub pull request. Please come to our meetings or contact us and let's discuss how we can work together. Come to Meetings Absolutely everyone is welcome to come to any of our meetings. You never need an invite to join us. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! You can find out more about our meetings here . You don\u2019t have to turn on your video. The first time you come, introducing yourself is more than enough. Over time, we hope that you feel comfortable voicing your opinions, giving feedback on others\u2019 ideas, and even sharing your own ideas, and experiences. Find an Issue We have good first issues for new contributors and help wanted issues suitable for any contributor. good first issue has extra information to help you make your first contribution. help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us on Slack and we will be happy to help. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine. Ask for Help The best way to reach us with a question when contributing is to ask on: The original github issue Our Slack channel Pull Request Lifecycle Pull requests are managed by Mergify. Our process is currently as follows: When you open a PR a maintainer will automatically be assigned for review Make sure that your PR is passing CI - if you need help with failing checks please feel free to ask! Once it is passing all CI checks, a maintainer will review your PR and you may be asked to make changes. When you have received at least one approval from a maintainer, your PR will be merged automiatcally. In some cases, other changes may conflict with your PR. If this happens, you will get notified by a comment in the issue that your PR requires a rebase, and the needs-rebase label will be applied. Once a rebase has been performed, this label will be automatically removed. Development Environment Setup Instructions Signoff Your Commits DCO Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s Logical Grouping of Commits It is a recommended best practice to keep your changes as logically grouped as possible within individual commits. If while you're developing you prefer doing a number of commits that are \"checkpoints\" and don't represent a single logical change, please squash those together before asking for a review. When addressing review comments, please perform an interactive rebase and edit commits directly rather than adding new commits with messages like \"Fix review comments\". Commit message guidelines A good commit message should describe what changed and why. The first line should: contain a short description of the change (preferably 50 characters or less, and no more than 72 characters) be entirely in lowercase with the exception of proper nouns, acronyms, and the words that refer to code, like function/variable names be prefixed with the name of the sub crate being changed Examples: * bpfman: validate program section names * bpf: add dispatcher program test slot Keep the second line blank. Wrap all other lines at 72 columns (except for long URLs). If your patch fixes an open issue, you can add a reference to it at the end of the log. Use the Fixes: # prefix and the issue number. For other references use Refs: # . Refs may include multiple issues, separated by a comma. Examples: Fixes: #1337 Refs: #1234 Sample complete commit message: subcrate: explain the commit in one line Body of commit message is a few lines of text, explaining things in more detail, possibly giving some background about the issue being fixed, etc. The body of the commit message can be several paragraphs, and please do proper word-wrap and keep columns shorter than about 72 characters or so. That way, `git log` will show things nicely even when it is indented. Fixes: #1337 Refs: #453, #154 Pull Request Checklist When you submit your pull request, or you push new commits to it, our automated systems will run some checks on your new code. We require that your pull request passes these checks, but we also have more criteria than just that before we can accept and merge it. We recommend that you check the following things locally before you submit your code: Verify that Rust code has been formatted and that all clippy lints have been fixed: cd src/bpfman/ cargo +nightly fmt --all -- --check cargo +nightly clippy --all -- --deny warnings Verify that Go code has been formatted and linted Verify that Yaml files have been formatted (see Install Yaml Formatter ) Verify that unit tests are passing locally (see Unit Testing ): cd src/bpfman/ cargo test Verify that integration tests are passing locally (see Basic Integration Tests ): cd src/bpfman/ cargo xtask integration-test If developing the bpfman-operator, verify that bpfman-operator unit tests are passing locally: cd src/bpfman/bpfman-operator/ make test If developing the bpfman-operator, verify that bpfman-operator integration tests are passing locally (see Kubernetes Integration Tests ): cd src/bpfman/bpfman-operator/ make test-integration","title":"Contributing"},{"location":"governance/CONTRIBUTING/#contributing-guide","text":"New Contributor Guide Ways to Contribute Find an Issue Ask for Help Pull Request Lifecycle Development Environment Setup Signoff Your Commits Pull Request Checklist Welcome! We are glad that you want to contribute to our project! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setting up a new developer environment Gaps in our Quickstart Guide or documentation Bugs in our automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!","title":"Contributing Guide"},{"location":"governance/CONTRIBUTING/#ways-to-contribute","text":"We welcome many different types of contributions including: New features Builds, CI/CD Bug fixes Documentation Issue Triage Answering questions on Slack/Mailing List Web design Communications / Social Media / Blog Posts Release management Not everything happens through a GitHub pull request. Please come to our meetings or contact us and let's discuss how we can work together.","title":"Ways to Contribute"},{"location":"governance/CONTRIBUTING/#come-to-meetings","text":"Absolutely everyone is welcome to come to any of our meetings. You never need an invite to join us. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! You can find out more about our meetings here . You don\u2019t have to turn on your video. The first time you come, introducing yourself is more than enough. Over time, we hope that you feel comfortable voicing your opinions, giving feedback on others\u2019 ideas, and even sharing your own ideas, and experiences.","title":"Come to Meetings"},{"location":"governance/CONTRIBUTING/#find-an-issue","text":"We have good first issues for new contributors and help wanted issues suitable for any contributor. good first issue has extra information to help you make your first contribution. help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us on Slack and we will be happy to help. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine.","title":"Find an Issue"},{"location":"governance/CONTRIBUTING/#ask-for-help","text":"The best way to reach us with a question when contributing is to ask on: The original github issue Our Slack channel","title":"Ask for Help"},{"location":"governance/CONTRIBUTING/#pull-request-lifecycle","text":"Pull requests are managed by Mergify. Our process is currently as follows: When you open a PR a maintainer will automatically be assigned for review Make sure that your PR is passing CI - if you need help with failing checks please feel free to ask! Once it is passing all CI checks, a maintainer will review your PR and you may be asked to make changes. When you have received at least one approval from a maintainer, your PR will be merged automiatcally. In some cases, other changes may conflict with your PR. If this happens, you will get notified by a comment in the issue that your PR requires a rebase, and the needs-rebase label will be applied. Once a rebase has been performed, this label will be automatically removed.","title":"Pull Request Lifecycle"},{"location":"governance/CONTRIBUTING/#development-environment-setup","text":"Instructions","title":"Development Environment Setup"},{"location":"governance/CONTRIBUTING/#signoff-your-commits","text":"","title":"Signoff Your Commits"},{"location":"governance/CONTRIBUTING/#dco","text":"Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"DCO"},{"location":"governance/CONTRIBUTING/#logical-grouping-of-commits","text":"It is a recommended best practice to keep your changes as logically grouped as possible within individual commits. If while you're developing you prefer doing a number of commits that are \"checkpoints\" and don't represent a single logical change, please squash those together before asking for a review. When addressing review comments, please perform an interactive rebase and edit commits directly rather than adding new commits with messages like \"Fix review comments\".","title":"Logical Grouping of Commits"},{"location":"governance/CONTRIBUTING/#commit-message-guidelines","text":"A good commit message should describe what changed and why. The first line should: contain a short description of the change (preferably 50 characters or less, and no more than 72 characters) be entirely in lowercase with the exception of proper nouns, acronyms, and the words that refer to code, like function/variable names be prefixed with the name of the sub crate being changed Examples: * bpfman: validate program section names * bpf: add dispatcher program test slot Keep the second line blank. Wrap all other lines at 72 columns (except for long URLs). If your patch fixes an open issue, you can add a reference to it at the end of the log. Use the Fixes: # prefix and the issue number. For other references use Refs: # . Refs may include multiple issues, separated by a comma. Examples: Fixes: #1337 Refs: #1234 Sample complete commit message: subcrate: explain the commit in one line Body of commit message is a few lines of text, explaining things in more detail, possibly giving some background about the issue being fixed, etc. The body of the commit message can be several paragraphs, and please do proper word-wrap and keep columns shorter than about 72 characters or so. That way, `git log` will show things nicely even when it is indented. Fixes: #1337 Refs: #453, #154","title":"Commit message guidelines"},{"location":"governance/CONTRIBUTING/#pull-request-checklist","text":"When you submit your pull request, or you push new commits to it, our automated systems will run some checks on your new code. We require that your pull request passes these checks, but we also have more criteria than just that before we can accept and merge it. We recommend that you check the following things locally before you submit your code: Verify that Rust code has been formatted and that all clippy lints have been fixed: cd src/bpfman/ cargo +nightly fmt --all -- --check cargo +nightly clippy --all -- --deny warnings Verify that Go code has been formatted and linted Verify that Yaml files have been formatted (see Install Yaml Formatter ) Verify that unit tests are passing locally (see Unit Testing ): cd src/bpfman/ cargo test Verify that integration tests are passing locally (see Basic Integration Tests ): cd src/bpfman/ cargo xtask integration-test If developing the bpfman-operator, verify that bpfman-operator unit tests are passing locally: cd src/bpfman/bpfman-operator/ make test If developing the bpfman-operator, verify that bpfman-operator integration tests are passing locally (see Kubernetes Integration Tests ): cd src/bpfman/bpfman-operator/ make test-integration","title":"Pull Request Checklist"},{"location":"governance/GOVERNANCE/","text":"bpfman Project Governance The bpfman project is dedicated to creating an easy way to run eBPF programs on a single host and in clusters. This governance explains how the project is run. Values Maintainers Becoming a Maintainer Meetings Code of Conduct Enforcement Security Response Team Voting Modifications Values The bpfman project and its leadership embrace the following values: Openness: Communication and decision-making happens in the open and is discoverable for future reference. As much as possible, all discussions and work take place in public forums and open repositories. Fairness: All stakeholders have the opportunity to provide feedback and submit contributions, which will be considered on their merits. Community over Product or Company: Sustaining and growing our community takes priority over shipping code or sponsors' organizational goals. Each contributor participates in the project as an individual. Inclusivity: We innovate through different perspectives and skill sets, which can only be accomplished in a welcoming and respectful environment. Participation: Responsibilities within the project are earned through participation, and there is a clear path up the contributor ladder into leadership positions. Maintainers bpfman Maintainers have write access to the project GitHub repository . They can merge their patches or patches from others. The list of current maintainers can be found at MAINTAINERS.md . Maintainers collectively manage the project's resources and contributors. This privilege is granted with some expectation of responsibility: maintainers are people who care about the bpfman project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests). A maintainer is a contributor to the project's success and a citizen helping the project succeed. The collective team of all Maintainers is known as the Maintainer Council, which is the governing body for the project. Becoming a Maintainer To become a Maintainer you need to demonstrate the following: commitment to the project: participate in discussions, contributions, code and documentation reviews, for 6 months or more, perform reviews for 10 non-trivial pull requests, contribute 10 non-trivial pull requests and have them merged, ability to write quality code and/or documentation, ability to collaborate with the team, understanding of how the team works (policies, processes for testing and code review, etc), understanding of the project's code base and coding and documentation style. A new Maintainer must be proposed by an existing maintainer by opening a Pull Request on GitHub to update the MAINTAINERS.md file. A simple majority vote of existing Maintainers approves the application. Maintainer nominations will be evaluated without prejudice to employers or demographics. Maintainers who are selected will be granted the necessary GitHub rights. Removing a Maintainer Maintainers may resign at any time if they feel that they will not be able to continue fulfilling their project duties. Maintainers may also be removed after being inactive, failing to fulfill their Maintainer responsibilities, violating the Code of Conduct, or for other reasons. Inactivity is defined as a period of very low or no activity in the project for a year or more, with no definite schedule to return to full Maintainer activity. A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers. Depending on the reason for removal, a Maintainer may be converted to Emeritus status. Emeritus Maintainers will still be consulted on some project matters and can be rapidly returned to Maintainer status if their availability changes. Meetings Time zones permitting, Maintainers are expected to participate in the public developer meeting, detailed in the meetings document . Maintainers will also have closed meetings to discuss security reports or Code of Conduct violations. Such meetings should be scheduled by any Maintainer on receipt of a security issue or CoC report. All current Maintainers must be invited to such closed meetings, except for any Maintainer who is accused of a CoC violation. Code of Conduct Code of Conduct violations by community members will be discussed and resolved on the private maintainer Slack channel. Security Response Team The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this responsibility is delegated, the Maintainers will appoint a team of at least two contributors to handle it. The Maintainers will review who is assigned to this at least once a year. The Security Response Team is responsible for handling all reports of security holes and breaches according to the security policy . Voting While most business in bpfman is conducted by \" lazy consensus \", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the private developer slack channel for security or conduct matters. Votes may also be taken at the developer meeting . Any Maintainer may demand a vote be taken. Most votes require a simple majority of all Maintainers to succeed, except where otherwise noted. Two-thirds majority votes mean at least two-thirds of all existing maintainers. Modifying this Charter Changes to this Governance and its supporting documents may be approved by a 2/3 vote of the Maintainers.","title":"Governance"},{"location":"governance/GOVERNANCE/#bpfman-project-governance","text":"The bpfman project is dedicated to creating an easy way to run eBPF programs on a single host and in clusters. This governance explains how the project is run. Values Maintainers Becoming a Maintainer Meetings Code of Conduct Enforcement Security Response Team Voting Modifications","title":"bpfman Project Governance"},{"location":"governance/GOVERNANCE/#values","text":"The bpfman project and its leadership embrace the following values: Openness: Communication and decision-making happens in the open and is discoverable for future reference. As much as possible, all discussions and work take place in public forums and open repositories. Fairness: All stakeholders have the opportunity to provide feedback and submit contributions, which will be considered on their merits. Community over Product or Company: Sustaining and growing our community takes priority over shipping code or sponsors' organizational goals. Each contributor participates in the project as an individual. Inclusivity: We innovate through different perspectives and skill sets, which can only be accomplished in a welcoming and respectful environment. Participation: Responsibilities within the project are earned through participation, and there is a clear path up the contributor ladder into leadership positions.","title":"Values"},{"location":"governance/GOVERNANCE/#maintainers","text":"bpfman Maintainers have write access to the project GitHub repository . They can merge their patches or patches from others. The list of current maintainers can be found at MAINTAINERS.md . Maintainers collectively manage the project's resources and contributors. This privilege is granted with some expectation of responsibility: maintainers are people who care about the bpfman project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests). A maintainer is a contributor to the project's success and a citizen helping the project succeed. The collective team of all Maintainers is known as the Maintainer Council, which is the governing body for the project.","title":"Maintainers"},{"location":"governance/GOVERNANCE/#becoming-a-maintainer","text":"To become a Maintainer you need to demonstrate the following: commitment to the project: participate in discussions, contributions, code and documentation reviews, for 6 months or more, perform reviews for 10 non-trivial pull requests, contribute 10 non-trivial pull requests and have them merged, ability to write quality code and/or documentation, ability to collaborate with the team, understanding of how the team works (policies, processes for testing and code review, etc), understanding of the project's code base and coding and documentation style. A new Maintainer must be proposed by an existing maintainer by opening a Pull Request on GitHub to update the MAINTAINERS.md file. A simple majority vote of existing Maintainers approves the application. Maintainer nominations will be evaluated without prejudice to employers or demographics. Maintainers who are selected will be granted the necessary GitHub rights.","title":"Becoming a Maintainer"},{"location":"governance/GOVERNANCE/#removing-a-maintainer","text":"Maintainers may resign at any time if they feel that they will not be able to continue fulfilling their project duties. Maintainers may also be removed after being inactive, failing to fulfill their Maintainer responsibilities, violating the Code of Conduct, or for other reasons. Inactivity is defined as a period of very low or no activity in the project for a year or more, with no definite schedule to return to full Maintainer activity. A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers. Depending on the reason for removal, a Maintainer may be converted to Emeritus status. Emeritus Maintainers will still be consulted on some project matters and can be rapidly returned to Maintainer status if their availability changes.","title":"Removing a Maintainer"},{"location":"governance/GOVERNANCE/#meetings","text":"Time zones permitting, Maintainers are expected to participate in the public developer meeting, detailed in the meetings document . Maintainers will also have closed meetings to discuss security reports or Code of Conduct violations. Such meetings should be scheduled by any Maintainer on receipt of a security issue or CoC report. All current Maintainers must be invited to such closed meetings, except for any Maintainer who is accused of a CoC violation.","title":"Meetings"},{"location":"governance/GOVERNANCE/#code-of-conduct","text":"Code of Conduct violations by community members will be discussed and resolved on the private maintainer Slack channel.","title":"Code of Conduct"},{"location":"governance/GOVERNANCE/#security-response-team","text":"The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this responsibility is delegated, the Maintainers will appoint a team of at least two contributors to handle it. The Maintainers will review who is assigned to this at least once a year. The Security Response Team is responsible for handling all reports of security holes and breaches according to the security policy .","title":"Security Response Team"},{"location":"governance/GOVERNANCE/#voting","text":"While most business in bpfman is conducted by \" lazy consensus \", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the private developer slack channel for security or conduct matters. Votes may also be taken at the developer meeting . Any Maintainer may demand a vote be taken. Most votes require a simple majority of all Maintainers to succeed, except where otherwise noted. Two-thirds majority votes mean at least two-thirds of all existing maintainers.","title":"Voting"},{"location":"governance/GOVERNANCE/#modifying-this-charter","text":"Changes to this Governance and its supporting documents may be approved by a 2/3 vote of the Maintainers.","title":"Modifying this Charter"},{"location":"governance/MAINTAINERS/","text":"Maintainers See CONTRIBUTING.md for general contribution guidelines. See GOVERNANCE.md for governance guidelines and maintainer responsibilities. See CODEOWNERS for a detailed list of owners for the various source directories. Name Employer Responsibilities Dave Tucker Red Hat Catch all Andrew Stoycos Red Hat bpfman-operator, bpfman-agent Andre Fredette Red Hat All things tc-bpf Billy McFall Red Hat All things systemd","title":"Maintainers"},{"location":"governance/MAINTAINERS/#maintainers","text":"See CONTRIBUTING.md for general contribution guidelines. See GOVERNANCE.md for governance guidelines and maintainer responsibilities. See CODEOWNERS for a detailed list of owners for the various source directories. Name Employer Responsibilities Dave Tucker Red Hat Catch all Andrew Stoycos Red Hat bpfman-operator, bpfman-agent Andre Fredette Red Hat All things tc-bpf Billy McFall Red Hat All things systemd","title":"Maintainers"},{"location":"governance/MEETINGS/","text":"bpfman Community Meetings Meeting time We meet every Thursday at 10:00 AM Eastern Time. The meetings last up to 1 hour. Meeting location Video call link: https://meet.google.com/ggz-zkmp-pxx Or dial: (US) +1 98ttp4-221-0859 PIN: 613 588 790# More phone numbers: https://tel.meet/ggz-zkmp-pxx?pin=3270510926446 Meeting agenda and minutes Meeting agenda","title":"Meetings"},{"location":"governance/MEETINGS/#bpfman-community-meetings","text":"","title":"bpfman Community Meetings"},{"location":"governance/MEETINGS/#meeting-time","text":"We meet every Thursday at 10:00 AM Eastern Time. The meetings last up to 1 hour.","title":"Meeting time"},{"location":"governance/MEETINGS/#meeting-location","text":"Video call link: https://meet.google.com/ggz-zkmp-pxx Or dial: (US) +1 98ttp4-221-0859 PIN: 613 588 790# More phone numbers: https://tel.meet/ggz-zkmp-pxx?pin=3270510926446","title":"Meeting location"},{"location":"governance/MEETINGS/#meeting-agenda-and-minutes","text":"Meeting agenda","title":"Meeting agenda and minutes"},{"location":"governance/REVIEWING/","text":"Reviewing Guide This document covers who may review pull requests for this project, and guides how to perform code reviews that meet our community standards and code of conduct. All reviewers must read this document and agree to follow the project review guidelines. Reviewers who do not follow these guidelines may have their privileges revoked. The Reviewer Role Only maintainers are REQUIRED to review pull requests. Other contributors may opt to review pull requests, but any LGTM from a non-maintainer won't count towards the required number of Approved Reviews in the Mergify policy. Values All reviewers must abide by the Code of Conduct and are also protected by it. A reviewer should not tolerate poor behavior and is encouraged to report any behavior that violates the Code of Conduct. All of our values listed above are distilled from our Code of Conduct. Below are concrete examples of how it applies to code review specifically: Inclusion Be welcoming and inclusive. You should proactively ensure that the author is successful. While any particular pull request may not ultimately be merged, overall we want people to have a great experience and be willing to contribute again. Answer the questions they didn't know to ask or offer concrete help when they appear stuck. Sustainability Avoid burnout by enforcing healthy boundaries. Here are some examples of how a reviewer is encouraged to act to take care of themselves: Authors should meet baseline expectations when submitting a pull request, such as writing tests. If your availability changes, you can step down from a pull request and have someone else assigned. If interactions with an author are not following the code of conduct, close the PR and raise it with your Code of Conduct committee or point of contact. It's not your job to coax people into behaving. Trust Be trustworthy. During a review, your actions both build and help maintain the trust that the community has placed in this project. Below are examples of ways that we build trust: Transparency - If a pull request won't be merged, clearly say why and close it. If a pull request won't be reviewed for a while, let the author know so they can set expectations and understand why it's blocked. Integrity - Put the project's best interests ahead of personal relationships or company affiliations when deciding if a change should be merged. Stability - Only merge when the change won't negatively impact project stability. It can be tempting to merge a pull request that doesn't meet our quality standards, for example when the review has been delayed, or because we are trying to deliver new features quickly, but regressions can significantly hurt trust in our project. Process Reviewers are automatically assigned based on the CODEOWNERS file. Reviewers should wait for automated checks to pass before reviewing At least 1 approved review is required from a maintainer before a pull request can be merged All CI checks must pass If a PR is stuck for some reason it is down to the reviewer to determine the best course of action: PRs may be closed if they are no longer relevant A maintainer may choose to carry a PR forward on their own, but they should ALWAYS include the original author's commits A maintainer may choose to open additional PRs to help lay a foundation on which the stuck PR can be unstuck. They may either rebase the stuck PR themselves or leave this to the author Maintainers should not merge their pull requests without a review Maintainers should let the Mergify bot merge PRs and not merge PRs directly In times of need, i.e. to fix pressing security issues, the Maintainers may, at their discretion, merge PRs without review. They should at least add a comment to the PR explaining why they did so. Checklist Below are a set of common questions that apply to all pull requests: [ ] Is this PR targeting the correct branch? [ ] Does the commit message provide an adequate description of the change? [ ] Does the affected code have corresponding tests? [ ] Are the changes documented, not just with inline documentation, but also with conceptual documentation such as an overview of a new feature, or task-based documentation like a tutorial? Consider if this change should be announced on your project blog. [ ] Does this introduce breaking changes that would require an announcement or bumping of the major version? [ ] Does this PR introduce any new dependencies? Reading List Reviewers are encouraged to read the following articles for help with common reviewer tasks: The Art of Closing: How to close an unfinished or rejected pull request Kindness and Code Reviews: Improving the Way We Give Feedback Code Review Guidelines for Humans: Examples of good and back feedback","title":"Reviewing Guide"},{"location":"governance/REVIEWING/#reviewing-guide","text":"This document covers who may review pull requests for this project, and guides how to perform code reviews that meet our community standards and code of conduct. All reviewers must read this document and agree to follow the project review guidelines. Reviewers who do not follow these guidelines may have their privileges revoked.","title":"Reviewing Guide"},{"location":"governance/REVIEWING/#the-reviewer-role","text":"Only maintainers are REQUIRED to review pull requests. Other contributors may opt to review pull requests, but any LGTM from a non-maintainer won't count towards the required number of Approved Reviews in the Mergify policy.","title":"The Reviewer Role"},{"location":"governance/REVIEWING/#values","text":"All reviewers must abide by the Code of Conduct and are also protected by it. A reviewer should not tolerate poor behavior and is encouraged to report any behavior that violates the Code of Conduct. All of our values listed above are distilled from our Code of Conduct. Below are concrete examples of how it applies to code review specifically:","title":"Values"},{"location":"governance/REVIEWING/#inclusion","text":"Be welcoming and inclusive. You should proactively ensure that the author is successful. While any particular pull request may not ultimately be merged, overall we want people to have a great experience and be willing to contribute again. Answer the questions they didn't know to ask or offer concrete help when they appear stuck.","title":"Inclusion"},{"location":"governance/REVIEWING/#sustainability","text":"Avoid burnout by enforcing healthy boundaries. Here are some examples of how a reviewer is encouraged to act to take care of themselves: Authors should meet baseline expectations when submitting a pull request, such as writing tests. If your availability changes, you can step down from a pull request and have someone else assigned. If interactions with an author are not following the code of conduct, close the PR and raise it with your Code of Conduct committee or point of contact. It's not your job to coax people into behaving.","title":"Sustainability"},{"location":"governance/REVIEWING/#trust","text":"Be trustworthy. During a review, your actions both build and help maintain the trust that the community has placed in this project. Below are examples of ways that we build trust: Transparency - If a pull request won't be merged, clearly say why and close it. If a pull request won't be reviewed for a while, let the author know so they can set expectations and understand why it's blocked. Integrity - Put the project's best interests ahead of personal relationships or company affiliations when deciding if a change should be merged. Stability - Only merge when the change won't negatively impact project stability. It can be tempting to merge a pull request that doesn't meet our quality standards, for example when the review has been delayed, or because we are trying to deliver new features quickly, but regressions can significantly hurt trust in our project.","title":"Trust"},{"location":"governance/REVIEWING/#process","text":"Reviewers are automatically assigned based on the CODEOWNERS file. Reviewers should wait for automated checks to pass before reviewing At least 1 approved review is required from a maintainer before a pull request can be merged All CI checks must pass If a PR is stuck for some reason it is down to the reviewer to determine the best course of action: PRs may be closed if they are no longer relevant A maintainer may choose to carry a PR forward on their own, but they should ALWAYS include the original author's commits A maintainer may choose to open additional PRs to help lay a foundation on which the stuck PR can be unstuck. They may either rebase the stuck PR themselves or leave this to the author Maintainers should not merge their pull requests without a review Maintainers should let the Mergify bot merge PRs and not merge PRs directly In times of need, i.e. to fix pressing security issues, the Maintainers may, at their discretion, merge PRs without review. They should at least add a comment to the PR explaining why they did so.","title":"Process"},{"location":"governance/REVIEWING/#checklist","text":"Below are a set of common questions that apply to all pull requests: [ ] Is this PR targeting the correct branch? [ ] Does the commit message provide an adequate description of the change? [ ] Does the affected code have corresponding tests? [ ] Are the changes documented, not just with inline documentation, but also with conceptual documentation such as an overview of a new feature, or task-based documentation like a tutorial? Consider if this change should be announced on your project blog. [ ] Does this introduce breaking changes that would require an announcement or bumping of the major version? [ ] Does this PR introduce any new dependencies?","title":"Checklist"},{"location":"governance/REVIEWING/#reading-list","text":"Reviewers are encouraged to read the following articles for help with common reviewer tasks: The Art of Closing: How to close an unfinished or rejected pull request Kindness and Code Reviews: Improving the Way We Give Feedback Code Review Guidelines for Humans: Examples of good and back feedback","title":"Reading List"},{"location":"governance/SECURITY/","text":"Security Policy Supported Versions No released versions of bpfman and bpfman-agent or bpfman-operator will receive regular security updates until a mainline release has been performed. A reported and fixed vulnerability will be included in the next minor release, which depending on the severity of the vulnerability may be immediate. Reporting a Vulnerability To report a vulnerability, please use the Private Vulnerability Reporting Feature on GitHub. We will endevour to respond within 48hrs of reporting. If a vulnerability is reported but considered low priority it may be converted into an issue and handled on the public issue tracker. Should a vulnerability be considered severe we will endeavour to patch it within 48hrs of acceptance, and may ask for you to collaborate with us on a temporary private fork of the repository.","title":"Security"},{"location":"governance/SECURITY/#security-policy","text":"","title":"Security Policy"},{"location":"governance/SECURITY/#supported-versions","text":"No released versions of bpfman and bpfman-agent or bpfman-operator will receive regular security updates until a mainline release has been performed. A reported and fixed vulnerability will be included in the next minor release, which depending on the severity of the vulnerability may be immediate.","title":"Supported Versions"},{"location":"governance/SECURITY/#reporting-a-vulnerability","text":"To report a vulnerability, please use the Private Vulnerability Reporting Feature on GitHub. We will endevour to respond within 48hrs of reporting. If a vulnerability is reported but considered low priority it may be converted into an issue and handled on the public issue tracker. Should a vulnerability be considered severe we will endeavour to patch it within 48hrs of acceptance, and may ask for you to collaborate with us on a temporary private fork of the repository.","title":"Reporting a Vulnerability"}]}